{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "974b2c96"
   },
   "source": [
    "# üèÉ‚Äç‚ôÄÔ∏è „ÇØ„Ç§„ÉÉ„ÇØ„Çπ„Çø„Éº„Éà\n",
    "\n",
    "Weave„ÅÆÊ©üËÉΩ„ÇíÁêÜËß£„Åó„Å¶„ÅÑ„Åç„Åæ„Åó„Çá„ÅÜÔºÅ\n",
    "\n",
    "# <a href=\"https://colab.research.google.com/drive/1bdymP7p7d4z7izsS-PhMUxXcD38p9Hqr\" target=\"_blank\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Google Colab„ÅßÈñã„Åè\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "56dc5c9d"
   },
   "source": [
    "## ü™Ñ `weave`„É©„Ç§„Éñ„É©„É™„ÅÆ„Ç§„É≥„Çπ„Éà„Éº„É´„Å®„É≠„Ç∞„Ç§„É≥\n",
    "\n",
    "\n",
    "„É©„Ç§„Éñ„É©„É™„Çí„Ç§„É≥„Çπ„Éà„Éº„É´„Åó„ÄÅ„Ç¢„Ç´„Ç¶„É≥„Éà„Å´„É≠„Ç∞„Ç§„É≥„Åô„Çã„Åì„Å®„Åã„ÇâÂßã„ÇÅ„Åæ„Åó„Çá„ÅÜ„ÄÇ\n",
    "\n",
    "„Åì„ÅÆ‰æã„Åß„ÅØ„ÄÅW&B Inference„Çí‰ΩøÁî®„Åô„Çã„Åü„ÇÅ„ÄÅPublic CloudÁí∞Â¢É„ÅÆWANDB_API_KEY„ÅåÂøÖË¶Å„Åß„Åô„ÄÇDedicated Cloud„ÇÑOnpremise„ÇíÂà©Áî®„Åï„Çå„Å¶„ÅÑ„ÇãÊñπ„ÅØ„ÄÅPublic Cloud„ÅÆkey„ÇÇÁî®ÊÑè„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0921bbef-d089-4960-b624-505172f90d76",
    "outputId": "a262f278-48dc-411f-9fd9-58d69def7db9"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import wandb\n",
    "import weave\n",
    "import openai\n",
    "import json\n",
    "#========================================\n",
    "# Áí∞Â¢ÉÂ§âÊï∞„ÇíÈÅ©Âàá„Å´Ë®≠ÂÆö„Åó„Å¶„Åè„Å†„Åï„ÅÑ\n",
    "#========================================\n",
    "#os.environ[\"WANDB_BASE_URL\"] = \"\"\n",
    "#os.environ[\"WANDB_API_KEY\"] = \"\"\n",
    "#os.environ[\"OPEN_API_KEY\"] = \"\"\n",
    "\n",
    "wandb.login()\n",
    "PROJECT = \"weave-handson\" # handson„ÅßÂà©Áî®„Åô„Çãproject„Åß„Åô„ÄÇentity(team)„ÅÆ„Å®„Åì„Çç„Çí„ÅîËá™Ë∫´„ÅÆteam„Å´ÁΩÆ„ÅçÊèõ„Åà„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ\n",
    "weave.init(PROJECT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "09467a33"
   },
   "source": [
    "# Èñ¢Êï∞„ÅÆÂÖ•Âäõ„Å®Âá∫Âäõ„Çí„Éà„É©„ÉÉ„Ç≠„É≥„Ç∞\n",
    "\n",
    "Weave„Å´„Çà„Çä„ÄÅ„É¶„Éº„Ç∂„Éº„ÅØÈñ¢Êï∞Âëº„Å≥Âá∫„Åó„Çí„ÄÅ„Ç≥„Éº„Éâ„ÄÅÂÖ•Âäõ„ÄÅÂá∫Âäõ„ÄÅ„Åï„Çâ„Å´„ÅØLLM„Éà„Éº„ÇØ„É≥„Å®„Ç≥„Çπ„Éà„Åæ„Åß„Éà„É©„ÉÉ„Ç≠„É≥„Ç∞„Åß„Åç„Åæ„Åô„ÄÇ‰ª•‰∏ã„ÅÆ„Çª„ÇØ„Ç∑„Éß„É≥„Åß„ÅØÊ¨°„ÅÆÂÜÖÂÆπ„Çí„Ç´„Éê„Éº„Åó„Åæ„ÅôÔºö\n",
    "\n",
    "* „Ç´„Çπ„Çø„É†Èñ¢Êï∞\n",
    "* „Éô„É≥„ÉÄ„ÉºÁµ±Âêà\n",
    "* „Éç„Çπ„Éà„Åó„ÅüÈñ¢Êï∞Âëº„Å≥Âá∫„Åó\n",
    "* „Ç®„É©„Éº„Éà„É©„ÉÉ„Ç≠„É≥„Ç∞"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "99453d48"
   },
   "source": [
    "## „Ç´„Çπ„Çø„É†Èñ¢Êï∞„ÅÆ„Éà„É©„ÉÉ„Ç≠„É≥„Ç∞\n",
    "\n",
    "„Éà„É©„ÉÉ„Ç≠„É≥„Ç∞„Åó„Åü„ÅÑÈñ¢Êï∞„Å´@weave.op„Éá„Ç≥„É¨„Éº„Çø„ÇíËøΩÂä†„Åó„Åæ„Åô"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cdcd1bb7",
    "outputId": "8c252a02-fc1f-4499-9f4a-7bd4362a2f61"
   },
   "outputs": [],
   "source": [
    "@weave.op()\n",
    "def echo(user_input):\n",
    "    return user_input + \" \" + user_input\n",
    "\n",
    "result = echo(\"hello\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "174cabef"
   },
   "source": [
    "‰∏äË®ò„ÅÆüëÜ wandb„É™„É≥„ÇØ„Çí„ÇØ„É™„ÉÉ„ÇØ„Åô„Çã„Åì„Å®„Åß„ÄÅ„Ç§„É≥„Çø„É©„ÇØ„ÉÜ„Ç£„Éñ„ÉÄ„ÉÉ„Ç∑„É•„Éú„Éº„Éâ„ÇíË¶ã„Å§„Åë„Çã„Åì„Å®„Åå„Åß„Åç„Åæ„Åô„ÄÇ\n",
    "\n",
    "`weave.op`„ÇíËøΩÂä†„Åó„Å¶Èñ¢Êï∞„ÇíÂëº„Å≥Âá∫„Åó„ÅüÂæå„ÄÅ„É™„É≥„ÇØ„Å´„Ç¢„ÇØ„Çª„Çπ„Åó„Å¶„ÄÅ„Éó„É≠„Ç∏„Çß„ÇØ„ÉàÂÜÖ„Åß„Éà„É©„ÉÉ„Ç≠„É≥„Ç∞„Åï„Çå„Å¶„ÅÑ„Çã„ÅÆ„ÇíÁ¢∫Ë™ç„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ\n",
    "\n",
    "üí° „Ç≥„Éº„Éâ„ÇíËá™ÂãïÁöÑ„Å´„Éà„É©„ÉÉ„Ç≠„É≥„Ç∞„Åó„Å¶„ÅÑ„Åæ„Åô„ÄÇ„Ç≥„Éº„Éâ„Çø„Éñ„Çí„ÅîË¶ß„Åè„Å†„Åï„ÅÑÔºÅ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5ef6f27b"
   },
   "source": [
    "## Integration„ÇíÂà©Áî®„Åó„Åü„Éà„É©„ÉÉ„Ç≠„É≥„Ç∞ÔºàW&B Inference„ÄÅOpenAI„ÄÅAnthropic„ÄÅMistral„Å™„Å©Ôºâ\n",
    "\n",
    "„Åì„Åì„Åß„ÅØ„ÄÅ`W&B Inference`„Å∏„ÅÆÂÖ®„Å¶„ÅÆÂëº„Å≥Âá∫„Åó„ÇíËá™ÂãïÁöÑ„Å´„Éà„É©„ÉÉ„Ç≠„É≥„Ç∞„Åó„Å¶„ÅÑ„Åæ„Åô„ÄÇWeave„ÅØ„ÄÅÂ§ö„Åè„ÅÆLLM„É©„Ç§„Éñ„É©„É™„Çí@weave„Å™„Åó„ÅßËá™ÂãïÁöÑ„Å´„Éà„É©„ÉÉ„Ç≠„É≥„Ç∞„Åó„Å¶„Åè„Çå„Åæ„Åô„ÄÇ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM API„ÅÆÂàùÊúüÂåñ\n",
    "use_openai = True # OpenAI„ÅÆAPI„ÇíÂà©Áî®„Åô„ÇãÂ†¥Âêà„ÅØTrue, W&B Inference„ÇíÂà©Áî®„Åô„ÇãÂ†¥Âêà„ÅØFalse„Å´„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ\n",
    "\n",
    "if use_openai:\n",
    "    ## OpenAI„ÅÆAPI„ÇíÂà©Áî®„Åô„ÇãÂ†¥Âêà\n",
    "    #os.environ[\"OPENAI_API_KEY\"] = \"\" #OpenAI„ÅÆAPI„ÇíÂà©Áî®„Åô„ÇãÂ†¥Âêà„ÅØ„ÄÅOpenAI„ÅÆAPI Key„ÇíÂÖ•Âäõ„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ\n",
    "    client = openai.OpenAI(\n",
    "        api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "    )\n",
    "    model_name=\"gpt-4.1-2025-04-14\"\n",
    "else:\n",
    "    ## W&B Inference„ÇíÂà©Áî®„Åô„ÇãÂ†¥Âêà\n",
    "    #os.environ[\"WANDB_API_KEY_PUBLIC_CLOUD\"] = \"\"  # Public cloud„É¶„Éº„Ç∂„Éº„ÅØWANDB_API_KEY„Å®Âêå„ÅòÂÄ§„Çí‰ΩøÁî®„Åó„Å¶ÂïèÈ°å„Å™„ÅÑ„Åß„Åô„ÄÇDedicated Cloud„ÇíÂà©Áî®„Åó„ÄÅÊé®Ë´ñ„Å†„ÅëPublic Cloud„ÇíÂà©Áî®„Åô„ÇãÂ†¥Âêà„Å™„Å©„ÅØ„ÄÅPublic Cloud„ÅÆAPI Key„ÇíÂÖ•Âäõ„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ\n",
    "    INFERENCE_PROJECT=PROJECT # Inference„ÅÆÂà©Áî®Êñô„Éà„É©„ÉÉ„ÇØ„ÅÆ„Åü„ÇÅ„Å´Âà©Áî®„Åô„ÇãProject„Åß„Åô„ÄÇWeave„ÅÆPROJECT„Å®‰∏ÄËá¥„Åó„Å™„ÅÑÂ†¥ÂêàÔºàDedicated Cloud„ÇíÂà©Áî®„Åó„ÄÅÊé®Ë´ñ„Å†„ÅëPublic Cloud„ÇíÂà©Áî®„Åô„ÇãÂ†¥Âêà„Å™„Å©Ôºâ„ÅØ„ÄÅpublic cloud„ÅÆ‰∏≠„ÅÆentity„Å®team„ÅÆÁµÑ„ÅøÂêà„Çè„Åõ„ÇíÂÖ•„Çå„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ\n",
    "    client = openai.OpenAI(\n",
    "        base_url='https://api.inference.wandb.ai/v1',\n",
    "        api_key=os.getenv(\"WANDB_API_KEY_PUBLIC_CLOUD\") or os.getenv(\"WANDB_API_KEY\"),\n",
    "        project=INFERENCE_PROJECT,\n",
    "    )\n",
    "    model_name=\"openai/gpt-oss-20b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trace the model call in Weave\n",
    "def run_chat():\n",
    "    response = client.chat.completions.create(\n",
    "        model=model_name,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": \"„Ç∏„Éß„Éº„ÇØ„ÇíË®Ä„Å£„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ\"}\n",
    "        ],\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# Run and log the traced call\n",
    "output = run_chat()\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "35fe43dc"
   },
   "source": [
    "## NestedÈñ¢Êï∞„ÅÆ„Éà„É©„ÉÉ„Ç≠„É≥„Ç∞\n",
    "\n",
    "Âü∫Êú¨„ÇíÁ¢∫Ë™ç„Åó„Åü„ÅÆ„Åß„ÄÅ‰∏äË®ò„ÅÆ„Åô„Åπ„Å¶„ÇíÁµÑ„ÅøÂêà„Çè„Åõ„Å¶„ÄÅNestedÈñ¢Êï∞„Çí„Éà„É©„ÉÉ„Ç≠„É≥„Ç∞„Åó„Å¶„Åø„Åæ„Åó„Çá„ÅÜ„ÄÇ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "422050d0",
    "outputId": "9f1f22d6-2592-47ad-c83a-5c08095d632f"
   },
   "outputs": [],
   "source": [
    "@weave.op()\n",
    "def correct_grammar(user_input):\n",
    "    echoed_input = echo(user_input)\n",
    "    response = client.chat.completions.create(\n",
    "        model=model_name,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are a helpful assistant. Please create a song by following the user input.\",\n",
    "            },\n",
    "            {\"role\": \"user\", \"content\": echoed_input},\n",
    "        ],\n",
    "        temperature=0,\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "\n",
    "result = correct_grammar(\"hello\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6f483413"
   },
   "source": [
    "## „Ç®„É©„Éº„ÅÆ„Éà„É©„ÉÉ„Ç≠„É≥„Ç∞\n",
    "\n",
    "„Ç≥„Éº„Éâ„Åå„ÇØ„É©„ÉÉ„Ç∑„É•„Åô„Çã„Åü„Å≥„Å´„ÄÅweave„ÅØÂïèÈ°å„ÅÆÂéüÂõ†„Çí„Éè„Ç§„É©„Ç§„Éà„Åó„Åæ„Åô„ÄÇLLM„É¨„Çπ„Éù„É≥„Çπ„Åã„Çâ„Éá„Éº„Çø„ÇíËß£Êûê„Åô„ÇãÈöõ„Å´ÊôÇ„ÄÖÁô∫Áîü„Åô„ÇãJSON„Éë„Éº„Çπ„Ç®„É©„Éº„Å™„Å©„ÇíË¶ã„Å§„Åë„Çã„ÅÆ„Å´Áâπ„Å´ÊúâÁî®„Åß„Åô„ÄÇ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 356
    },
    "id": "14e7e103",
    "outputId": "1a121775-5b5c-4212-f205-d97c0ef7cac2"
   },
   "outputs": [],
   "source": [
    "@weave.op()\n",
    "def correct_grammar(user_input):\n",
    "    echoed_input = echo(user_input)\n",
    "    response = client.chat.completions.create(\n",
    "        model=model_name,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are a helpful assistant. Please create a song by following the user input.\",\n",
    "            },\n",
    "            {\"role\": \"user\", \"content\": echoed_input},\n",
    "        ],\n",
    "        temperature=0,\n",
    "        response_format={\"type\": \"json_object\"},\n",
    "    )\n",
    "    return json.loads(response.choices[0].message.content)\n",
    "\n",
    "\n",
    "result = correct_grammar(\"hello\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "knBo-VtWxl6g"
   },
   "source": [
    "# Trace„ÅÆÈ´òÂ∫¶„Å™Tips\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6KWJhiBsxl6h"
   },
   "source": [
    "### „Çµ„É≥„Éó„É™„É≥„Ç∞„É¨„Éº„Éà„ÅÆÂà∂Âæ°\n",
    "\n",
    "@weave.op„Éá„Ç≥„É¨„Éº„Çø„ÅÆtracing_sample_rate„Éë„É©„É°„Éº„Çø„ÇíË®≠ÂÆö„Åô„Çã„Åì„Å®„Åß„ÄÅop„ÅÆÂëº„Å≥Âá∫„Åó„Åå„Éà„É¨„Éº„Çπ„Åï„Çå„ÇãÈ†ªÂ∫¶„ÇíÂà∂Âæ°„Åß„Åç„Åæ„Åô„ÄÇ„Åì„Çå„ÅØ„ÄÅÂëº„Å≥Âá∫„Åó„ÅÆ„Çµ„Éñ„Çª„ÉÉ„Éà„ÅÆ„Åø„Çí„Éà„É¨„Éº„Çπ„Åô„ÇãÂøÖË¶Å„Åå„ÅÇ„ÇãÈ´òÈ†ªÂ∫¶op„Å´ÊúâÁî®„Åß„Åô„ÄÇ\n",
    "\n",
    "„Çµ„É≥„Éó„É™„É≥„Ç∞„É¨„Éº„Éà„ÅØ„É´„Éº„ÉàÂëº„Å≥Âá∫„Åó„Å´„ÅÆ„ÅøÈÅ©Áî®„Åï„Çå„Çã„Åì„Å®„Å´Ê≥®ÊÑè„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇop„Å´„Çµ„É≥„Éó„É´„É¨„Éº„Éà„Åå„ÅÇ„Å£„Å¶„ÇÇ„ÄÅÊúÄÂàù„Å´Âà•„ÅÆop„Å´„Çà„Å£„Å¶Âëº„Å≥Âá∫„Åï„Çå„ÅüÂ†¥Âêà„ÄÅ„Åù„ÅÆ„Çµ„É≥„Éó„É™„É≥„Ç∞„É¨„Éº„Éà„ÅØÁÑ°Ë¶ñ„Åï„Çå„Åæ„Åô„ÄÇ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cpX43hPbdeOZ"
   },
   "outputs": [],
   "source": [
    "@weave.op(tracing_sample_rate=0.1)  # Only trace ~10% of calls\n",
    "def high_frequency_op(x: int) -> int:\n",
    "    return x + 1\n",
    "\n",
    "@weave.op(tracing_sample_rate=1.0)  # Always trace (default)\n",
    "def always_traced_op(x: int) -> int:\n",
    "    return x + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ntJpevJQeLoA"
   },
   "source": [
    "### Âëº„Å≥Âá∫„ÅóË°®Á§∫Âêç"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iqOMqMIseYSV",
    "outputId": "f1fa226c-f099-4c5a-f411-6f4bc0caab19"
   },
   "outputs": [],
   "source": [
    "# Decorate your function\n",
    "@weave.op\n",
    "def my_function(name: str):\n",
    "    return f\"Hello, {name}!\"\n",
    "\n",
    "# Call your function -- Weave will automatically track inputs and outputs\n",
    "print(my_function(\"World\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RV9wwY4veaz8"
   },
   "outputs": [],
   "source": [
    "# 1st method\n",
    "result = my_function(\"World\", __weave={\"display_name\": \"My Custom Display Name\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R8d4mcWserLq",
    "outputId": "1938848a-75f4-487b-d46b-09d4995a66cb"
   },
   "outputs": [],
   "source": [
    "# 2nd method\n",
    "result, call = my_function.call(\"World\")\n",
    "call.set_display_name(\"My Custom Display Name\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "-M2abQSbew9M",
    "outputId": "81b67f69-507c-46e1-9d9b-3ae52d23c762"
   },
   "outputs": [],
   "source": [
    "# 3rd method\n",
    "@weave.op(call_display_name=\"My Custom Display Name\")\n",
    "def my_function(name: str):\n",
    "    return f\"Hello, {name}!\"\n",
    "\n",
    "my_function(\"World\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ExWp8BFIxl6g"
   },
   "source": [
    "\n",
    "### PII„ÅÆÁ∑®ÈõÜ\n",
    "\n",
    "‰∏ÄÈÉ®„ÅÆÁµÑÁπî„Åß„ÅØ„ÄÅÂ§ßË¶èÊ®°Ë®ÄË™û„É¢„Éá„É´ÔºàLLMÔºâ„ÉØ„Éº„ÇØ„Éï„É≠„Éº„ÅßÂêçÂâç„ÄÅÈõªË©±Áï™Âè∑„ÄÅ„É°„Éº„É´„Ç¢„Éâ„É¨„Çπ„Å™„Å©„ÅÆÂÄã‰∫∫Ë≠òÂà•ÊÉÖÂ†±ÔºàPIIÔºâ„ÇíÂá¶ÁêÜ„Åó„Åæ„Åô„ÄÇ„Åì„ÅÆ„Éá„Éº„Çø„ÇíWeights & BiasesÔºàW&BÔºâWeave„Å´‰øùÂ≠ò„Åô„Çã„Åì„Å®„ÅØ„ÄÅ„Ç≥„É≥„Éó„É©„Ç§„Ç¢„É≥„Çπ„Å®„Çª„Ç≠„É•„É™„ÉÜ„Ç£„ÅÆ„É™„Çπ„ÇØ„Çí„ÇÇ„Åü„Çâ„Åó„Åæ„Åô„ÄÇ\n",
    "\n",
    "Ê©üÂØÜ„Éá„Éº„Çø‰øùË≠∑Ê©üËÉΩ„Å´„Çà„Çä„ÄÅ„Éà„É¨„Éº„Çπ„ÅåWeave„Çµ„Éº„Éê„Éº„Å´ÈÄÅ‰ø°„Åï„Çå„ÇãÂâç„Å´„ÄÅÂÄã‰∫∫Ë≠òÂà•ÊÉÖÂ†±ÔºàPIIÔºâ„ÇíËá™ÂãïÁöÑ„Å´Á∑®ÈõÜ„Åß„Åç„Åæ„Åô„ÄÇ„Åì„ÅÆÊ©üËÉΩ„ÅØMicrosoft Presidio„ÇíWeave Python SDK„Å´Áµ±Âêà„Åó„Å¶„Åä„Çä„ÄÅSDK „É¨„Éô„É´„ÅßÁ∑®ÈõÜË®≠ÂÆö„ÇíÂà∂Âæ°„Åß„Åç„Çã„Åì„Å®„ÇíÊÑèÂë≥„Åó„Åæ„Åô„ÄÇ\n",
    "\n",
    "[Ë©≥Á¥∞„Éâ„Ç≠„É•„É°„É≥„Éà](https://weave-docs.wandb.ai/guides/tracking/redact-pii)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f957c9c8"
   },
   "source": [
    "# „Ç™„Éñ„Ç∏„Çß„ÇØ„Éà„ÅÆ„Éà„É©„ÉÉ„Ç≠„É≥„Ç∞\n",
    "\n",
    "Weave„Åß„ÅØ„ÄÅ„Ç∑„Çπ„ÉÜ„É†„Éó„É≠„É≥„Éó„Éà„ÇÑ‰ΩøÁî®„Åó„Å¶„ÅÑ„Çã„É¢„Éá„É´„ÄÅ„Éá„Éº„Çø„Çª„ÉÉ„Éà„Å™„Å©„ÅÆ„Ç¢„Çª„ÉÉ„Éà„Çí`weave.Objects`ÂÜÖ„Åß„Éê„Éº„Ç∏„Éß„É≥ÁÆ°ÁêÜ„Åß„Åç„Åæ„Åô„ÄÇ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8f6129cc"
   },
   "source": [
    "## „Éó„É≠„É≥„Éó„Éà„Éà„É©„ÉÉ„Ç≠„É≥„Ç∞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "08e4e852",
    "outputId": "d5b8c38a-0f00-4e9b-a09f-268f1ea1eb1c"
   },
   "outputs": [],
   "source": [
    "# StringPrompt1\n",
    "system_prompt = weave.StringPrompt(\"You are a pirate\")\n",
    "weave.publish(system_prompt, name=\"pirate_prompt\")\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "  model=model_name,\n",
    "  messages=[\n",
    "    {\n",
    "      \"role\": \"system\",\n",
    "      \"content\": system_prompt.format()\n",
    "    },\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": \"Explain general relativity in one paragraph.\"\n",
    "    }\n",
    "  ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ke21Y2OJg9Uo",
    "outputId": "f821bf2a-0577-4c96-9661-df18e4eb1524"
   },
   "outputs": [],
   "source": [
    "# StringPrompt2\n",
    "system_prompt = weave.StringPrompt(\"Talk like a pirate. I need to know I'm listening to a pirate.\")\n",
    "weave.publish(system_prompt, name=\"pirate_prompt\")\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "  model=model_name,\n",
    "  messages=[\n",
    "    {\n",
    "      \"role\": \"system\",\n",
    "      \"content\": system_prompt.format()\n",
    "    },\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": \"Explain general relativity in one paragraph.\"\n",
    "    }\n",
    "  ],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZrpZa01lg_-D",
    "outputId": "661423d9-9a58-4635-bce8-4f360838706d"
   },
   "outputs": [],
   "source": [
    "# MessagesPrompt1\n",
    "prompt = weave.MessagesPrompt([\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a stegosaurus, but don't be too obvious about it.\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"What's good to eat around here?\"\n",
    "    }\n",
    "])\n",
    "weave.publish(prompt, name=\"dino_prompt\")\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "  model=\"gpt-4o\",\n",
    "  messages=prompt.format(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WY5UZLgIhHYW",
    "outputId": "7a1cb983-044b-4d14-81fa-941493c870a0"
   },
   "outputs": [],
   "source": [
    "# parameterizing prompts\n",
    "prompt = weave.StringPrompt(\"Solve the equation {equation}\")\n",
    "weave.publish(prompt, name=\"calculator_prompt\")\n",
    "\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "  model=model_name,\n",
    "  messages=[\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": prompt.format(equation=\"1 + 1 = ?\")\n",
    "    }\n",
    "  ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0c873438"
   },
   "source": [
    "## „É¢„Éá„É´„Éà„É©„ÉÉ„Ç≠„É≥„Ç∞\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e3e60d81",
    "outputId": "f0fb3219-66be-4048-e5a2-3db0c047a651"
   },
   "outputs": [],
   "source": [
    "class WandBInferenceGrammarCorrector(weave.Model):\n",
    "    # „Éó„É≠„Éë„ÉÜ„Ç£„ÅØÂÆåÂÖ®„Å´„É¶„Éº„Ç∂„ÉºÂÆöÁæ©\n",
    "    model_name: str\n",
    "    system_message: str\n",
    "\n",
    "    @weave.op()\n",
    "    def predict(self, user_input):\n",
    "\n",
    "        response = client.chat.completions.create(\n",
    "            model=self.model_name,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": self.system_message},\n",
    "                {\"role\": \"user\", \"content\": user_input},\n",
    "            ],\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "\n",
    "corrector = WandBInferenceGrammarCorrector(\n",
    "    model_name=model_name,\n",
    "    system_message=\"You are a grammar checker, correct the following user input.\",\n",
    ")\n",
    "\n",
    "\n",
    "result = corrector.predict(\"     That was so easy, it was a piece of pie!       \")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "624b04c1"
   },
   "source": [
    "## „Éá„Éº„Çø„Çª„ÉÉ„Éà„Éà„É©„ÉÉ„Ç≠„É≥„Ç∞\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ac2c5145",
    "outputId": "2918a900-8263-49e8-d784-4c35cde32407"
   },
   "outputs": [],
   "source": [
    "dataset = weave.Dataset(\n",
    "    name=\"grammar-correction\",\n",
    "    rows=[\n",
    "        {\n",
    "            \"user_input\": \"   That was so easy, it was a piece of pie!   \",\n",
    "            \"expected\": \"That was so easy, it was a piece of cake!\",\n",
    "        },\n",
    "        {\"user_input\": \"  I write good   \",\n",
    "         \"expected\": \"I write well\"},\n",
    "        {\n",
    "            \"user_input\": \"  GPT-3 is smartest AI model.   \",\n",
    "            \"expected\": \"GPT-3 is the smartest AI model.\",\n",
    "        },\n",
    "    ],\n",
    ")\n",
    "\n",
    "weave.publish(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "71c3aaea"
   },
   "source": [
    "## Retrieve Published Objects & Ops\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "caa5c5e0",
    "outputId": "57c8c724-9748-4892-864c-8f189b664a81"
   },
   "outputs": [],
   "source": [
    "ref_url = \"weave:///wandb-smle/weave-handson/object/pirate_prompt:GDp90qF5SsaoFgeMqEliU2e99AXxKDZwBub08BVipRQ\"\n",
    "prompt = weave.ref(ref_url).get()\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "77b94dd5"
   },
   "source": [
    "# „Ç™„Éï„É©„Ç§„É≥Ë©ï‰æ°\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "72bdf072"
   },
   "source": [
    "## ÊñπÊ≥ï1: [Ê®ôÊ∫ñ„É°„ÇΩ„ÉÉ„Éâ](https://weave-docs.wandb.ai/guides/core-types/evaluations)\n",
    "‰∫àÊ∏¨„Å®Ë©ï‰æ°„ÅÆ‰∏°Êñπ„Çí„Çµ„É≥„Éó„É´„Åî„Å®„Å´ÂÆüË°å„Åó„ÄÅË©ï‰æ°„Çí„Åó„Åæ„Åô„ÄÇ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 148
    },
    "id": "fpiwHM4WdV9A",
    "outputId": "f1cd62cf-422b-4de7-8990-a24b8a9f8f0b"
   },
   "outputs": [],
   "source": [
    "from weave import Evaluation, Model\n",
    "import weave\n",
    "import asyncio\n",
    "import openai\n",
    "import os\n",
    "\n",
    "\n",
    "examples = [\n",
    "    {\"question\": \"What is the capital of France?\", \"expected\": \"Paris\"},\n",
    "    {\"question\": \"Who wrote 'To Kill a Mockingbird'?\", \"expected\": \"Harper Lee\"},\n",
    "    {\"question\": \"What is the square root of 64?\", \"expected\": \"8\"},\n",
    "]\n",
    "\n",
    "@weave.op()\n",
    "def exact_match_scorer(expected: str, output: dict) -> dict:\n",
    "    \"\"\"Exact string matching evaluation\"\"\"\n",
    "    generated = output.get('generated_text', '')\n",
    "    return {'exact_match': expected.lower().strip() == generated.lower().strip()}\n",
    "\n",
    "@weave.op()\n",
    "def contains_answer_scorer(expected: str, output: dict) -> dict:\n",
    "    \"\"\"Check if the expected answer is contained in the generated text\"\"\"\n",
    "    generated = output.get('generated_text', '').lower()\n",
    "    expected_lower = expected.lower()\n",
    "    return {'contains_answer': expected_lower in generated}\n",
    "\n",
    "class OpenAIQAModel(Model):\n",
    "    model_name: str = \"gpt-4o-mini\"\n",
    "    temperature: float = 0.3\n",
    "    max_tokens: int = 100\n",
    "\n",
    "    @weave.op()\n",
    "    def predict(self, question: str):\n",
    "        client = openai.OpenAI()\n",
    "        \"\"\"Generate answer using OpenAI API\"\"\"\n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=self.model_name,\n",
    "                messages=[\n",
    "                    {\n",
    "                        \"role\": \"system\",\n",
    "                        \"content\": \"You are a helpful assistant that provides accurate and concise answers to questions. Please include only answer. Don't include other words\"\n",
    "                    },\n",
    "                    {\n",
    "                        \"role\": \"user\", \n",
    "                        \"content\": f\"Question: {question}\"\n",
    "                    }\n",
    "                ],\n",
    "                temperature=self.temperature,\n",
    "                max_tokens=self.max_tokens\n",
    "            )\n",
    "            \n",
    "            answer = response.choices[0].message.content.strip()\n",
    "            return {\n",
    "                'generated_text': answer,\n",
    "                'model': self.model_name,\n",
    "                'question': question\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error calling OpenAI API: {e}\")\n",
    "            return {\n",
    "                'generated_text': f\"Error: {str(e)}\",\n",
    "                'model': self.model_name,\n",
    "                'question': question\n",
    "            }\n",
    "\n",
    "# Create model and evaluation\n",
    "model = OpenAIQAModel()\n",
    "evaluation = Evaluation(\n",
    "    dataset=examples, \n",
    "    scorers=[exact_match_scorer, contains_answer_scorer]\n",
    ")\n",
    "\n",
    "# Run evaluation\n",
    "await evaluation.evaluate(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v6VE09bweQzu"
   },
   "source": [
    "## ÊñπÊ≥ï2: [EvaluationLogger](https://weave-docs.wandb.ai/guides/evaluation/evaluation_logger)\n",
    "„Éê„ÉÉ„ÉÅ‰∫àÊ∏¨„ÅåÈÅ©Áî®„Åß„Åç„Åæ„Åô„ÄÇ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5jeULyjweeBI",
    "outputId": "fa75ee71-71f9-4fbf-b211-6206714f01ce"
   },
   "outputs": [],
   "source": [
    "import weave\n",
    "from weave import EvaluationLogger\n",
    "\n",
    "# Initialize the logger with optional metadata\n",
    "eval_logger = EvaluationLogger(\n",
    "    model=\"my_local_model\",\n",
    "    dataset=\"my_dataset\"\n",
    ")\n",
    "\n",
    "# Example input data\n",
    "eval_samples = [\n",
    "    {'inputs': {'a': 1, 'b': 2}, 'expected': 3},\n",
    "    {'inputs': {'a': 2, 'b': 3}, 'expected': 5},\n",
    "    {'inputs': {'a': 3, 'b': 4}, 'expected': 7},\n",
    "]\n",
    "\n",
    "# Local model logic: simply add the numbers\n",
    "@weave.op\n",
    "def user_model(a: int, b: int) -> int:\n",
    "    return a + b\n",
    "# Example model logic using OpenAI\n",
    "@weave.op\n",
    "def user_model(a: int, b: int) -> int:\n",
    "    oai = openai.OpenAI()\n",
    "    response = oai.chat.completions.create(\n",
    "        messages=[{\"role\": \"user\", \"content\": f\"What is {a}+{b}?\"}],\n",
    "        model=\"gpt-4o-mini\"\n",
    "    )\n",
    "    # Use the response in some way (here we just return a + b for simplicity)\n",
    "    return a + b\n",
    "\n",
    "# Iterate through examples, predict, and log\n",
    "for sample in eval_samples:\n",
    "    inputs = sample[\"inputs\"]\n",
    "    model_output = user_model(**inputs) # Pass inputs as kwargs\n",
    "\n",
    "    # Log the prediction input and output\n",
    "    pred_logger = eval_logger.log_prediction(\n",
    "        inputs=inputs,\n",
    "        output=model_output\n",
    "    )\n",
    "\n",
    "    # Calculate and log a score for this prediction\n",
    "    expected = sample[\"expected\"]\n",
    "    correctness_score = model_output == expected\n",
    "    pred_logger.log_score(\n",
    "        scorer=\"correctness\", # Simple string name for the scorer\n",
    "        score=correctness_score\n",
    "    )\n",
    "\n",
    "    # Finish logging for this specific prediction\n",
    "    pred_logger.finish()\n",
    "\n",
    "# Log a final summary for the entire evaluation.\n",
    "# Weave auto-aggregates the 'correctness' scores logged above.\n",
    "summary_stats = {\"subjective_overall_score\": 0.8}\n",
    "eval_logger.log_summary(summary_stats)\n",
    "\n",
    "print(\"Evaluation logging complete. View results in the Weave UI.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lk4DY3RqkKnf"
   },
   "source": [
    "# „Ç™„É≥„É©„Ç§„É≥Ë©ï‰æ°"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9ZRRHZyaDdpu"
   },
   "source": [
    "# „Éï„Ç£„Éº„Éâ„Éê„ÉÉ„ÇØ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 356
    },
    "id": "kLBVpBlkxVdi",
    "outputId": "88608be4-dd3b-4609-dd4d-f160fe021222"
   },
   "outputs": [],
   "source": [
    "client = weave.init(PROJECT)\n",
    "call = client.get_call(\"\") #@param\n",
    "\n",
    "# Adding an emoji reaction\n",
    "call.feedback.add_reaction(\"üëç\")\n",
    "\n",
    "# Adding a note\n",
    "call.feedback.add_note(\"this is a note\")\n",
    "\n",
    "# Adding custom key/value pairs.\n",
    "# The first argument is a user-defined \"type\" string.\n",
    "# Feedback must be JSON serializable and less than 1 KB when serialized.\n",
    "call.feedback.add(\"correctness\", { \"value\": 5 })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M497nWagx0PP"
   },
   "source": [
    "# Scorers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d0tqUV9_1KgA"
   },
   "outputs": [],
   "source": [
    "import weave\n",
    "from weave import Scorer\n",
    "\n",
    "\n",
    "class LengthScorer(Scorer):\n",
    "    @weave.op\n",
    "    def score(self, output: str) -> dict:\n",
    "        \"\"\"A simple scorer that checks output length.\"\"\"\n",
    "        return {\n",
    "            \"length\": len(output),\n",
    "            \"is_short\": len(output) < 100\n",
    "        }\n",
    "\n",
    "@weave.op\n",
    "def generate_text(prompt: str) -> str:\n",
    "    return \"Hello, world!\"\n",
    "\n",
    "# Get both result and Call object\n",
    "result, call = generate_text.call(\"Say hello\")\n",
    "\n",
    "# Now you can apply scorers\n",
    "await call.apply_scorer(LengthScorer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SfipbjSi2dee"
   },
   "source": [
    "## „Ç¨„Éº„Éâ„É¨„Éº„É´„Å®„Åó„Å¶„ÅÆScorers„ÅÆ‰ΩøÁî®\n",
    "„Ç¨„Éº„Éâ„É¨„Éº„É´„ÅØ„ÄÅLLMÂá∫Âäõ„Åå„É¶„Éº„Ç∂„Éº„Å´Âà∞ÈÅî„Åô„ÇãÂâç„Å´ÂÆüË°å„Åï„Çå„ÇãÂÆâÂÖ®„ÉÅ„Çß„ÉÉ„ÇØ„Å®„Åó„Å¶Ê©üËÉΩ„Åó„Åæ„Åô„ÄÇÂÆüÁî®ÁöÑ„Å™‰æã„Çí‰ª•‰∏ã„Å´Á§∫„Åó„Åæ„ÅôÔºö"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r9rKr-_g2YLk"
   },
   "outputs": [],
   "source": [
    "import weave\n",
    "from weave import Scorer\n",
    "import asyncio\n",
    "import nest_asyncio  # Required for running asyncio in Google Colab\n",
    "\n",
    "# Apply nest_asyncio to avoid event loop issues in Google Colab\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# ==== 1. Define text generation function ====\n",
    "@weave.op\n",
    "def generate_text(prompt: str) -> str:\n",
    "    \"\"\"Simulated LLM text generation (basic logic).\"\"\"\n",
    "    responses = {\n",
    "        \"hello\": \"Hello! How can I help you?\",\n",
    "        \"bad\": \"You are terrible!\",  # Example of toxic response\n",
    "        \"good\": \"You are wonderful!\",\n",
    "    }\n",
    "    return responses.get(prompt.lower(), \"I don't understand your request.\")\n",
    "\n",
    "# ==== 2. Define the Toxicity Scorer ====\n",
    "class ToxicityScorer(Scorer):\n",
    "    @weave.op\n",
    "    def score(self, output: str) -> dict:\n",
    "        \"\"\"\n",
    "        Evaluate the generated content for toxic language.\n",
    "        \"\"\"\n",
    "        toxic_words = {\"terrible\", \"hate\", \"stupid\"}  # Simple keyword-based detection\n",
    "        flagged = any(word in output.lower() for word in toxic_words)\n",
    "\n",
    "        return {\n",
    "            \"flagged\": flagged,\n",
    "            \"reason\": \"Detected toxic language\" if flagged else None\n",
    "        }\n",
    "\n",
    "# ==== 3. Function to generate safe responses ====\n",
    "async def generate_safe_response(prompt: str) -> str:\n",
    "    # Generate text using LLM\n",
    "    result, call = generate_text.call(prompt)\n",
    "\n",
    "    # Apply toxicity scoring\n",
    "    safety = await call.apply_scorer(ToxicityScorer())\n",
    "\n",
    "    # If flagged as toxic, return a warning message\n",
    "    if safety.result[\"flagged\"]:\n",
    "        return f\"I cannot generate that content: {safety.result['reason']}\"\n",
    "\n",
    "    return result\n",
    "\n",
    "# ==== 4. Run test cases ====\n",
    "async def main():\n",
    "    prompts = [\"hello\", \"bad\", \"good\"]\n",
    "\n",
    "    for prompt in prompts:\n",
    "        response = await generate_safe_response(prompt)\n",
    "        print(f\"Prompt: {prompt}\\nResponse: {response}\\n\")\n",
    "\n",
    "# Run the async function in Google Colab\n",
    "loop = asyncio.get_event_loop()\n",
    "loop.run_until_complete(main())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kdfIOJOGDiL9"
   },
   "source": [
    "## „É¢„Éã„Çø„Éº„Å®„Åó„Å¶„ÅÆScorers„ÅÆ‰ΩøÁî®\n",
    "\n",
    "„É¢„Éã„Çø„Éº„ÅØ„ÄÅÊìç‰Ωú„Çí„Éñ„É≠„ÉÉ„ÇØ„Åô„Çã„Åì„Å®„Å™„Åè„ÄÅÊôÇÈñì„ÅÆÁµåÈÅé„Å®„Å®„ÇÇ„Å´ÂìÅË≥™„É°„Éà„É™„ÇØ„Çπ„ÇíËøΩË∑°„Åô„Çã„ÅÆ„Å´ÂΩπÁ´ã„Å°„Åæ„Åô„ÄÇ„Åì„Çå„ÅØ‰ª•‰∏ã„Å´ÊúâÁî®„Åß„ÅôÔºö\n",
    "\n",
    "* ÂìÅË≥™„Éà„É¨„É≥„Éâ„ÅÆÁâπÂÆö\n",
    "* „É¢„Éá„É´„Éâ„É™„Éï„Éà„ÅÆÊ§úÂá∫\n",
    "* „É¢„Éá„É´ÊîπÂñÑ„ÅÆ„Åü„ÇÅ„ÅÆ„Éá„Éº„ÇøÂèéÈõÜ\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hpEQ3TDS3lVh"
   },
   "outputs": [],
   "source": [
    "import weave\n",
    "from weave import Scorer\n",
    "import json\n",
    "import xml.etree.ElementTree as ET\n",
    "import random\n",
    "import asyncio\n",
    "import nest_asyncio  # Required for Google Colab\n",
    "\n",
    "# Apply nest_asyncio to avoid event loop issues in Google Colab\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# ==== 1. Define Text Generation Function ====\n",
    "@weave.op\n",
    "def generate_text(prompt: str) -> str:\n",
    "    \"\"\"Simulated LLM response generation.\"\"\"\n",
    "    if prompt.lower() == \"json\":\n",
    "        return '{\"message\": \"Hello, world!\"}'  # Valid JSON\n",
    "    elif prompt.lower() == \"xml\":\n",
    "        return \"<message>Hello, world!</message>\"  # Valid XML\n",
    "    else:\n",
    "        return \"Generated response...\"\n",
    "\n",
    "# ==== 2. Custom Scorer for JSON Validation ====\n",
    "class CustomJSONScorer(Scorer):\n",
    "    @weave.op\n",
    "    def score(self, output: str) -> dict:\n",
    "        \"\"\"Check if the output is valid JSON.\"\"\"\n",
    "        try:\n",
    "            json.loads(output)\n",
    "            return {\"valid_json\": True}\n",
    "        except json.JSONDecodeError:\n",
    "            return {\"valid_json\": False}\n",
    "\n",
    "# ==== 3. Custom Scorer for XML Validation ====\n",
    "class CustomXMLScorer(Scorer):\n",
    "    @weave.op\n",
    "    def score(self, output: str) -> dict:\n",
    "        \"\"\"Check if the output is valid XML.\"\"\"\n",
    "        try:\n",
    "            ET.fromstring(output)\n",
    "            return {\"valid_xml\": True}\n",
    "        except ET.ParseError:\n",
    "            return {\"valid_xml\": False}\n",
    "\n",
    "# ==== 4. Function to Generate Response with Monitoring ====\n",
    "async def generate_with_monitoring(prompt: str) -> str:\n",
    "    \"\"\"\n",
    "    Generates a response and applies monitoring (randomly 10% of the time).\n",
    "    \"\"\"\n",
    "    # Generate text and capture tracking info\n",
    "    result, call = generate_text.call(prompt)\n",
    "\n",
    "    # Sample monitoring (only apply scorers to 10% of calls)\n",
    "    if random.random() < 0.1:\n",
    "        # Apply manual scorers asynchronously\n",
    "        json_score = await call.apply_scorer(CustomJSONScorer())\n",
    "        xml_score = await call.apply_scorer(CustomXMLScorer())\n",
    "\n",
    "        print(f\"Monitoring Applied - JSON Valid: {json_score.result['valid_json']}, XML Valid: {xml_score.result['valid_xml']}\")\n",
    "\n",
    "    return result\n",
    "\n",
    "# ==== 5. Run Test Cases ====\n",
    "async def main():\n",
    "    prompts = [\"json\", \"xml\", \"text\"]\n",
    "\n",
    "    for prompt in prompts:\n",
    "        response = await generate_with_monitoring(prompt)\n",
    "        print(f\"Prompt: {prompt}\\nResponse: {response}\\n\")\n",
    "\n",
    "# Run the async function in Google Colab\n",
    "loop = asyncio.get_event_loop()\n",
    "loop.run_until_complete(main())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# „É°„Éá„Ç£„Ç¢"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ÁîªÂÉè"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import weave\n",
    "from openai import OpenAI\n",
    "import requests\n",
    "from PIL import Image\n",
    "\n",
    "weave.init(project_name=PROJECT)\n",
    "#os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
    "client = OpenAI(\n",
    "    api_key=os.environ.get(\"OPENAI_API_KEY\")\n",
    ")\n",
    "\n",
    "@weave.op()\n",
    "def generate_image(prompt: str) -> Image:\n",
    "    response = client.images.generate(\n",
    "        model=\"dall-e-3\",\n",
    "        prompt=prompt,\n",
    "        size=\"1024x1024\",\n",
    "        quality=\"standard\",\n",
    "        n=1,\n",
    "    )\n",
    "    image_url = response.data[0].url\n",
    "    image_response = requests.get(image_url, stream=True)\n",
    "    image = Image.open(image_response.raw)\n",
    "\n",
    "    # return a PIL.Image.Image object to be logged as an image\n",
    "    return image\n",
    "\n",
    "image = generate_image(\"a cat with a pumpkin hat\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Èü≥Â£∞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import weave\n",
    "from openai import OpenAI\n",
    "import wave\n",
    "\n",
    "weave.init(project_name=PROJECT)\n",
    "#os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
    "client = OpenAI(\n",
    "    api_key=os.environ.get(\"OPENAI_API_KEY\")\n",
    ")\n",
    "@weave.op\n",
    "def make_audio_file_streaming(text: str) -> wave.Wave_read:\n",
    "    with client.audio.speech.with_streaming_response.create(\n",
    "        model=\"tts-1\",\n",
    "        voice=\"alloy\",\n",
    "        input=text,\n",
    "        response_format=\"wav\",\n",
    "    ) as res:\n",
    "        res.stream_to_file(\"output.wav\")\n",
    "\n",
    "    # return a wave.Wave_read object to be logged as audio\n",
    "    return wave.open(\"output.wav\")\n",
    "\n",
    "make_audio_file_streaming(\"Hello, how are you? What did you do yesterday?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ÂãïÁîª"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "from moviepy.editor import VideoFileClip, ColorClip, VideoClip\n",
    "import weave\n",
    "\n",
    "weave.init(PROJECT)\n",
    "\n",
    "@weave.op()\n",
    "def store_videos(name, client, generated_video):\n",
    "  client.files.download(file=generated_video.video)\n",
    "  generated_video.video.save(f\"new_video{name}.mp4\")  # save the video\n",
    "\n",
    "@weave.op()\n",
    "def save_videos_in_weave(video_path):\n",
    "    clip = VideoFileClip(video_path, has_mask=False, audio=True)\n",
    "    new_clip = clip.subclip(0, 1)\n",
    "    return new_clip\n",
    "\n",
    "@weave.op()\n",
    "def generate_videos(prompt):\n",
    "    client = genai.Client()  # read API key from GOOGLE_API_KEY\n",
    "    operation = client.models.generate_videos(\n",
    "        model=\"veo-2.0-generate-001\",\n",
    "        prompt=\"Panning wide shot of a calico kitten sleeping in the sunshine\",\n",
    "        config=types.GenerateVideosConfig(\n",
    "            person_generation=\"dont_allow\",  # \"dont_allow\" or \"allow_adult\"\n",
    "            aspect_ratio=\"16:9\",  # \"16:9\" or \"9:16\"\n",
    "            ),\n",
    "        )\n",
    "    \n",
    "    while not operation.done:\n",
    "        time.sleep(10)\n",
    "        operation = client.operations.get(operation)\n",
    "    \n",
    "    for n, generated_video in enumerate(operation.response.generated_videos):\n",
    "        client.files.download(file=generated_video.video)\n",
    "        generated_video.video.save(f\"video{n}.mp4\")  # save the video\n",
    "        save_videos_in_weave(f\"video{n}.mp4\")\n",
    "        \n",
    " \n",
    "generate_videos(\"Panning wide shot of a calico kitten sleeping in the sunshine\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
