{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "974b2c96"
   },
   "source": [
    "# 🏃‍♀️ クイックスタート\n",
    "\n",
    "Weaveの機能を理解していきましょう！\n",
    "\n",
    "# <a href=\"https://colab.research.google.com/drive/1bdymP7p7d4z7izsS-PhMUxXcD38p9Hqr\" target=\"_blank\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Google Colabで開く\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "56dc5c9d"
   },
   "source": [
    "## 🪄 `weave`ライブラリのインストールとログイン\n",
    "\n",
    "\n",
    "ライブラリをインストールし、アカウントにログインすることから始めましょう。\n",
    "\n",
    "この例では、W&B Inferenceを使用するため、Public Cloud環境のWANDB_API_KEYが必要です。Dedicated CloudやOnpremiseを利用されている方は、Public Cloudのkeyも用意してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0921bbef-d089-4960-b624-505172f90d76",
    "outputId": "a262f278-48dc-411f-9fd9-58d69def7db9"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import wandb\n",
    "import weave\n",
    "import openai\n",
    "import json\n",
    "#========================================\n",
    "# 環境変数を適切に設定してください\n",
    "#========================================\n",
    "# os.environ[\"WANDB_BASE_URL\"] = \"https://api.wandb.ai\"\n",
    "os.environ[\"WANDB_API_KEY\"] = \"\"\n",
    "os.environ[\"WANDB_API_KEY_PUBLIC_CLOUD\"] = \"\"  # Public cloudユーザーはWANDB_API_KEYと同じ値を使用して問題ないです。\n",
    "\n",
    "wandb.login()\n",
    "PROJECT = \"<entity>/weave-handson\" # handsonで利用するprojectです。entityのところを置き換えてください。\n",
    "INFERENCE_PROJECT=\"<entity>/<project>\" # Inferenceの利用料トラックのために利用するProjectです。詳しくはREADMEをご確認ください。\n",
    "weave.init(PROJECT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "09467a33"
   },
   "source": [
    "# 関数の入力と出力をトラッキング\n",
    "\n",
    "Weaveにより、ユーザーは関数呼び出しを、コード、入力、出力、さらにはLLMトークンとコストまでトラッキングできます。以下のセクションでは次の内容をカバーします：\n",
    "\n",
    "* カスタム関数\n",
    "* ベンダー統合\n",
    "* ネストした関数呼び出し\n",
    "* エラートラッキング"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "99453d48"
   },
   "source": [
    "## カスタム関数のトラッキング\n",
    "\n",
    "トラッキングしたい関数に@weave.opデコレータを追加します"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cdcd1bb7",
    "outputId": "8c252a02-fc1f-4499-9f4a-7bd4362a2f61"
   },
   "outputs": [],
   "source": [
    "@weave.op()\n",
    "def echo(user_input):\n",
    "    return user_input + \" \" + user_input\n",
    "\n",
    "result = echo(\"hello\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "174cabef"
   },
   "source": [
    "上記の👆 wandbリンクをクリックすることで、インタラクティブダッシュボードを見つけることができます。\n",
    "\n",
    "`weave.op`を追加して関数を呼び出した後、リンクにアクセスして、プロジェクト内でトラッキングされているのを確認してください。\n",
    "\n",
    "💡 コードを自動的にトラッキングしています。コードタブをご覧ください！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5ef6f27b"
   },
   "source": [
    "## Integrationを利用したトラッキング（W&B Inference、OpenAI、Anthropic、Mistralなど）\n",
    "\n",
    "ここでは、`W&B Inference`への全ての呼び出しを自動的にトラッキングしています。Weaveは、多くのLLMライブラリを@weaveなしで自動的にトラッキングしてくれます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# W&B Inference clientの初期化\n",
    "# Public cloudユーザーはWANDB_API_KEYと同じ値をWANDB_API_KEY_PUBLIC_CLOUDとして使用してください\n",
    "client = openai.OpenAI(\n",
    "    base_url='https://api.inference.wandb.ai/v1',\n",
    "    api_key=os.getenv(\"WANDB_API_KEY_PUBLIC_CLOUD\") or os.getenv(\"WANDB_API_KEY\"),\n",
    "    project=INFERENCE_PROJECT,\n",
    ")\n",
    "model_name=\"openai/gpt-oss-20b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trace the model call in Weave\n",
    "def run_chat():\n",
    "    response = client.chat.completions.create(\n",
    "        model=model_name,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": \"ジョークを言ってください。\"}\n",
    "        ],\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# Run and log the traced call\n",
    "output = run_chat()\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "35fe43dc"
   },
   "source": [
    "## Nested関数のトラッキング\n",
    "\n",
    "基本を確認したので、上記のすべてを組み合わせて、Nested関数をトラッキングしてみましょう。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "422050d0",
    "outputId": "9f1f22d6-2592-47ad-c83a-5c08095d632f"
   },
   "outputs": [],
   "source": [
    "@weave.op()\n",
    "def correct_grammar(user_input):\n",
    "    echoed_input = echo(user_input)\n",
    "    response = client.chat.completions.create(\n",
    "        model=model_name,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are a helpful assistant. Please create a song by following the user input.\",\n",
    "            },\n",
    "            {\"role\": \"user\", \"content\": echoed_input},\n",
    "        ],\n",
    "        temperature=0,\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "\n",
    "result = correct_grammar(\"hello\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6f483413"
   },
   "source": [
    "## エラーのトラッキング\n",
    "\n",
    "コードがクラッシュするたびに、weaveは問題の原因をハイライトします。LLMレスポンスからデータを解析する際に時々発生するJSONパースエラーなどを見つけるのに特に有用です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 356
    },
    "id": "14e7e103",
    "outputId": "1a121775-5b5c-4212-f205-d97c0ef7cac2"
   },
   "outputs": [],
   "source": [
    "@weave.op()\n",
    "def correct_grammar(user_input):\n",
    "    echoed_input = echo(user_input)\n",
    "    response = client.chat.completions.create(\n",
    "        model=model_name,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are a helpful assistant. Please create a song by following the user input.\",\n",
    "            },\n",
    "            {\"role\": \"user\", \"content\": echoed_input},\n",
    "        ],\n",
    "        temperature=0,\n",
    "        response_format={\"type\": \"json_object\"},\n",
    "    )\n",
    "    return json.loads(response.choices[0].message.content)\n",
    "\n",
    "\n",
    "result = correct_grammar(\"hello\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "knBo-VtWxl6g"
   },
   "source": [
    "# Traceの高度なTips\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6KWJhiBsxl6h"
   },
   "source": [
    "### サンプリングレートの制御\n",
    "\n",
    "@weave.opデコレータのtracing_sample_rateパラメータを設定することで、opの呼び出しがトレースされる頻度を制御できます。これは、呼び出しのサブセットのみをトレースする必要がある高頻度opに有用です。\n",
    "\n",
    "サンプリングレートはルート呼び出しにのみ適用されることに注意してください。opにサンプルレートがあっても、最初に別のopによって呼び出された場合、そのサンプリングレートは無視されます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cpX43hPbdeOZ"
   },
   "outputs": [],
   "source": [
    "@weave.op(tracing_sample_rate=0.1)  # Only trace ~10% of calls\n",
    "def high_frequency_op(x: int) -> int:\n",
    "    return x + 1\n",
    "\n",
    "@weave.op(tracing_sample_rate=1.0)  # Always trace (default)\n",
    "def always_traced_op(x: int) -> int:\n",
    "    return x + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ntJpevJQeLoA"
   },
   "source": [
    "### 呼び出し表示名"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iqOMqMIseYSV",
    "outputId": "f1fa226c-f099-4c5a-f411-6f4bc0caab19"
   },
   "outputs": [],
   "source": [
    "# Decorate your function\n",
    "@weave.op\n",
    "def my_function(name: str):\n",
    "    return f\"Hello, {name}!\"\n",
    "\n",
    "# Call your function -- Weave will automatically track inputs and outputs\n",
    "print(my_function(\"World\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RV9wwY4veaz8"
   },
   "outputs": [],
   "source": [
    "# 1st method\n",
    "result = my_function(\"World\", __weave={\"display_name\": \"My Custom Display Name\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R8d4mcWserLq",
    "outputId": "1938848a-75f4-487b-d46b-09d4995a66cb"
   },
   "outputs": [],
   "source": [
    "# 2nd method\n",
    "result, call = my_function.call(\"World\")\n",
    "call.set_display_name(\"My Custom Display Name\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "-M2abQSbew9M",
    "outputId": "81b67f69-507c-46e1-9d9b-3ae52d23c762"
   },
   "outputs": [],
   "source": [
    "# 3rd method\n",
    "@weave.op(call_display_name=\"My Custom Display Name\")\n",
    "def my_function(name: str):\n",
    "    return f\"Hello, {name}!\"\n",
    "\n",
    "my_function(\"World\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ExWp8BFIxl6g"
   },
   "source": [
    "\n",
    "### PIIの編集\n",
    "\n",
    "一部の組織では、大規模言語モデル（LLM）ワークフローで名前、電話番号、メールアドレスなどの個人識別情報（PII）を処理します。このデータをWeights & Biases（W&B）Weaveに保存することは、コンプライアンスとセキュリティのリスクをもたらします。\n",
    "\n",
    "機密データ保護機能により、トレースがWeaveサーバーに送信される前に、個人識別情報（PII）を自動的に編集できます。この機能はMicrosoft PresidioをWeave Python SDKに統合しており、SDK レベルで編集設定を制御できることを意味します。\n",
    "\n",
    "[詳細ドキュメント](https://weave-docs.wandb.ai/guides/tracking/redact-pii)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f957c9c8"
   },
   "source": [
    "# オブジェクトのトラッキング\n",
    "\n",
    "Weaveでは、システムプロンプトや使用しているモデル、データセットなどのアセットを`weave.Objects`内でバージョン管理できます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8f6129cc"
   },
   "source": [
    "## プロンプトトラッキング"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "08e4e852",
    "outputId": "d5b8c38a-0f00-4e9b-a09f-268f1ea1eb1c"
   },
   "outputs": [],
   "source": [
    "# StringPrompt1\n",
    "system_prompt = weave.StringPrompt(\"You are a pirate\")\n",
    "weave.publish(system_prompt, name=\"pirate_prompt\")\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "  model=model_name,\n",
    "  messages=[\n",
    "    {\n",
    "      \"role\": \"system\",\n",
    "      \"content\": system_prompt.format()\n",
    "    },\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": \"Explain general relativity in one paragraph.\"\n",
    "    }\n",
    "  ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ke21Y2OJg9Uo",
    "outputId": "f821bf2a-0577-4c96-9661-df18e4eb1524"
   },
   "outputs": [],
   "source": [
    "# StringPrompt2\n",
    "system_prompt = weave.StringPrompt(\"Talk like a pirate. I need to know I'm listening to a pirate.\")\n",
    "weave.publish(system_prompt, name=\"pirate_prompt\")\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "  model=model_name,\n",
    "  messages=[\n",
    "    {\n",
    "      \"role\": \"system\",\n",
    "      \"content\": system_prompt.format()\n",
    "    },\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": \"Explain general relativity in one paragraph.\"\n",
    "    }\n",
    "  ],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZrpZa01lg_-D",
    "outputId": "661423d9-9a58-4635-bce8-4f360838706d"
   },
   "outputs": [],
   "source": [
    "# MessagesPrompt1\n",
    "prompt = weave.MessagesPrompt([\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a stegosaurus, but don't be too obvious about it.\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"What's good to eat around here?\"\n",
    "    }\n",
    "])\n",
    "weave.publish(prompt, name=\"dino_prompt\")\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "  model=\"gpt-4o\",\n",
    "  messages=prompt.format(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WY5UZLgIhHYW",
    "outputId": "7a1cb983-044b-4d14-81fa-941493c870a0"
   },
   "outputs": [],
   "source": [
    "# parameterizing prompts\n",
    "prompt = weave.StringPrompt(\"Solve the equation {equation}\")\n",
    "weave.publish(prompt, name=\"calculator_prompt\")\n",
    "\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "  model=model_name,\n",
    "  messages=[\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": prompt.format(equation=\"1 + 1 = ?\")\n",
    "    }\n",
    "  ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0c873438"
   },
   "source": [
    "## モデルトラッキング\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e3e60d81",
    "outputId": "f0fb3219-66be-4048-e5a2-3db0c047a651"
   },
   "outputs": [],
   "source": [
    "class WandBInferenceGrammarCorrector(weave.Model):\n",
    "    # プロパティは完全にユーザー定義\n",
    "    model_name: str\n",
    "    system_message: str\n",
    "\n",
    "    @weave.op()\n",
    "    def predict(self, user_input):\n",
    "\n",
    "        response = client.chat.completions.create(\n",
    "            model=self.model_name,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": self.system_message},\n",
    "                {\"role\": \"user\", \"content\": user_input},\n",
    "            ],\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "\n",
    "corrector = WandBInferenceGrammarCorrector(\n",
    "    model_name=model_name,\n",
    "    system_message=\"You are a grammar checker, correct the following user input.\",\n",
    ")\n",
    "\n",
    "\n",
    "result = corrector.predict(\"     That was so easy, it was a piece of pie!       \")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "624b04c1"
   },
   "source": [
    "## データセットトラッキング\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ac2c5145",
    "outputId": "2918a900-8263-49e8-d784-4c35cde32407"
   },
   "outputs": [],
   "source": [
    "dataset = weave.Dataset(\n",
    "    name=\"grammar-correction\",\n",
    "    rows=[\n",
    "        {\n",
    "            \"user_input\": \"   That was so easy, it was a piece of pie!   \",\n",
    "            \"expected\": \"That was so easy, it was a piece of cake!\",\n",
    "        },\n",
    "        {\"user_input\": \"  I write good   \",\n",
    "         \"expected\": \"I write well\"},\n",
    "        {\n",
    "            \"user_input\": \"  GPT-3 is smartest AI model.   \",\n",
    "            \"expected\": \"GPT-3 is the smartest AI model.\",\n",
    "        },\n",
    "    ],\n",
    ")\n",
    "\n",
    "weave.publish(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "71c3aaea"
   },
   "source": [
    "## Retrieve Published Objects & Ops\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "caa5c5e0",
    "outputId": "57c8c724-9748-4892-864c-8f189b664a81"
   },
   "outputs": [],
   "source": [
    "ref_url = \"\"\n",
    "prompt = weave.ref(ref_url).get()\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "77b94dd5"
   },
   "source": [
    "# オフライン評価\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "72bdf072"
   },
   "source": [
    "## 方法1: [標準メソッド](https://weave-docs.wandb.ai/guides/core-types/evaluations)\n",
    "予測と評価の両方をサンプルごとに実行し、評価をします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 148
    },
    "id": "fpiwHM4WdV9A",
    "outputId": "f1cd62cf-422b-4de7-8990-a24b8a9f8f0b"
   },
   "outputs": [],
   "source": [
    "# Method1\n",
    "import weave\n",
    "from weave import Evaluation\n",
    "\n",
    "# Our dataset has \"input_text\" but our model expects \"question\"\n",
    "examples = [\n",
    "    {\"input_text\": \"What is the capital of France?\", \"expected\": \"Paris\"},\n",
    "    {\"input_text\": \"Who wrote 'To Kill a Mockingbird'?\", \"expected\": \"Harper Lee\"},\n",
    "    {\"input_text\": \"What is the square root of 64?\", \"expected\": \"8\"},\n",
    "]\n",
    "\n",
    "@weave.op()\n",
    "def preprocess_example(example):\n",
    "    # Rename input_text to question\n",
    "    return {\n",
    "        \"question\": example[\"input_text\"]\n",
    "    }\n",
    "\n",
    "@weave.op()\n",
    "def match_score(expected: str, output: dict) -> dict:\n",
    "    return {'match': expected == output['generated_text']}\n",
    "\n",
    "@weave.op()\n",
    "def function_to_evaluate(question: str):\n",
    "    return {'generated_text': f'Answer to: {question}'}\n",
    "\n",
    "# Create evaluation with preprocessing\n",
    "evaluation = weave.Evaluation(\n",
    "    dataset=examples,\n",
    "    scorers=[match_score],\n",
    "    preprocess_model_input=preprocess_example\n",
    ")\n",
    "\n",
    "# Initialize the Weave project\n",
    "weave.init(PROJECT)\n",
    "\n",
    "# In Jupyter/Colab, use 'await' instead of 'asyncio.run'\n",
    "async def run_eval():\n",
    "    await evaluation.evaluate(function_to_evaluate)\n",
    "\n",
    "await run_eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v6VE09bweQzu"
   },
   "source": [
    "## 方法2: [EvaluationLogger](https://weave-docs.wandb.ai/guides/evaluation/evaluation_logger)\n",
    "バッチ予測が適用できます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5jeULyjweeBI",
    "outputId": "fa75ee71-71f9-4fbf-b211-6206714f01ce"
   },
   "outputs": [],
   "source": [
    "import weave\n",
    "from weave.flow.eval_imperative import EvaluationLogger\n",
    "\n",
    "# Initialize the logger with optional metadata\n",
    "eval_logger = EvaluationLogger(\n",
    "    model=\"my_local_model\",\n",
    "    dataset=\"my_dataset\"\n",
    ")\n",
    "\n",
    "# Example input data\n",
    "eval_samples = [\n",
    "    {'inputs': {'a': 1, 'b': 2}, 'expected': 3},\n",
    "    {'inputs': {'a': 2, 'b': 3}, 'expected': 5},\n",
    "    {'inputs': {'a': 3, 'b': 4}, 'expected': 7},\n",
    "]\n",
    "\n",
    "# Local model logic: simply add the numbers\n",
    "@weave.op\n",
    "def user_model(a: int, b: int) -> int:\n",
    "    return a + b\n",
    "\n",
    "# Evaluate each sample\n",
    "for sample in eval_samples:\n",
    "    inputs = sample[\"inputs\"]\n",
    "    model_output = user_model(**inputs)  # Call model with unpacked input\n",
    "\n",
    "    # Log the prediction\n",
    "    pred_logger = eval_logger.log_prediction(\n",
    "        inputs=inputs,\n",
    "        output=model_output\n",
    "    )\n",
    "\n",
    "    # Compare output with expected value\n",
    "    expected = sample[\"expected\"]\n",
    "    correctness_score = model_output == expected\n",
    "    pred_logger.log_score(\n",
    "        scorer=\"correctness\",\n",
    "        score=correctness_score\n",
    "    )\n",
    "\n",
    "    # Finalize log for this prediction\n",
    "    pred_logger.finish()\n",
    "\n",
    "# Log overall evaluation summary\n",
    "summary_stats = {\"subjective_overall_score\": 1.0}\n",
    "eval_logger.log_summary(summary_stats)\n",
    "\n",
    "print(\"Evaluation logging complete. View results in the Weave UI.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lk4DY3RqkKnf"
   },
   "source": [
    "# オンライン評価"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9ZRRHZyaDdpu"
   },
   "source": [
    "# フィードバック"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 356
    },
    "id": "kLBVpBlkxVdi",
    "outputId": "88608be4-dd3b-4609-dd4d-f160fe021222"
   },
   "outputs": [],
   "source": [
    "client = weave.init(PROJECT)\n",
    "call = client.get_call(\"\") #@param\n",
    "\n",
    "# Adding an emoji reaction\n",
    "call.feedback.add_reaction(\"👍\")\n",
    "\n",
    "# Adding a note\n",
    "call.feedback.add_note(\"this is a note\")\n",
    "\n",
    "# Adding custom key/value pairs.\n",
    "# The first argument is a user-defined \"type\" string.\n",
    "# Feedback must be JSON serializable and less than 1 KB when serialized.\n",
    "call.feedback.add(\"correctness\", { \"value\": 5 })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M497nWagx0PP"
   },
   "source": [
    "# Scorers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d0tqUV9_1KgA"
   },
   "outputs": [],
   "source": [
    "import weave\n",
    "from weave import Scorer\n",
    "\n",
    "\n",
    "class LengthScorer(Scorer):\n",
    "    @weave.op\n",
    "    def score(self, output: str) -> dict:\n",
    "        \"\"\"A simple scorer that checks output length.\"\"\"\n",
    "        return {\n",
    "            \"length\": len(output),\n",
    "            \"is_short\": len(output) < 100\n",
    "        }\n",
    "\n",
    "@weave.op\n",
    "def generate_text(prompt: str) -> str:\n",
    "    return \"Hello, world!\"\n",
    "\n",
    "# Get both result and Call object\n",
    "result, call = generate_text.call(\"Say hello\")\n",
    "\n",
    "# Now you can apply scorers\n",
    "await call.apply_scorer(LengthScorer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SfipbjSi2dee"
   },
   "source": [
    "## ガードレールとしてのScorersの使用\n",
    "ガードレールは、LLM出力がユーザーに到達する前に実行される安全チェックとして機能します。実用的な例を以下に示します："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r9rKr-_g2YLk"
   },
   "outputs": [],
   "source": [
    "import weave\n",
    "from weave import Scorer\n",
    "import asyncio\n",
    "import nest_asyncio  # Required for running asyncio in Google Colab\n",
    "\n",
    "# Apply nest_asyncio to avoid event loop issues in Google Colab\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# ==== 1. Define text generation function ====\n",
    "@weave.op\n",
    "def generate_text(prompt: str) -> str:\n",
    "    \"\"\"Simulated LLM text generation (basic logic).\"\"\"\n",
    "    responses = {\n",
    "        \"hello\": \"Hello! How can I help you?\",\n",
    "        \"bad\": \"You are terrible!\",  # Example of toxic response\n",
    "        \"good\": \"You are wonderful!\",\n",
    "    }\n",
    "    return responses.get(prompt.lower(), \"I don't understand your request.\")\n",
    "\n",
    "# ==== 2. Define the Toxicity Scorer ====\n",
    "class ToxicityScorer(Scorer):\n",
    "    @weave.op\n",
    "    def score(self, output: str) -> dict:\n",
    "        \"\"\"\n",
    "        Evaluate the generated content for toxic language.\n",
    "        \"\"\"\n",
    "        toxic_words = {\"terrible\", \"hate\", \"stupid\"}  # Simple keyword-based detection\n",
    "        flagged = any(word in output.lower() for word in toxic_words)\n",
    "\n",
    "        return {\n",
    "            \"flagged\": flagged,\n",
    "            \"reason\": \"Detected toxic language\" if flagged else None\n",
    "        }\n",
    "\n",
    "# ==== 3. Function to generate safe responses ====\n",
    "async def generate_safe_response(prompt: str) -> str:\n",
    "    # Generate text using LLM\n",
    "    result, call = generate_text.call(prompt)\n",
    "\n",
    "    # Apply toxicity scoring\n",
    "    safety = await call.apply_scorer(ToxicityScorer())\n",
    "\n",
    "    # If flagged as toxic, return a warning message\n",
    "    if safety.result[\"flagged\"]:\n",
    "        return f\"I cannot generate that content: {safety.result['reason']}\"\n",
    "\n",
    "    return result\n",
    "\n",
    "# ==== 4. Run test cases ====\n",
    "async def main():\n",
    "    prompts = [\"hello\", \"bad\", \"good\"]\n",
    "\n",
    "    for prompt in prompts:\n",
    "        response = await generate_safe_response(prompt)\n",
    "        print(f\"Prompt: {prompt}\\nResponse: {response}\\n\")\n",
    "\n",
    "# Run the async function in Google Colab\n",
    "loop = asyncio.get_event_loop()\n",
    "loop.run_until_complete(main())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kdfIOJOGDiL9"
   },
   "source": [
    "## モニターとしてのScorersの使用\n",
    "\n",
    "モニターは、操作をブロックすることなく、時間の経過とともに品質メトリクスを追跡するのに役立ちます。これは以下に有用です：\n",
    "\n",
    "* 品質トレンドの特定\n",
    "* モデルドリフトの検出\n",
    "* モデル改善のためのデータ収集\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hpEQ3TDS3lVh"
   },
   "outputs": [],
   "source": [
    "import weave\n",
    "from weave import Scorer\n",
    "import json\n",
    "import xml.etree.ElementTree as ET\n",
    "import random\n",
    "import asyncio\n",
    "import nest_asyncio  # Required for Google Colab\n",
    "\n",
    "# Apply nest_asyncio to avoid event loop issues in Google Colab\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# ==== 1. Define Text Generation Function ====\n",
    "@weave.op\n",
    "def generate_text(prompt: str) -> str:\n",
    "    \"\"\"Simulated LLM response generation.\"\"\"\n",
    "    if prompt.lower() == \"json\":\n",
    "        return '{\"message\": \"Hello, world!\"}'  # Valid JSON\n",
    "    elif prompt.lower() == \"xml\":\n",
    "        return \"<message>Hello, world!</message>\"  # Valid XML\n",
    "    else:\n",
    "        return \"Generated response...\"\n",
    "\n",
    "# ==== 2. Custom Scorer for JSON Validation ====\n",
    "class CustomJSONScorer(Scorer):\n",
    "    @weave.op\n",
    "    def score(self, output: str) -> dict:\n",
    "        \"\"\"Check if the output is valid JSON.\"\"\"\n",
    "        try:\n",
    "            json.loads(output)\n",
    "            return {\"valid_json\": True}\n",
    "        except json.JSONDecodeError:\n",
    "            return {\"valid_json\": False}\n",
    "\n",
    "# ==== 3. Custom Scorer for XML Validation ====\n",
    "class CustomXMLScorer(Scorer):\n",
    "    @weave.op\n",
    "    def score(self, output: str) -> dict:\n",
    "        \"\"\"Check if the output is valid XML.\"\"\"\n",
    "        try:\n",
    "            ET.fromstring(output)\n",
    "            return {\"valid_xml\": True}\n",
    "        except ET.ParseError:\n",
    "            return {\"valid_xml\": False}\n",
    "\n",
    "# ==== 4. Function to Generate Response with Monitoring ====\n",
    "async def generate_with_monitoring(prompt: str) -> str:\n",
    "    \"\"\"\n",
    "    Generates a response and applies monitoring (randomly 10% of the time).\n",
    "    \"\"\"\n",
    "    # Generate text and capture tracking info\n",
    "    result, call = generate_text.call(prompt)\n",
    "\n",
    "    # Sample monitoring (only apply scorers to 10% of calls)\n",
    "    if random.random() < 0.1:\n",
    "        # Apply manual scorers asynchronously\n",
    "        json_score = await call.apply_scorer(CustomJSONScorer())\n",
    "        xml_score = await call.apply_scorer(CustomXMLScorer())\n",
    "\n",
    "        print(f\"Monitoring Applied - JSON Valid: {json_score.result['valid_json']}, XML Valid: {xml_score.result['valid_xml']}\")\n",
    "\n",
    "    return result\n",
    "\n",
    "# ==== 5. Run Test Cases ====\n",
    "async def main():\n",
    "    prompts = [\"json\", \"xml\", \"text\"]\n",
    "\n",
    "    for prompt in prompts:\n",
    "        response = await generate_with_monitoring(prompt)\n",
    "        print(f\"Prompt: {prompt}\\nResponse: {response}\\n\")\n",
    "\n",
    "# Run the async function in Google Colab\n",
    "loop = asyncio.get_event_loop()\n",
    "loop.run_until_complete(main())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# メディア"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 画像"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import weave\n",
    "from openai import OpenAI\n",
    "import requests\n",
    "from PIL import Image\n",
    "\n",
    "weave.init(project_name=PROJECT)\n",
    "client = OpenAI()\n",
    "\n",
    "@weave.op()\n",
    "def generate_image(prompt: str) -> Image:\n",
    "    response = client.images.generate(\n",
    "        model=\"dall-e-3\",\n",
    "        prompt=prompt,\n",
    "        size=\"1024x1024\",\n",
    "        quality=\"standard\",\n",
    "        n=1,\n",
    "    )\n",
    "    image_url = response.data[0].url\n",
    "    image_response = requests.get(image_url, stream=True)\n",
    "    image = Image.open(image_response.raw)\n",
    "\n",
    "    # return a PIL.Image.Image object to be logged as an image\n",
    "    return image\n",
    "\n",
    "image = generate_image(\"a cat with a pumpkin hat\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 音声"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import weave\n",
    "from openai import OpenAI\n",
    "import wave\n",
    "\n",
    "weave.init(project_name=PROJECT)\n",
    "client = OpenAI()\n",
    "\n",
    "@weave.op\n",
    "def make_audio_file_streaming(text: str) -> wave.Wave_read:\n",
    "    with client.audio.speech.with_streaming_response.create(\n",
    "        model=\"tts-1\",\n",
    "        voice=\"alloy\",\n",
    "        input=text,\n",
    "        response_format=\"wav\",\n",
    "    ) as res:\n",
    "        res.stream_to_file(\"output.wav\")\n",
    "\n",
    "    # return a wave.Wave_read object to be logged as audio\n",
    "    return wave.open(\"output.wav\")\n",
    "\n",
    "make_audio_file_streaming(\"Hello, how are you? What did you do yesterday?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 動画"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "from moviepy.editor import VideoFileClip, ColorClip, VideoClip\n",
    "import weave\n",
    "\n",
    "weave.init(PROJECT)\n",
    "\n",
    "@weave.op()\n",
    "def store_videos(name, client, generated_video):\n",
    "  client.files.download(file=generated_video.video)\n",
    "  generated_video.video.save(f\"new_video{name}.mp4\")  # save the video\n",
    "\n",
    "@weave.op()\n",
    "def save_videos_in_weave(video_path):\n",
    "    VideoFileClip(video_path, has_mask=False, audio=True)\n",
    "    new_clip = clip.subclip(0, 1)\n",
    "    return new_clip\n",
    "\n",
    "@weave.op()\n",
    "def generate_videos(prompt):\n",
    "    client = genai.Client()  # read API key from GOOGLE_API_KEY\n",
    "    operation = client.models.generate_videos(\n",
    "        model=\"veo-2.0-generate-001\",\n",
    "        prompt=\"Panning wide shot of a calico kitten sleeping in the sunshine\",\n",
    "        config=types.GenerateVideosConfig(\n",
    "            person_generation=\"dont_allow\",  # \"dont_allow\" or \"allow_adult\"\n",
    "            aspect_ratio=\"16:9\",  # \"16:9\" or \"9:16\"\n",
    "            ),\n",
    "        )\n",
    "    \n",
    "    while not operation.done:\n",
    "        time.sleep(10)\n",
    "        operation = client.operations.get(operation)\n",
    "    \n",
    "    for n, generated_video in enumerate(operation.response.generated_videos):\n",
    "        client.files.download(file=generated_video.video)\n",
    "        generated_video.video.save(f\"video{n}.mp4\")  # save the video\n",
    "        save_videos_in_weave(f\"video{n}.mp4\")\n",
    "        \n",
    " \n",
    "generate_videos(\"Panning wide shot of a calico kitten sleeping in the sunshine\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
