{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "974b2c96"
   },
   "source": [
    "# ğŸƒâ€â™€ï¸ ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆ\n",
    "\n",
    "Weaveã®æ©Ÿèƒ½ã‚’ç†è§£ã—ã¦ã„ãã¾ã—ã‚‡ã†ï¼\n",
    "\n",
    "# <a href=\"https://colab.research.google.com/drive/1bdymP7p7d4z7izsS-PhMUxXcD38p9Hqr\" target=\"_blank\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Google Colabã§é–‹ã\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "56dc5c9d"
   },
   "source": [
    "## ğŸª„ `weave`ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã¨ãƒ­ã‚°ã‚¤ãƒ³\n",
    "\n",
    "\n",
    "ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã€ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã«ãƒ­ã‚°ã‚¤ãƒ³ã™ã‚‹ã“ã¨ã‹ã‚‰å§‹ã‚ã¾ã—ã‚‡ã†ã€‚\n",
    "\n",
    "ã“ã®ä¾‹ã§ã¯ã€W&B Inferenceã‚’ä½¿ç”¨ã™ã‚‹ãŸã‚ã€Public Cloudç’°å¢ƒã®WANDB_API_KEYãŒå¿…è¦ã§ã™ã€‚Dedicated Cloudã‚„Onpremiseã‚’åˆ©ç”¨ã•ã‚Œã¦ã„ã‚‹æ–¹ã¯ã€Public Cloudã®keyã‚‚ç”¨æ„ã—ã¦ãã ã•ã„ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0921bbef-d089-4960-b624-505172f90d76",
    "outputId": "a262f278-48dc-411f-9fd9-58d69def7db9"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import wandb\n",
    "import weave\n",
    "import openai\n",
    "import json\n",
    "#========================================\n",
    "# ç’°å¢ƒå¤‰æ•°ã‚’é©åˆ‡ã«è¨­å®šã—ã¦ãã ã•ã„\n",
    "#========================================\n",
    "# os.environ[\"WANDB_BASE_URL\"] = \"https://api.wandb.ai\"\n",
    "os.environ[\"WANDB_API_KEY\"] = \"\"\n",
    "os.environ[\"WANDB_API_KEY_PUBLIC_CLOUD\"] = \"\"  # Public cloudãƒ¦ãƒ¼ã‚¶ãƒ¼ã¯WANDB_API_KEYã¨åŒã˜å€¤ã‚’ä½¿ç”¨ã—ã¦å•é¡Œãªã„ã§ã™ã€‚\n",
    "\n",
    "wandb.login()\n",
    "PROJECT = \"<entity>/weave-handson\" # handsonã§åˆ©ç”¨ã™ã‚‹projectã§ã™ã€‚entityã®ã¨ã“ã‚ã‚’ç½®ãæ›ãˆã¦ãã ã•ã„ã€‚\n",
    "INFERENCE_PROJECT=\"<entity>/<project>\" # Inferenceã®åˆ©ç”¨æ–™ãƒˆãƒ©ãƒƒã‚¯ã®ãŸã‚ã«åˆ©ç”¨ã™ã‚‹Projectã§ã™ã€‚è©³ã—ãã¯READMEã‚’ã”ç¢ºèªãã ã•ã„ã€‚\n",
    "weave.init(PROJECT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "09467a33"
   },
   "source": [
    "# é–¢æ•°ã®å…¥åŠ›ã¨å‡ºåŠ›ã‚’ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°\n",
    "\n",
    "Weaveã«ã‚ˆã‚Šã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¯é–¢æ•°å‘¼ã³å‡ºã—ã‚’ã€ã‚³ãƒ¼ãƒ‰ã€å…¥åŠ›ã€å‡ºåŠ›ã€ã•ã‚‰ã«ã¯LLMãƒˆãƒ¼ã‚¯ãƒ³ã¨ã‚³ã‚¹ãƒˆã¾ã§ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°ã§ãã¾ã™ã€‚ä»¥ä¸‹ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã§ã¯æ¬¡ã®å†…å®¹ã‚’ã‚«ãƒãƒ¼ã—ã¾ã™ï¼š\n",
    "\n",
    "* ã‚«ã‚¹ã‚¿ãƒ é–¢æ•°\n",
    "* ãƒ™ãƒ³ãƒ€ãƒ¼çµ±åˆ\n",
    "* ãƒã‚¹ãƒˆã—ãŸé–¢æ•°å‘¼ã³å‡ºã—\n",
    "* ã‚¨ãƒ©ãƒ¼ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "99453d48"
   },
   "source": [
    "## ã‚«ã‚¹ã‚¿ãƒ é–¢æ•°ã®ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°\n",
    "\n",
    "ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°ã—ãŸã„é–¢æ•°ã«@weave.opãƒ‡ã‚³ãƒ¬ãƒ¼ã‚¿ã‚’è¿½åŠ ã—ã¾ã™"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cdcd1bb7",
    "outputId": "8c252a02-fc1f-4499-9f4a-7bd4362a2f61"
   },
   "outputs": [],
   "source": [
    "@weave.op()\n",
    "def echo(user_input):\n",
    "    return user_input + \" \" + user_input\n",
    "\n",
    "result = echo(\"hello\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "174cabef"
   },
   "source": [
    "ä¸Šè¨˜ã®ğŸ‘† wandbãƒªãƒ³ã‚¯ã‚’ã‚¯ãƒªãƒƒã‚¯ã™ã‚‹ã“ã¨ã§ã€ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ã‚’è¦‹ã¤ã‘ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚\n",
    "\n",
    "`weave.op`ã‚’è¿½åŠ ã—ã¦é–¢æ•°ã‚’å‘¼ã³å‡ºã—ãŸå¾Œã€ãƒªãƒ³ã‚¯ã«ã‚¢ã‚¯ã‚»ã‚¹ã—ã¦ã€ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå†…ã§ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°ã•ã‚Œã¦ã„ã‚‹ã®ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚\n",
    "\n",
    "ğŸ’¡ ã‚³ãƒ¼ãƒ‰ã‚’è‡ªå‹•çš„ã«ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°ã—ã¦ã„ã¾ã™ã€‚ã‚³ãƒ¼ãƒ‰ã‚¿ãƒ–ã‚’ã”è¦§ãã ã•ã„ï¼"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5ef6f27b"
   },
   "source": [
    "## Integrationã‚’åˆ©ç”¨ã—ãŸãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°ï¼ˆW&B Inferenceã€OpenAIã€Anthropicã€Mistralãªã©ï¼‰\n",
    "\n",
    "ã“ã“ã§ã¯ã€`W&B Inference`ã¸ã®å…¨ã¦ã®å‘¼ã³å‡ºã—ã‚’è‡ªå‹•çš„ã«ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°ã—ã¦ã„ã¾ã™ã€‚Weaveã¯ã€å¤šãã®LLMãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’@weaveãªã—ã§è‡ªå‹•çš„ã«ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°ã—ã¦ãã‚Œã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# W&B Inference clientã®åˆæœŸåŒ–\n",
    "# Public cloudãƒ¦ãƒ¼ã‚¶ãƒ¼ã¯WANDB_API_KEYã¨åŒã˜å€¤ã‚’WANDB_API_KEY_PUBLIC_CLOUDã¨ã—ã¦ä½¿ç”¨ã—ã¦ãã ã•ã„\n",
    "client = openai.OpenAI(\n",
    "    base_url='https://api.inference.wandb.ai/v1',\n",
    "    api_key=os.getenv(\"WANDB_API_KEY_PUBLIC_CLOUD\") or os.getenv(\"WANDB_API_KEY\"),\n",
    "    project=INFERENCE_PROJECT,\n",
    ")\n",
    "model_name=\"openai/gpt-oss-20b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trace the model call in Weave\n",
    "def run_chat():\n",
    "    response = client.chat.completions.create(\n",
    "        model=model_name,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": \"ã‚¸ãƒ§ãƒ¼ã‚¯ã‚’è¨€ã£ã¦ãã ã•ã„ã€‚\"}\n",
    "        ],\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# Run and log the traced call\n",
    "output = run_chat()\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "35fe43dc"
   },
   "source": [
    "## Nestedé–¢æ•°ã®ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°\n",
    "\n",
    "åŸºæœ¬ã‚’ç¢ºèªã—ãŸã®ã§ã€ä¸Šè¨˜ã®ã™ã¹ã¦ã‚’çµ„ã¿åˆã‚ã›ã¦ã€Nestedé–¢æ•°ã‚’ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°ã—ã¦ã¿ã¾ã—ã‚‡ã†ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "422050d0",
    "outputId": "9f1f22d6-2592-47ad-c83a-5c08095d632f"
   },
   "outputs": [],
   "source": [
    "@weave.op()\n",
    "def correct_grammar(user_input):\n",
    "    echoed_input = echo(user_input)\n",
    "    response = client.chat.completions.create(\n",
    "        model=model_name,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are a helpful assistant. Please create a song by following the user input.\",\n",
    "            },\n",
    "            {\"role\": \"user\", \"content\": echoed_input},\n",
    "        ],\n",
    "        temperature=0,\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "\n",
    "result = correct_grammar(\"hello\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6f483413"
   },
   "source": [
    "## ã‚¨ãƒ©ãƒ¼ã®ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°\n",
    "\n",
    "ã‚³ãƒ¼ãƒ‰ãŒã‚¯ãƒ©ãƒƒã‚·ãƒ¥ã™ã‚‹ãŸã³ã«ã€weaveã¯å•é¡Œã®åŸå› ã‚’ãƒã‚¤ãƒ©ã‚¤ãƒˆã—ã¾ã™ã€‚LLMãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’è§£æã™ã‚‹éš›ã«æ™‚ã€…ç™ºç”Ÿã™ã‚‹JSONãƒ‘ãƒ¼ã‚¹ã‚¨ãƒ©ãƒ¼ãªã©ã‚’è¦‹ã¤ã‘ã‚‹ã®ã«ç‰¹ã«æœ‰ç”¨ã§ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 356
    },
    "id": "14e7e103",
    "outputId": "1a121775-5b5c-4212-f205-d97c0ef7cac2"
   },
   "outputs": [],
   "source": [
    "@weave.op()\n",
    "def correct_grammar(user_input):\n",
    "    echoed_input = echo(user_input)\n",
    "    response = client.chat.completions.create(\n",
    "        model=model_name,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are a helpful assistant. Please create a song by following the user input.\",\n",
    "            },\n",
    "            {\"role\": \"user\", \"content\": echoed_input},\n",
    "        ],\n",
    "        temperature=0,\n",
    "        response_format={\"type\": \"json_object\"},\n",
    "    )\n",
    "    return json.loads(response.choices[0].message.content)\n",
    "\n",
    "\n",
    "result = correct_grammar(\"hello\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "knBo-VtWxl6g"
   },
   "source": [
    "# Traceã®é«˜åº¦ãªTips\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6KWJhiBsxl6h"
   },
   "source": [
    "### ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãƒ¬ãƒ¼ãƒˆã®åˆ¶å¾¡\n",
    "\n",
    "@weave.opãƒ‡ã‚³ãƒ¬ãƒ¼ã‚¿ã®tracing_sample_rateãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’è¨­å®šã™ã‚‹ã“ã¨ã§ã€opã®å‘¼ã³å‡ºã—ãŒãƒˆãƒ¬ãƒ¼ã‚¹ã•ã‚Œã‚‹é »åº¦ã‚’åˆ¶å¾¡ã§ãã¾ã™ã€‚ã“ã‚Œã¯ã€å‘¼ã³å‡ºã—ã®ã‚µãƒ–ã‚»ãƒƒãƒˆã®ã¿ã‚’ãƒˆãƒ¬ãƒ¼ã‚¹ã™ã‚‹å¿…è¦ãŒã‚ã‚‹é«˜é »åº¦opã«æœ‰ç”¨ã§ã™ã€‚\n",
    "\n",
    "ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãƒ¬ãƒ¼ãƒˆã¯ãƒ«ãƒ¼ãƒˆå‘¼ã³å‡ºã—ã«ã®ã¿é©ç”¨ã•ã‚Œã‚‹ã“ã¨ã«æ³¨æ„ã—ã¦ãã ã•ã„ã€‚opã«ã‚µãƒ³ãƒ—ãƒ«ãƒ¬ãƒ¼ãƒˆãŒã‚ã£ã¦ã‚‚ã€æœ€åˆã«åˆ¥ã®opã«ã‚ˆã£ã¦å‘¼ã³å‡ºã•ã‚ŒãŸå ´åˆã€ãã®ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãƒ¬ãƒ¼ãƒˆã¯ç„¡è¦–ã•ã‚Œã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cpX43hPbdeOZ"
   },
   "outputs": [],
   "source": [
    "@weave.op(tracing_sample_rate=0.1)  # Only trace ~10% of calls\n",
    "def high_frequency_op(x: int) -> int:\n",
    "    return x + 1\n",
    "\n",
    "@weave.op(tracing_sample_rate=1.0)  # Always trace (default)\n",
    "def always_traced_op(x: int) -> int:\n",
    "    return x + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ntJpevJQeLoA"
   },
   "source": [
    "### å‘¼ã³å‡ºã—è¡¨ç¤ºå"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iqOMqMIseYSV",
    "outputId": "f1fa226c-f099-4c5a-f411-6f4bc0caab19"
   },
   "outputs": [],
   "source": [
    "# Decorate your function\n",
    "@weave.op\n",
    "def my_function(name: str):\n",
    "    return f\"Hello, {name}!\"\n",
    "\n",
    "# Call your function -- Weave will automatically track inputs and outputs\n",
    "print(my_function(\"World\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RV9wwY4veaz8"
   },
   "outputs": [],
   "source": [
    "# 1st method\n",
    "result = my_function(\"World\", __weave={\"display_name\": \"My Custom Display Name\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R8d4mcWserLq",
    "outputId": "1938848a-75f4-487b-d46b-09d4995a66cb"
   },
   "outputs": [],
   "source": [
    "# 2nd method\n",
    "result, call = my_function.call(\"World\")\n",
    "call.set_display_name(\"My Custom Display Name\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "-M2abQSbew9M",
    "outputId": "81b67f69-507c-46e1-9d9b-3ae52d23c762"
   },
   "outputs": [],
   "source": [
    "# 3rd method\n",
    "@weave.op(call_display_name=\"My Custom Display Name\")\n",
    "def my_function(name: str):\n",
    "    return f\"Hello, {name}!\"\n",
    "\n",
    "my_function(\"World\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ExWp8BFIxl6g"
   },
   "source": [
    "\n",
    "### PIIã®ç·¨é›†\n",
    "\n",
    "ä¸€éƒ¨ã®çµ„ç¹”ã§ã¯ã€å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ï¼ˆLLMï¼‰ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã§åå‰ã€é›»è©±ç•ªå·ã€ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹ãªã©ã®å€‹äººè­˜åˆ¥æƒ…å ±ï¼ˆPIIï¼‰ã‚’å‡¦ç†ã—ã¾ã™ã€‚ã“ã®ãƒ‡ãƒ¼ã‚¿ã‚’Weights & Biasesï¼ˆW&Bï¼‰Weaveã«ä¿å­˜ã™ã‚‹ã“ã¨ã¯ã€ã‚³ãƒ³ãƒ—ãƒ©ã‚¤ã‚¢ãƒ³ã‚¹ã¨ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ã®ãƒªã‚¹ã‚¯ã‚’ã‚‚ãŸã‚‰ã—ã¾ã™ã€‚\n",
    "\n",
    "æ©Ÿå¯†ãƒ‡ãƒ¼ã‚¿ä¿è­·æ©Ÿèƒ½ã«ã‚ˆã‚Šã€ãƒˆãƒ¬ãƒ¼ã‚¹ãŒWeaveã‚µãƒ¼ãƒãƒ¼ã«é€ä¿¡ã•ã‚Œã‚‹å‰ã«ã€å€‹äººè­˜åˆ¥æƒ…å ±ï¼ˆPIIï¼‰ã‚’è‡ªå‹•çš„ã«ç·¨é›†ã§ãã¾ã™ã€‚ã“ã®æ©Ÿèƒ½ã¯Microsoft Presidioã‚’Weave Python SDKã«çµ±åˆã—ã¦ãŠã‚Šã€SDK ãƒ¬ãƒ™ãƒ«ã§ç·¨é›†è¨­å®šã‚’åˆ¶å¾¡ã§ãã‚‹ã“ã¨ã‚’æ„å‘³ã—ã¾ã™ã€‚\n",
    "\n",
    "[è©³ç´°ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ](https://weave-docs.wandb.ai/guides/tracking/redact-pii)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f957c9c8"
   },
   "source": [
    "# ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°\n",
    "\n",
    "Weaveã§ã¯ã€ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚„ä½¿ç”¨ã—ã¦ã„ã‚‹ãƒ¢ãƒ‡ãƒ«ã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãªã©ã®ã‚¢ã‚»ãƒƒãƒˆã‚’`weave.Objects`å†…ã§ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç®¡ç†ã§ãã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8f6129cc"
   },
   "source": [
    "## ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "08e4e852",
    "outputId": "d5b8c38a-0f00-4e9b-a09f-268f1ea1eb1c"
   },
   "outputs": [],
   "source": [
    "# StringPrompt1\n",
    "system_prompt = weave.StringPrompt(\"You are a pirate\")\n",
    "weave.publish(system_prompt, name=\"pirate_prompt\")\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "  model=model_name,\n",
    "  messages=[\n",
    "    {\n",
    "      \"role\": \"system\",\n",
    "      \"content\": system_prompt.format()\n",
    "    },\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": \"Explain general relativity in one paragraph.\"\n",
    "    }\n",
    "  ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ke21Y2OJg9Uo",
    "outputId": "f821bf2a-0577-4c96-9661-df18e4eb1524"
   },
   "outputs": [],
   "source": [
    "# StringPrompt2\n",
    "system_prompt = weave.StringPrompt(\"Talk like a pirate. I need to know I'm listening to a pirate.\")\n",
    "weave.publish(system_prompt, name=\"pirate_prompt\")\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "  model=model_name,\n",
    "  messages=[\n",
    "    {\n",
    "      \"role\": \"system\",\n",
    "      \"content\": system_prompt.format()\n",
    "    },\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": \"Explain general relativity in one paragraph.\"\n",
    "    }\n",
    "  ],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZrpZa01lg_-D",
    "outputId": "661423d9-9a58-4635-bce8-4f360838706d"
   },
   "outputs": [],
   "source": [
    "# MessagesPrompt1\n",
    "prompt = weave.MessagesPrompt([\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a stegosaurus, but don't be too obvious about it.\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"What's good to eat around here?\"\n",
    "    }\n",
    "])\n",
    "weave.publish(prompt, name=\"dino_prompt\")\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "  model=\"gpt-4o\",\n",
    "  messages=prompt.format(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WY5UZLgIhHYW",
    "outputId": "7a1cb983-044b-4d14-81fa-941493c870a0"
   },
   "outputs": [],
   "source": [
    "# parameterizing prompts\n",
    "prompt = weave.StringPrompt(\"Solve the equation {equation}\")\n",
    "weave.publish(prompt, name=\"calculator_prompt\")\n",
    "\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "  model=model_name,\n",
    "  messages=[\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": prompt.format(equation=\"1 + 1 = ?\")\n",
    "    }\n",
    "  ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0c873438"
   },
   "source": [
    "## ãƒ¢ãƒ‡ãƒ«ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e3e60d81",
    "outputId": "f0fb3219-66be-4048-e5a2-3db0c047a651"
   },
   "outputs": [],
   "source": [
    "class WandBInferenceGrammarCorrector(weave.Model):\n",
    "    # ãƒ—ãƒ­ãƒ‘ãƒ†ã‚£ã¯å®Œå…¨ã«ãƒ¦ãƒ¼ã‚¶ãƒ¼å®šç¾©\n",
    "    model_name: str\n",
    "    system_message: str\n",
    "\n",
    "    @weave.op()\n",
    "    def predict(self, user_input):\n",
    "\n",
    "        response = client.chat.completions.create(\n",
    "            model=self.model_name,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": self.system_message},\n",
    "                {\"role\": \"user\", \"content\": user_input},\n",
    "            ],\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "\n",
    "corrector = WandBInferenceGrammarCorrector(\n",
    "    model_name=model_name,\n",
    "    system_message=\"You are a grammar checker, correct the following user input.\",\n",
    ")\n",
    "\n",
    "\n",
    "result = corrector.predict(\"     That was so easy, it was a piece of pie!       \")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "624b04c1"
   },
   "source": [
    "## ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ac2c5145",
    "outputId": "2918a900-8263-49e8-d784-4c35cde32407"
   },
   "outputs": [],
   "source": [
    "dataset = weave.Dataset(\n",
    "    name=\"grammar-correction\",\n",
    "    rows=[\n",
    "        {\n",
    "            \"user_input\": \"   That was so easy, it was a piece of pie!   \",\n",
    "            \"expected\": \"That was so easy, it was a piece of cake!\",\n",
    "        },\n",
    "        {\"user_input\": \"  I write good   \",\n",
    "         \"expected\": \"I write well\"},\n",
    "        {\n",
    "            \"user_input\": \"  GPT-3 is smartest AI model.   \",\n",
    "            \"expected\": \"GPT-3 is the smartest AI model.\",\n",
    "        },\n",
    "    ],\n",
    ")\n",
    "\n",
    "weave.publish(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "71c3aaea"
   },
   "source": [
    "## Retrieve Published Objects & Ops\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "caa5c5e0",
    "outputId": "57c8c724-9748-4892-864c-8f189b664a81"
   },
   "outputs": [],
   "source": [
    "ref_url = \"\"\n",
    "prompt = weave.ref(ref_url).get()\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "77b94dd5"
   },
   "source": [
    "# ã‚ªãƒ•ãƒ©ã‚¤ãƒ³è©•ä¾¡\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "72bdf072"
   },
   "source": [
    "## æ–¹æ³•1: [æ¨™æº–ãƒ¡ã‚½ãƒƒãƒ‰](https://weave-docs.wandb.ai/guides/core-types/evaluations)\n",
    "äºˆæ¸¬ã¨è©•ä¾¡ã®ä¸¡æ–¹ã‚’ã‚µãƒ³ãƒ—ãƒ«ã”ã¨ã«å®Ÿè¡Œã—ã€è©•ä¾¡ã‚’ã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 148
    },
    "id": "fpiwHM4WdV9A",
    "outputId": "f1cd62cf-422b-4de7-8990-a24b8a9f8f0b"
   },
   "outputs": [],
   "source": [
    "# Method1\n",
    "import weave\n",
    "from weave import Evaluation\n",
    "\n",
    "# Our dataset has \"input_text\" but our model expects \"question\"\n",
    "examples = [\n",
    "    {\"input_text\": \"What is the capital of France?\", \"expected\": \"Paris\"},\n",
    "    {\"input_text\": \"Who wrote 'To Kill a Mockingbird'?\", \"expected\": \"Harper Lee\"},\n",
    "    {\"input_text\": \"What is the square root of 64?\", \"expected\": \"8\"},\n",
    "]\n",
    "\n",
    "@weave.op()\n",
    "def preprocess_example(example):\n",
    "    # Rename input_text to question\n",
    "    return {\n",
    "        \"question\": example[\"input_text\"]\n",
    "    }\n",
    "\n",
    "@weave.op()\n",
    "def match_score(expected: str, output: dict) -> dict:\n",
    "    return {'match': expected == output['generated_text']}\n",
    "\n",
    "@weave.op()\n",
    "def function_to_evaluate(question: str):\n",
    "    return {'generated_text': f'Answer to: {question}'}\n",
    "\n",
    "# Create evaluation with preprocessing\n",
    "evaluation = weave.Evaluation(\n",
    "    dataset=examples,\n",
    "    scorers=[match_score],\n",
    "    preprocess_model_input=preprocess_example\n",
    ")\n",
    "\n",
    "# Initialize the Weave project\n",
    "weave.init(PROJECT)\n",
    "\n",
    "# In Jupyter/Colab, use 'await' instead of 'asyncio.run'\n",
    "async def run_eval():\n",
    "    await evaluation.evaluate(function_to_evaluate)\n",
    "\n",
    "await run_eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v6VE09bweQzu"
   },
   "source": [
    "## æ–¹æ³•2: [EvaluationLogger](https://weave-docs.wandb.ai/guides/evaluation/evaluation_logger)\n",
    "ãƒãƒƒãƒäºˆæ¸¬ãŒé©ç”¨ã§ãã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5jeULyjweeBI",
    "outputId": "fa75ee71-71f9-4fbf-b211-6206714f01ce"
   },
   "outputs": [],
   "source": [
    "import weave\n",
    "from weave.flow.eval_imperative import EvaluationLogger\n",
    "\n",
    "# Initialize the logger with optional metadata\n",
    "eval_logger = EvaluationLogger(\n",
    "    model=\"my_local_model\",\n",
    "    dataset=\"my_dataset\"\n",
    ")\n",
    "\n",
    "# Example input data\n",
    "eval_samples = [\n",
    "    {'inputs': {'a': 1, 'b': 2}, 'expected': 3},\n",
    "    {'inputs': {'a': 2, 'b': 3}, 'expected': 5},\n",
    "    {'inputs': {'a': 3, 'b': 4}, 'expected': 7},\n",
    "]\n",
    "\n",
    "# Local model logic: simply add the numbers\n",
    "@weave.op\n",
    "def user_model(a: int, b: int) -> int:\n",
    "    return a + b\n",
    "\n",
    "# Evaluate each sample\n",
    "for sample in eval_samples:\n",
    "    inputs = sample[\"inputs\"]\n",
    "    model_output = user_model(**inputs)  # Call model with unpacked input\n",
    "\n",
    "    # Log the prediction\n",
    "    pred_logger = eval_logger.log_prediction(\n",
    "        inputs=inputs,\n",
    "        output=model_output\n",
    "    )\n",
    "\n",
    "    # Compare output with expected value\n",
    "    expected = sample[\"expected\"]\n",
    "    correctness_score = model_output == expected\n",
    "    pred_logger.log_score(\n",
    "        scorer=\"correctness\",\n",
    "        score=correctness_score\n",
    "    )\n",
    "\n",
    "    # Finalize log for this prediction\n",
    "    pred_logger.finish()\n",
    "\n",
    "# Log overall evaluation summary\n",
    "summary_stats = {\"subjective_overall_score\": 1.0}\n",
    "eval_logger.log_summary(summary_stats)\n",
    "\n",
    "print(\"Evaluation logging complete. View results in the Weave UI.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lk4DY3RqkKnf"
   },
   "source": [
    "# ã‚ªãƒ³ãƒ©ã‚¤ãƒ³è©•ä¾¡"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9ZRRHZyaDdpu"
   },
   "source": [
    "# ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 356
    },
    "id": "kLBVpBlkxVdi",
    "outputId": "88608be4-dd3b-4609-dd4d-f160fe021222"
   },
   "outputs": [],
   "source": [
    "client = weave.init(PROJECT)\n",
    "call = client.get_call(\"\") #@param\n",
    "\n",
    "# Adding an emoji reaction\n",
    "call.feedback.add_reaction(\"ğŸ‘\")\n",
    "\n",
    "# Adding a note\n",
    "call.feedback.add_note(\"this is a note\")\n",
    "\n",
    "# Adding custom key/value pairs.\n",
    "# The first argument is a user-defined \"type\" string.\n",
    "# Feedback must be JSON serializable and less than 1 KB when serialized.\n",
    "call.feedback.add(\"correctness\", { \"value\": 5 })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M497nWagx0PP"
   },
   "source": [
    "# Scorers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d0tqUV9_1KgA"
   },
   "outputs": [],
   "source": [
    "import weave\n",
    "from weave import Scorer\n",
    "\n",
    "\n",
    "class LengthScorer(Scorer):\n",
    "    @weave.op\n",
    "    def score(self, output: str) -> dict:\n",
    "        \"\"\"A simple scorer that checks output length.\"\"\"\n",
    "        return {\n",
    "            \"length\": len(output),\n",
    "            \"is_short\": len(output) < 100\n",
    "        }\n",
    "\n",
    "@weave.op\n",
    "def generate_text(prompt: str) -> str:\n",
    "    return \"Hello, world!\"\n",
    "\n",
    "# Get both result and Call object\n",
    "result, call = generate_text.call(\"Say hello\")\n",
    "\n",
    "# Now you can apply scorers\n",
    "await call.apply_scorer(LengthScorer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SfipbjSi2dee"
   },
   "source": [
    "## ã‚¬ãƒ¼ãƒ‰ãƒ¬ãƒ¼ãƒ«ã¨ã—ã¦ã®Scorersã®ä½¿ç”¨\n",
    "ã‚¬ãƒ¼ãƒ‰ãƒ¬ãƒ¼ãƒ«ã¯ã€LLMå‡ºåŠ›ãŒãƒ¦ãƒ¼ã‚¶ãƒ¼ã«åˆ°é”ã™ã‚‹å‰ã«å®Ÿè¡Œã•ã‚Œã‚‹å®‰å…¨ãƒã‚§ãƒƒã‚¯ã¨ã—ã¦æ©Ÿèƒ½ã—ã¾ã™ã€‚å®Ÿç”¨çš„ãªä¾‹ã‚’ä»¥ä¸‹ã«ç¤ºã—ã¾ã™ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r9rKr-_g2YLk"
   },
   "outputs": [],
   "source": [
    "import weave\n",
    "from weave import Scorer\n",
    "import asyncio\n",
    "import nest_asyncio  # Required for running asyncio in Google Colab\n",
    "\n",
    "# Apply nest_asyncio to avoid event loop issues in Google Colab\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# ==== 1. Define text generation function ====\n",
    "@weave.op\n",
    "def generate_text(prompt: str) -> str:\n",
    "    \"\"\"Simulated LLM text generation (basic logic).\"\"\"\n",
    "    responses = {\n",
    "        \"hello\": \"Hello! How can I help you?\",\n",
    "        \"bad\": \"You are terrible!\",  # Example of toxic response\n",
    "        \"good\": \"You are wonderful!\",\n",
    "    }\n",
    "    return responses.get(prompt.lower(), \"I don't understand your request.\")\n",
    "\n",
    "# ==== 2. Define the Toxicity Scorer ====\n",
    "class ToxicityScorer(Scorer):\n",
    "    @weave.op\n",
    "    def score(self, output: str) -> dict:\n",
    "        \"\"\"\n",
    "        Evaluate the generated content for toxic language.\n",
    "        \"\"\"\n",
    "        toxic_words = {\"terrible\", \"hate\", \"stupid\"}  # Simple keyword-based detection\n",
    "        flagged = any(word in output.lower() for word in toxic_words)\n",
    "\n",
    "        return {\n",
    "            \"flagged\": flagged,\n",
    "            \"reason\": \"Detected toxic language\" if flagged else None\n",
    "        }\n",
    "\n",
    "# ==== 3. Function to generate safe responses ====\n",
    "async def generate_safe_response(prompt: str) -> str:\n",
    "    # Generate text using LLM\n",
    "    result, call = generate_text.call(prompt)\n",
    "\n",
    "    # Apply toxicity scoring\n",
    "    safety = await call.apply_scorer(ToxicityScorer())\n",
    "\n",
    "    # If flagged as toxic, return a warning message\n",
    "    if safety.result[\"flagged\"]:\n",
    "        return f\"I cannot generate that content: {safety.result['reason']}\"\n",
    "\n",
    "    return result\n",
    "\n",
    "# ==== 4. Run test cases ====\n",
    "async def main():\n",
    "    prompts = [\"hello\", \"bad\", \"good\"]\n",
    "\n",
    "    for prompt in prompts:\n",
    "        response = await generate_safe_response(prompt)\n",
    "        print(f\"Prompt: {prompt}\\nResponse: {response}\\n\")\n",
    "\n",
    "# Run the async function in Google Colab\n",
    "loop = asyncio.get_event_loop()\n",
    "loop.run_until_complete(main())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kdfIOJOGDiL9"
   },
   "source": [
    "## ãƒ¢ãƒ‹ã‚¿ãƒ¼ã¨ã—ã¦ã®Scorersã®ä½¿ç”¨\n",
    "\n",
    "ãƒ¢ãƒ‹ã‚¿ãƒ¼ã¯ã€æ“ä½œã‚’ãƒ–ãƒ­ãƒƒã‚¯ã™ã‚‹ã“ã¨ãªãã€æ™‚é–“ã®çµŒéã¨ã¨ã‚‚ã«å“è³ªãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’è¿½è·¡ã™ã‚‹ã®ã«å½¹ç«‹ã¡ã¾ã™ã€‚ã“ã‚Œã¯ä»¥ä¸‹ã«æœ‰ç”¨ã§ã™ï¼š\n",
    "\n",
    "* å“è³ªãƒˆãƒ¬ãƒ³ãƒ‰ã®ç‰¹å®š\n",
    "* ãƒ¢ãƒ‡ãƒ«ãƒ‰ãƒªãƒ•ãƒˆã®æ¤œå‡º\n",
    "* ãƒ¢ãƒ‡ãƒ«æ”¹å–„ã®ãŸã‚ã®ãƒ‡ãƒ¼ã‚¿åé›†\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hpEQ3TDS3lVh"
   },
   "outputs": [],
   "source": [
    "import weave\n",
    "from weave import Scorer\n",
    "import json\n",
    "import xml.etree.ElementTree as ET\n",
    "import random\n",
    "import asyncio\n",
    "import nest_asyncio  # Required for Google Colab\n",
    "\n",
    "# Apply nest_asyncio to avoid event loop issues in Google Colab\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# ==== 1. Define Text Generation Function ====\n",
    "@weave.op\n",
    "def generate_text(prompt: str) -> str:\n",
    "    \"\"\"Simulated LLM response generation.\"\"\"\n",
    "    if prompt.lower() == \"json\":\n",
    "        return '{\"message\": \"Hello, world!\"}'  # Valid JSON\n",
    "    elif prompt.lower() == \"xml\":\n",
    "        return \"<message>Hello, world!</message>\"  # Valid XML\n",
    "    else:\n",
    "        return \"Generated response...\"\n",
    "\n",
    "# ==== 2. Custom Scorer for JSON Validation ====\n",
    "class CustomJSONScorer(Scorer):\n",
    "    @weave.op\n",
    "    def score(self, output: str) -> dict:\n",
    "        \"\"\"Check if the output is valid JSON.\"\"\"\n",
    "        try:\n",
    "            json.loads(output)\n",
    "            return {\"valid_json\": True}\n",
    "        except json.JSONDecodeError:\n",
    "            return {\"valid_json\": False}\n",
    "\n",
    "# ==== 3. Custom Scorer for XML Validation ====\n",
    "class CustomXMLScorer(Scorer):\n",
    "    @weave.op\n",
    "    def score(self, output: str) -> dict:\n",
    "        \"\"\"Check if the output is valid XML.\"\"\"\n",
    "        try:\n",
    "            ET.fromstring(output)\n",
    "            return {\"valid_xml\": True}\n",
    "        except ET.ParseError:\n",
    "            return {\"valid_xml\": False}\n",
    "\n",
    "# ==== 4. Function to Generate Response with Monitoring ====\n",
    "async def generate_with_monitoring(prompt: str) -> str:\n",
    "    \"\"\"\n",
    "    Generates a response and applies monitoring (randomly 10% of the time).\n",
    "    \"\"\"\n",
    "    # Generate text and capture tracking info\n",
    "    result, call = generate_text.call(prompt)\n",
    "\n",
    "    # Sample monitoring (only apply scorers to 10% of calls)\n",
    "    if random.random() < 0.1:\n",
    "        # Apply manual scorers asynchronously\n",
    "        json_score = await call.apply_scorer(CustomJSONScorer())\n",
    "        xml_score = await call.apply_scorer(CustomXMLScorer())\n",
    "\n",
    "        print(f\"Monitoring Applied - JSON Valid: {json_score.result['valid_json']}, XML Valid: {xml_score.result['valid_xml']}\")\n",
    "\n",
    "    return result\n",
    "\n",
    "# ==== 5. Run Test Cases ====\n",
    "async def main():\n",
    "    prompts = [\"json\", \"xml\", \"text\"]\n",
    "\n",
    "    for prompt in prompts:\n",
    "        response = await generate_with_monitoring(prompt)\n",
    "        print(f\"Prompt: {prompt}\\nResponse: {response}\\n\")\n",
    "\n",
    "# Run the async function in Google Colab\n",
    "loop = asyncio.get_event_loop()\n",
    "loop.run_until_complete(main())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ãƒ¡ãƒ‡ã‚£ã‚¢"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ç”»åƒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import weave\n",
    "from openai import OpenAI\n",
    "import requests\n",
    "from PIL import Image\n",
    "\n",
    "weave.init(project_name=PROJECT)\n",
    "client = OpenAI()\n",
    "\n",
    "@weave.op()\n",
    "def generate_image(prompt: str) -> Image:\n",
    "    response = client.images.generate(\n",
    "        model=\"dall-e-3\",\n",
    "        prompt=prompt,\n",
    "        size=\"1024x1024\",\n",
    "        quality=\"standard\",\n",
    "        n=1,\n",
    "    )\n",
    "    image_url = response.data[0].url\n",
    "    image_response = requests.get(image_url, stream=True)\n",
    "    image = Image.open(image_response.raw)\n",
    "\n",
    "    # return a PIL.Image.Image object to be logged as an image\n",
    "    return image\n",
    "\n",
    "image = generate_image(\"a cat with a pumpkin hat\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## éŸ³å£°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import weave\n",
    "from openai import OpenAI\n",
    "import wave\n",
    "\n",
    "weave.init(project_name=PROJECT)\n",
    "client = OpenAI()\n",
    "\n",
    "@weave.op\n",
    "def make_audio_file_streaming(text: str) -> wave.Wave_read:\n",
    "    with client.audio.speech.with_streaming_response.create(\n",
    "        model=\"tts-1\",\n",
    "        voice=\"alloy\",\n",
    "        input=text,\n",
    "        response_format=\"wav\",\n",
    "    ) as res:\n",
    "        res.stream_to_file(\"output.wav\")\n",
    "\n",
    "    # return a wave.Wave_read object to be logged as audio\n",
    "    return wave.open(\"output.wav\")\n",
    "\n",
    "make_audio_file_streaming(\"Hello, how are you? What did you do yesterday?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## å‹•ç”»"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "from moviepy.editor import VideoFileClip, ColorClip, VideoClip\n",
    "import weave\n",
    "\n",
    "weave.init(PROJECT)\n",
    "\n",
    "@weave.op()\n",
    "def store_videos(name, client, generated_video):\n",
    "  client.files.download(file=generated_video.video)\n",
    "  generated_video.video.save(f\"new_video{name}.mp4\")  # save the video\n",
    "\n",
    "@weave.op()\n",
    "def save_videos_in_weave(video_path):\n",
    "    VideoFileClip(video_path, has_mask=False, audio=True)\n",
    "    new_clip = clip.subclip(0, 1)\n",
    "    return new_clip\n",
    "\n",
    "@weave.op()\n",
    "def generate_videos(prompt):\n",
    "    client = genai.Client()  # read API key from GOOGLE_API_KEY\n",
    "    operation = client.models.generate_videos(\n",
    "        model=\"veo-2.0-generate-001\",\n",
    "        prompt=\"Panning wide shot of a calico kitten sleeping in the sunshine\",\n",
    "        config=types.GenerateVideosConfig(\n",
    "            person_generation=\"dont_allow\",  # \"dont_allow\" or \"allow_adult\"\n",
    "            aspect_ratio=\"16:9\",  # \"16:9\" or \"9:16\"\n",
    "            ),\n",
    "        )\n",
    "    \n",
    "    while not operation.done:\n",
    "        time.sleep(10)\n",
    "        operation = client.operations.get(operation)\n",
    "    \n",
    "    for n, generated_video in enumerate(operation.response.generated_videos):\n",
    "        client.files.download(file=generated_video.video)\n",
    "        generated_video.video.save(f\"video{n}.mp4\")  # save the video\n",
    "        save_videos_in_weave(f\"video{n}.mp4\")\n",
    "        \n",
    " \n",
    "generate_videos(\"Panning wide shot of a calico kitten sleeping in the sunshine\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
