{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "974b2c96"
   },
   "source": [
    "# ğŸƒâ€â™€ï¸ ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆ\n",
    "\n",
    "Weaveã®æ©Ÿèƒ½ã‚’ç†è§£ã—ã¦ã„ãã¾ã—ã‚‡ã†ï¼\n",
    "\n",
    "# <a href=\"https://colab.research.google.com/drive/1bdymP7p7d4z7izsS-PhMUxXcD38p9Hqr\" target=\"_blank\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Google Colabã§é–‹ã\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "56dc5c9d"
   },
   "source": [
    "## ğŸª„ `weave`ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã¨ãƒ­ã‚°ã‚¤ãƒ³\n",
    "\n",
    "\n",
    "ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã€ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã«ãƒ­ã‚°ã‚¤ãƒ³ã™ã‚‹ã“ã¨ã‹ã‚‰å§‹ã‚ã¾ã—ã‚‡ã†ã€‚\n",
    "\n",
    "ã“ã®ä¾‹ã§ã¯ã€W&B Inferenceã‚’ä½¿ç”¨ã™ã‚‹ãŸã‚ã€Public Cloudç’°å¢ƒã®WANDB_API_KEYãŒå¿…è¦ã§ã™ã€‚Dedicated Cloudã‚„Onpremiseã‚’åˆ©ç”¨ã•ã‚Œã¦ã„ã‚‹æ–¹ã¯ã€Public Cloudã®keyã‚‚ç”¨æ„ã—ã¦ãã ã•ã„ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0921bbef-d089-4960-b624-505172f90d76",
    "outputId": "a262f278-48dc-411f-9fd9-58d69def7db9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkeisuke-kamata\u001b[0m (\u001b[33mwandb-smle\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[36m\u001b[1mweave\u001b[0m: wandb version 0.22.2 is available!  To upgrade, please run:\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[36m\u001b[1mweave\u001b[0m: weave version 0.52.9 is available!  To upgrade, please run:\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:  $ pip install weave --upgrade\n",
      "\u001b[36m\u001b[1mweave\u001b[0m: Logged in as Weights & Biases user: keisuke-kamata.\n",
      "\u001b[36m\u001b[1mweave\u001b[0m: View Weave data at https://wandb.ai/wandb-japan/weave-handson-20251015/weave\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<weave.trace.weave_client.WeaveClient at 0x11bb83a10>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import wandb\n",
    "import weave\n",
    "import openai\n",
    "import json\n",
    "#========================================\n",
    "# ç’°å¢ƒå¤‰æ•°ã‚’é©åˆ‡ã«è¨­å®šã—ã¦ãã ã•ã„\n",
    "#========================================\n",
    "# os.environ[\"WANDB_BASE_URL\"] = \"https://api.wandb.ai\" # \n",
    "\n",
    "wandb.login()\n",
    "PROJECT = \"wandb-japan/weave-handson\" # handsonã§åˆ©ç”¨ã™ã‚‹projectã§ã™ã€‚entity(team)ã®ã¨ã“ã‚ã‚’ã”è‡ªèº«ã®teamã«ç½®ãæ›ãˆã¦ãã ã•ã„ã€‚\n",
    "weave.init(PROJECT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "09467a33"
   },
   "source": [
    "# é–¢æ•°ã®å…¥åŠ›ã¨å‡ºåŠ›ã‚’ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°\n",
    "\n",
    "Weaveã«ã‚ˆã‚Šã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¯é–¢æ•°å‘¼ã³å‡ºã—ã‚’ã€ã‚³ãƒ¼ãƒ‰ã€å…¥åŠ›ã€å‡ºåŠ›ã€ã•ã‚‰ã«ã¯LLMãƒˆãƒ¼ã‚¯ãƒ³ã¨ã‚³ã‚¹ãƒˆã¾ã§ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°ã§ãã¾ã™ã€‚ä»¥ä¸‹ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã§ã¯æ¬¡ã®å†…å®¹ã‚’ã‚«ãƒãƒ¼ã—ã¾ã™ï¼š\n",
    "\n",
    "* ã‚«ã‚¹ã‚¿ãƒ é–¢æ•°\n",
    "* ãƒ™ãƒ³ãƒ€ãƒ¼çµ±åˆ\n",
    "* ãƒã‚¹ãƒˆã—ãŸé–¢æ•°å‘¼ã³å‡ºã—\n",
    "* ã‚¨ãƒ©ãƒ¼ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "99453d48"
   },
   "source": [
    "## ã‚«ã‚¹ã‚¿ãƒ é–¢æ•°ã®ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°\n",
    "\n",
    "ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°ã—ãŸã„é–¢æ•°ã«@weave.opãƒ‡ã‚³ãƒ¬ãƒ¼ã‚¿ã‚’è¿½åŠ ã—ã¾ã™"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cdcd1bb7",
    "outputId": "8c252a02-fc1f-4499-9f4a-7bd4362a2f61"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello hello\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m\u001b[1mweave\u001b[0m: ğŸ© https://wandb.ai/wandb-japan/weave-handson-20251015/r/call/0199ebff-446d-7c30-a8d4-d8c5fcad5ce9\n",
      "\u001b[36m\u001b[1mweave\u001b[0m: ğŸ© https://wandb.ai/wandb-japan/weave-handson-20251015/r/call/0199ebff-4b6a-7651-aaf1-81852f492acc\n",
      "\u001b[36m\u001b[1mweave\u001b[0m: ğŸ© https://wandb.ai/wandb-japan/weave-handson-20251015/r/call/0199ebff-50fc-7ef7-b8d5-fe397229a659\n",
      "\u001b[36m\u001b[1mweave\u001b[0m: ğŸ© https://wandb.ai/wandb-japan/weave-handson-20251015/r/call/0199ebff-616d-7a21-b2e6-9533f67c1df4\n"
     ]
    }
   ],
   "source": [
    "@weave.op()\n",
    "def echo(user_input):\n",
    "    return user_input + \" \" + user_input\n",
    "\n",
    "result = echo(\"hello\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "174cabef"
   },
   "source": [
    "ä¸Šè¨˜ã®ğŸ‘† wandbãƒªãƒ³ã‚¯ã‚’ã‚¯ãƒªãƒƒã‚¯ã™ã‚‹ã“ã¨ã§ã€ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ã‚’è¦‹ã¤ã‘ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚\n",
    "\n",
    "`weave.op`ã‚’è¿½åŠ ã—ã¦é–¢æ•°ã‚’å‘¼ã³å‡ºã—ãŸå¾Œã€ãƒªãƒ³ã‚¯ã«ã‚¢ã‚¯ã‚»ã‚¹ã—ã¦ã€ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå†…ã§ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°ã•ã‚Œã¦ã„ã‚‹ã®ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚\n",
    "\n",
    "ğŸ’¡ ã‚³ãƒ¼ãƒ‰ã‚’è‡ªå‹•çš„ã«ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°ã—ã¦ã„ã¾ã™ã€‚ã‚³ãƒ¼ãƒ‰ã‚¿ãƒ–ã‚’ã”è¦§ãã ã•ã„ï¼"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5ef6f27b"
   },
   "source": [
    "## Integrationã‚’åˆ©ç”¨ã—ãŸãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°ï¼ˆW&B Inferenceã€OpenAIã€Anthropicã€Mistralãªã©ï¼‰\n",
    "\n",
    "ã“ã“ã§ã¯ã€`W&B Inference`ã¸ã®å…¨ã¦ã®å‘¼ã³å‡ºã—ã‚’è‡ªå‹•çš„ã«ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°ã—ã¦ã„ã¾ã™ã€‚Weaveã¯ã€å¤šãã®LLMãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’@weaveãªã—ã§è‡ªå‹•çš„ã«ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°ã—ã¦ãã‚Œã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM APIã®åˆæœŸåŒ–\n",
    "use_openai = True # OpenAIã®APIã‚’åˆ©ç”¨ã™ã‚‹å ´åˆã¯True, W&B Inferenceã‚’åˆ©ç”¨ã™ã‚‹å ´åˆã¯Falseã«ã—ã¦ãã ã•ã„ã€‚\n",
    "\n",
    "if use_openai:\n",
    "    ## OpenAIã®APIã‚’åˆ©ç”¨ã™ã‚‹å ´åˆ\n",
    "    #os.environ[\"OPENAI_API_KEY\"] = \"\" #OpenAIã®APIã‚’åˆ©ç”¨ã™ã‚‹å ´åˆã¯ã€OpenAIã®API Keyã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚\n",
    "    client = openai.OpenAI(\n",
    "        api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "    )\n",
    "    model_name=\"gpt-4.1-2025-04-14\"\n",
    "else:\n",
    "    ## W&B Inferenceã‚’åˆ©ç”¨ã™ã‚‹å ´åˆ\n",
    "    #os.environ[\"WANDB_API_KEY_PUBLIC_CLOUD\"] = \"\"  # Public cloudãƒ¦ãƒ¼ã‚¶ãƒ¼ã¯WANDB_API_KEYã¨åŒã˜å€¤ã‚’ä½¿ç”¨ã—ã¦å•é¡Œãªã„ã§ã™ã€‚Dedicated Cloudã‚’åˆ©ç”¨ã—ã€æ¨è«–ã ã‘Public Cloudã‚’åˆ©ç”¨ã™ã‚‹å ´åˆãªã©ã¯ã€Public Cloudã®API Keyã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚\n",
    "    INFERENCE_PROJECT=PROJECT # Inferenceã®åˆ©ç”¨æ–™ãƒˆãƒ©ãƒƒã‚¯ã®ãŸã‚ã«åˆ©ç”¨ã™ã‚‹Projectã§ã™ã€‚Weaveã®PROJECTã¨ä¸€è‡´ã—ãªã„å ´åˆï¼ˆDedicated Cloudã‚’åˆ©ç”¨ã—ã€æ¨è«–ã ã‘Public Cloudã‚’åˆ©ç”¨ã™ã‚‹å ´åˆãªã©ï¼‰ã¯ã€public cloudã®ä¸­ã®entityã¨teamã®çµ„ã¿åˆã‚ã›ã‚’å…¥ã‚Œã¦ãã ã•ã„ã€‚\n",
    "    client = openai.OpenAI(\n",
    "        base_url='https://api.inference.wandb.ai/v1',\n",
    "        api_key=os.getenv(\"WANDB_API_KEY_PUBLIC_CLOUD\") or os.getenv(\"WANDB_API_KEY\"),\n",
    "        project=INFERENCE_PROJECT,\n",
    "    )\n",
    "    model_name=\"openai/gpt-oss-20b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ã‚‚ã¡ã‚ã‚“ï¼ã§ã¯ã€ä¸€ã¤ã©ã†ãã€‚\n",
      "\n",
      "ãƒ‘ãƒ³ãƒ€ãŒãƒ¬ã‚¹ãƒˆãƒ©ãƒ³ã§é£Ÿäº‹ã‚’ã—ã¾ã—ãŸã€‚é£Ÿã¹çµ‚ã‚ã£ãŸå¾Œã€æ€¥ã«ç«‹ã¡ä¸ŠãŒã£ã¦ãŠé‡‘ã‚‚æ‰•ã‚ãšã«å¸°ã‚ã†ã¨ã—ã¾ã—ãŸã€‚åº—å“¡ãŒæ…Œã¦ã¦ã€ŒãŠå®¢æ§˜ã€ãŠä¼šè¨ˆã¯ï¼Ÿã€ã¨èãã¨ã€ãƒ‘ãƒ³ãƒ€ã¯è¨€ã„ã¾ã—ãŸã€‚\n",
      "\n",
      "ã€Œã”ã‚ã‚“ã€æŒã¡ã‚ã„ï¼ˆæŒã¡åˆã„ï¼æŒã¡åˆã‚ã›ï¼‰ãŒãªã„ã‚“ã ã€‚ã€\n",
      "\n",
      "ã„ã‹ãŒã§ã—ã‚‡ã†ã‹ï¼Ÿ\n"
     ]
    }
   ],
   "source": [
    "# Trace the model call in Weave\n",
    "def run_chat():\n",
    "    response = client.chat.completions.create(\n",
    "        model=model_name,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": \"ã‚¸ãƒ§ãƒ¼ã‚¯ã‚’è¨€ã£ã¦ãã ã•ã„ã€‚\"}\n",
    "        ],\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# Run and log the traced call\n",
    "output = run_chat()\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "35fe43dc"
   },
   "source": [
    "## Nestedé–¢æ•°ã®ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°\n",
    "\n",
    "åŸºæœ¬ã‚’ç¢ºèªã—ãŸã®ã§ã€ä¸Šè¨˜ã®ã™ã¹ã¦ã‚’çµ„ã¿åˆã‚ã›ã¦ã€Nestedé–¢æ•°ã‚’ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°ã—ã¦ã¿ã¾ã—ã‚‡ã†ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "422050d0",
    "outputId": "9f1f22d6-2592-47ad-c83a-5c08095d632f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, hello, can you hear me now?  \n",
      "Waves of words are floating through the crowd  \n",
      "Hello, hello, letâ€™s begin again  \n",
      "A brand new story waiting to be penned  \n",
      "\n",
      "(Chorus)  \n",
      "Hello, hello, letâ€™s light up the night  \n",
      "With every greeting, weâ€™re shining so bright  \n",
      "Hello, hello, letâ€™s see where we go  \n",
      "With every â€œhello,â€ we open the door  \n",
      "\n",
      "Verse 2:  \n",
      "Hello, hello, to the world outside  \n",
      "Smiles and laughter, nowhere to hide  \n",
      "Hello, hello, letâ€™s make it clear  \n",
      "Every â€œhelloâ€ brings someone near  \n",
      "\n",
      "(Chorus)  \n",
      "Hello, hello, letâ€™s light up the night  \n",
      "With every greeting, weâ€™re shining so bright  \n",
      "Hello, hello, letâ€™s see where we go  \n",
      "With every â€œhello,â€ we open the door  \n",
      "\n",
      "(Bridge)  \n",
      "Every â€œhelloâ€ is a chance to start  \n",
      "A little kindness, a work of art  \n",
      "So say it loud, let your voice flow  \n",
      "The world gets warmer with every â€œhelloâ€  \n",
      "\n",
      "(Chorus)  \n",
      "Hello, hello, letâ€™s light up the night  \n",
      "With every greeting, weâ€™re shining so bright  \n",
      "Hello, hello, letâ€™s see where we go  \n",
      "With every â€œhello,â€ we open the door  \n",
      "\n",
      "(Outro)  \n",
      "Hello, hello, letâ€™s begin again  \n",
      "With every â€œhello,â€ we find a friend\n"
     ]
    }
   ],
   "source": [
    "@weave.op()\n",
    "def correct_grammar(user_input):\n",
    "    echoed_input = echo(user_input)\n",
    "    response = client.chat.completions.create(\n",
    "        model=model_name,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are a helpful assistant. Please create a song by following the user input.\",\n",
    "            },\n",
    "            {\"role\": \"user\", \"content\": echoed_input},\n",
    "        ],\n",
    "        temperature=0,\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "\n",
    "result = correct_grammar(\"hello\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6f483413"
   },
   "source": [
    "## ã‚¨ãƒ©ãƒ¼ã®ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°\n",
    "\n",
    "ã‚³ãƒ¼ãƒ‰ãŒã‚¯ãƒ©ãƒƒã‚·ãƒ¥ã™ã‚‹ãŸã³ã«ã€weaveã¯å•é¡Œã®åŸå› ã‚’ãƒã‚¤ãƒ©ã‚¤ãƒˆã—ã¾ã™ã€‚LLMãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’è§£æã™ã‚‹éš›ã«æ™‚ã€…ç™ºç”Ÿã™ã‚‹JSONãƒ‘ãƒ¼ã‚¹ã‚¨ãƒ©ãƒ¼ãªã©ã‚’è¦‹ã¤ã‘ã‚‹ã®ã«ç‰¹ã«æœ‰ç”¨ã§ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 356
    },
    "id": "14e7e103",
    "outputId": "1a121775-5b5c-4212-f205-d97c0ef7cac2"
   },
   "outputs": [
    {
     "ename": "BadRequestError",
     "evalue": "Error code: 400 - {'error': {'message': \"'messages' must contain the word 'json' in some form, to use 'response_format' of type 'json_object'.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': None}}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mBadRequestError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 19\u001b[39m\n\u001b[32m      4\u001b[39m     response = client.chat.completions.create(\n\u001b[32m      5\u001b[39m         model=model_name,\n\u001b[32m      6\u001b[39m         messages=[\n\u001b[32m   (...)\u001b[39m\u001b[32m     14\u001b[39m         response_format={\u001b[33m\"\u001b[39m\u001b[33mtype\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mjson_object\u001b[39m\u001b[33m\"\u001b[39m},\n\u001b[32m     15\u001b[39m     )\n\u001b[32m     16\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m json.loads(response.choices[\u001b[32m0\u001b[39m].message.content)\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m result = \u001b[43mcorrect_grammar\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mhello\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Project/.venv/lib/python3.13/site-packages/weave/trace/op.py:1248\u001b[39m, in \u001b[36mop.<locals>.op_deco.<locals>.create_wrapper.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m   1246\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[32m   1247\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper\u001b[39m(*args: P.args, **kwargs: P.kwargs) -> R:\n\u001b[32m-> \u001b[39m\u001b[32m1248\u001b[39m     res, _ = \u001b[43m_call_sync_func\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1249\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mOp\u001b[49m\u001b[43m[\u001b[49m\u001b[43mP\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mR\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwrapper\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m__should_raise\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m   1250\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1251\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(R, res)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Project/.venv/lib/python3.13/site-packages/weave/trace/op.py:502\u001b[39m, in \u001b[36m_call_sync_func\u001b[39m\u001b[34m(op, __weave, __should_raise, __require_explicit_finish, *args, **kwargs)\u001b[39m\n\u001b[32m    499\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m output\n\u001b[32m    501\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m502\u001b[39m     res = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    503\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    504\u001b[39m     finish(exception=e)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 4\u001b[39m, in \u001b[36mcorrect_grammar\u001b[39m\u001b[34m(user_input)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;129m@weave\u001b[39m.op()\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcorrect_grammar\u001b[39m(user_input):\n\u001b[32m      3\u001b[39m     echoed_input = echo(user_input)\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m     response = \u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mchat\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompletions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrole\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msystem\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcontent\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mYou are a helpful assistant. Please create a song by following the user input.\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrole\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcontent\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mechoed_input\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m        \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtype\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mjson_object\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m json.loads(response.choices[\u001b[32m0\u001b[39m].message.content)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Project/.venv/lib/python3.13/site-packages/weave/trace/op.py:1248\u001b[39m, in \u001b[36mop.<locals>.op_deco.<locals>.create_wrapper.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m   1246\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[32m   1247\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper\u001b[39m(*args: P.args, **kwargs: P.kwargs) -> R:\n\u001b[32m-> \u001b[39m\u001b[32m1248\u001b[39m     res, _ = \u001b[43m_call_sync_func\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1249\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mOp\u001b[49m\u001b[43m[\u001b[49m\u001b[43mP\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mR\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwrapper\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m__should_raise\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m   1250\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1251\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(R, res)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Project/.venv/lib/python3.13/site-packages/weave/trace/op.py:502\u001b[39m, in \u001b[36m_call_sync_func\u001b[39m\u001b[34m(op, __weave, __should_raise, __require_explicit_finish, *args, **kwargs)\u001b[39m\n\u001b[32m    499\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m output\n\u001b[32m    501\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m502\u001b[39m     res = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    503\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    504\u001b[39m     finish(exception=e)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Project/.venv/lib/python3.13/site-packages/weave/integrations/openai/openai_sdk.py:421\u001b[39m, in \u001b[36mcreate_wrapper_sync.<locals>.wrapper.<locals>._add_stream_options.<locals>._wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    418\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m urlparse(base_url).hostname == \u001b[33m\"\u001b[39m\u001b[33mapi.openai.com\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    419\u001b[39m         kwargs[\u001b[33m\"\u001b[39m\u001b[33mstream_options\u001b[39m\u001b[33m\"\u001b[39m] = {\u001b[33m\"\u001b[39m\u001b[33minclude_usage\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m}\n\u001b[32m--> \u001b[39m\u001b[32m421\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Project/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py:286\u001b[39m, in \u001b[36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    284\u001b[39m             msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[32m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    285\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[32m--> \u001b[39m\u001b[32m286\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Project/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py:1147\u001b[39m, in \u001b[36mCompletions.create\u001b[39m\u001b[34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, prompt_cache_key, reasoning_effort, response_format, safety_identifier, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, verbosity, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m   1101\u001b[39m \u001b[38;5;129m@required_args\u001b[39m([\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m], [\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m   1102\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate\u001b[39m(\n\u001b[32m   1103\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1144\u001b[39m     timeout: \u001b[38;5;28mfloat\u001b[39m | httpx.Timeout | \u001b[38;5;28;01mNone\u001b[39;00m | NotGiven = NOT_GIVEN,\n\u001b[32m   1145\u001b[39m ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[32m   1146\u001b[39m     validate_response_format(response_format)\n\u001b[32m-> \u001b[39m\u001b[32m1147\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1148\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/chat/completions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1149\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1150\u001b[39m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m   1151\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1152\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1153\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43maudio\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1154\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfrequency_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1155\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunction_call\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1156\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunctions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1157\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogit_bias\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1158\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1159\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_completion_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1160\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1161\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1162\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodalities\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodalities\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1163\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mn\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1164\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mparallel_tool_calls\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1165\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprediction\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1166\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpresence_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1167\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprompt_cache_key\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_cache_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1168\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mreasoning_effort\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_effort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1169\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mresponse_format\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1170\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msafety_identifier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msafety_identifier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1171\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mseed\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1172\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mservice_tier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1173\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstop\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1174\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstore\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1175\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1176\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1177\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtemperature\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1178\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtool_choice\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1179\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtools\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1180\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_logprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1181\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_p\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1182\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1183\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mverbosity\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbosity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1184\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mweb_search_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mweb_search_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1185\u001b[39m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1186\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParamsStreaming\u001b[49m\n\u001b[32m   1187\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\n\u001b[32m   1188\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParamsNonStreaming\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1189\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1190\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1191\u001b[39m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m   1192\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1193\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1194\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1195\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1196\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Project/.venv/lib/python3.13/site-packages/openai/_base_client.py:1259\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1245\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1246\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1247\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1254\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1255\u001b[39m ) -> ResponseT | _StreamT:\n\u001b[32m   1256\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1257\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001b[32m   1258\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1259\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Project/.venv/lib/python3.13/site-packages/openai/_base_client.py:1047\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1044\u001b[39m             err.response.read()\n\u001b[32m   1046\u001b[39m         log.debug(\u001b[33m\"\u001b[39m\u001b[33mRe-raising status error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1047\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_status_error_from_response(err.response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1049\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m   1051\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mcould not resolve response (should never happen)\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mBadRequestError\u001b[39m: Error code: 400 - {'error': {'message': \"'messages' must contain the word 'json' in some form, to use 'response_format' of type 'json_object'.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': None}}"
     ]
    }
   ],
   "source": [
    "@weave.op()\n",
    "def correct_grammar(user_input):\n",
    "    echoed_input = echo(user_input)\n",
    "    response = client.chat.completions.create(\n",
    "        model=model_name,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are a helpful assistant. Please create a song by following the user input.\",\n",
    "            },\n",
    "            {\"role\": \"user\", \"content\": echoed_input},\n",
    "        ],\n",
    "        temperature=0,\n",
    "        response_format={\"type\": \"json_object\"},\n",
    "    )\n",
    "    return json.loads(response.choices[0].message.content)\n",
    "\n",
    "\n",
    "result = correct_grammar(\"hello\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "knBo-VtWxl6g"
   },
   "source": [
    "# Traceã®é«˜åº¦ãªTips\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6KWJhiBsxl6h"
   },
   "source": [
    "### ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãƒ¬ãƒ¼ãƒˆã®åˆ¶å¾¡\n",
    "\n",
    "@weave.opãƒ‡ã‚³ãƒ¬ãƒ¼ã‚¿ã®tracing_sample_rateãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’è¨­å®šã™ã‚‹ã“ã¨ã§ã€opã®å‘¼ã³å‡ºã—ãŒãƒˆãƒ¬ãƒ¼ã‚¹ã•ã‚Œã‚‹é »åº¦ã‚’åˆ¶å¾¡ã§ãã¾ã™ã€‚ã“ã‚Œã¯ã€å‘¼ã³å‡ºã—ã®ã‚µãƒ–ã‚»ãƒƒãƒˆã®ã¿ã‚’ãƒˆãƒ¬ãƒ¼ã‚¹ã™ã‚‹å¿…è¦ãŒã‚ã‚‹é«˜é »åº¦opã«æœ‰ç”¨ã§ã™ã€‚\n",
    "\n",
    "ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãƒ¬ãƒ¼ãƒˆã¯ãƒ«ãƒ¼ãƒˆå‘¼ã³å‡ºã—ã«ã®ã¿é©ç”¨ã•ã‚Œã‚‹ã“ã¨ã«æ³¨æ„ã—ã¦ãã ã•ã„ã€‚opã«ã‚µãƒ³ãƒ—ãƒ«ãƒ¬ãƒ¼ãƒˆãŒã‚ã£ã¦ã‚‚ã€æœ€åˆã«åˆ¥ã®opã«ã‚ˆã£ã¦å‘¼ã³å‡ºã•ã‚ŒãŸå ´åˆã€ãã®ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãƒ¬ãƒ¼ãƒˆã¯ç„¡è¦–ã•ã‚Œã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cpX43hPbdeOZ"
   },
   "outputs": [],
   "source": [
    "@weave.op(tracing_sample_rate=0.1)  # Only trace ~10% of calls\n",
    "def high_frequency_op(x: int) -> int:\n",
    "    return x + 1\n",
    "\n",
    "@weave.op(tracing_sample_rate=1.0)  # Always trace (default)\n",
    "def always_traced_op(x: int) -> int:\n",
    "    return x + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ntJpevJQeLoA"
   },
   "source": [
    "### å‘¼ã³å‡ºã—è¡¨ç¤ºå"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iqOMqMIseYSV",
    "outputId": "f1fa226c-f099-4c5a-f411-6f4bc0caab19"
   },
   "outputs": [],
   "source": [
    "# Decorate your function\n",
    "@weave.op\n",
    "def my_function(name: str):\n",
    "    return f\"Hello, {name}!\"\n",
    "\n",
    "# Call your function -- Weave will automatically track inputs and outputs\n",
    "print(my_function(\"World\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RV9wwY4veaz8"
   },
   "outputs": [],
   "source": [
    "# 1st method\n",
    "result = my_function(\"World\", __weave={\"display_name\": \"My Custom Display Name\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R8d4mcWserLq",
    "outputId": "1938848a-75f4-487b-d46b-09d4995a66cb"
   },
   "outputs": [],
   "source": [
    "# 2nd method\n",
    "result, call = my_function.call(\"World\")\n",
    "call.set_display_name(\"My Custom Display Name\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "-M2abQSbew9M",
    "outputId": "81b67f69-507c-46e1-9d9b-3ae52d23c762"
   },
   "outputs": [],
   "source": [
    "# 3rd method\n",
    "@weave.op(call_display_name=\"My Custom Display Name\")\n",
    "def my_function(name: str):\n",
    "    return f\"Hello, {name}!\"\n",
    "\n",
    "my_function(\"World\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ExWp8BFIxl6g"
   },
   "source": [
    "\n",
    "### PIIã®ç·¨é›†\n",
    "\n",
    "ä¸€éƒ¨ã®çµ„ç¹”ã§ã¯ã€å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ï¼ˆLLMï¼‰ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã§åå‰ã€é›»è©±ç•ªå·ã€ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹ãªã©ã®å€‹äººè­˜åˆ¥æƒ…å ±ï¼ˆPIIï¼‰ã‚’å‡¦ç†ã—ã¾ã™ã€‚ã“ã®ãƒ‡ãƒ¼ã‚¿ã‚’Weights & Biasesï¼ˆW&Bï¼‰Weaveã«ä¿å­˜ã™ã‚‹ã“ã¨ã¯ã€ã‚³ãƒ³ãƒ—ãƒ©ã‚¤ã‚¢ãƒ³ã‚¹ã¨ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ã®ãƒªã‚¹ã‚¯ã‚’ã‚‚ãŸã‚‰ã—ã¾ã™ã€‚\n",
    "\n",
    "æ©Ÿå¯†ãƒ‡ãƒ¼ã‚¿ä¿è­·æ©Ÿèƒ½ã«ã‚ˆã‚Šã€ãƒˆãƒ¬ãƒ¼ã‚¹ãŒWeaveã‚µãƒ¼ãƒãƒ¼ã«é€ä¿¡ã•ã‚Œã‚‹å‰ã«ã€å€‹äººè­˜åˆ¥æƒ…å ±ï¼ˆPIIï¼‰ã‚’è‡ªå‹•çš„ã«ç·¨é›†ã§ãã¾ã™ã€‚ã“ã®æ©Ÿèƒ½ã¯Microsoft Presidioã‚’Weave Python SDKã«çµ±åˆã—ã¦ãŠã‚Šã€SDK ãƒ¬ãƒ™ãƒ«ã§ç·¨é›†è¨­å®šã‚’åˆ¶å¾¡ã§ãã‚‹ã“ã¨ã‚’æ„å‘³ã—ã¾ã™ã€‚\n",
    "\n",
    "[è©³ç´°ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ](https://weave-docs.wandb.ai/guides/tracking/redact-pii)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f957c9c8"
   },
   "source": [
    "# ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°\n",
    "\n",
    "Weaveã§ã¯ã€ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚„ä½¿ç”¨ã—ã¦ã„ã‚‹ãƒ¢ãƒ‡ãƒ«ã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãªã©ã®ã‚¢ã‚»ãƒƒãƒˆã‚’`weave.Objects`å†…ã§ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç®¡ç†ã§ãã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8f6129cc"
   },
   "source": [
    "## ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "08e4e852",
    "outputId": "d5b8c38a-0f00-4e9b-a09f-268f1ea1eb1c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m\u001b[1mweave\u001b[0m: ğŸ“¦ Published to https://wandb.ai/wandb-japan/weave-handson-20251015/weave/objects/pirate_prompt/versions/sShdJdKADGGYJiZ85UxB5wOJprgdiSwLH7VXMYxrHSA\n"
     ]
    }
   ],
   "source": [
    "# StringPrompt1\n",
    "system_prompt = weave.StringPrompt(\"You are a pirate\")\n",
    "weave.publish(system_prompt, name=\"pirate_prompt\")\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "  model=model_name,\n",
    "  messages=[\n",
    "    {\n",
    "      \"role\": \"system\",\n",
    "      \"content\": system_prompt.format()\n",
    "    },\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": \"Explain general relativity in one paragraph.\"\n",
    "    }\n",
    "  ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ke21Y2OJg9Uo",
    "outputId": "f821bf2a-0577-4c96-9661-df18e4eb1524"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m\u001b[1mweave\u001b[0m: ğŸ“¦ Published to https://wandb.ai/wandb-japan/weave-handson-20251015/weave/objects/pirate_prompt/versions/GDp90qF5SsaoFgeMqEliU2e99AXxKDZwBub08BVipRQ\n"
     ]
    }
   ],
   "source": [
    "# StringPrompt2\n",
    "system_prompt = weave.StringPrompt(\"Talk like a pirate. I need to know I'm listening to a pirate.\")\n",
    "weave.publish(system_prompt, name=\"pirate_prompt\")\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "  model=model_name,\n",
    "  messages=[\n",
    "    {\n",
    "      \"role\": \"system\",\n",
    "      \"content\": system_prompt.format()\n",
    "    },\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": \"Explain general relativity in one paragraph.\"\n",
    "    }\n",
    "  ],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZrpZa01lg_-D",
    "outputId": "661423d9-9a58-4635-bce8-4f360838706d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m\u001b[1mweave\u001b[0m: ğŸ“¦ Published to https://wandb.ai/wandb-japan/weave-handson-20251015/weave/objects/dino_prompt/versions/rXZ6qrk5SFNdXIkGehiVXrv9PYX62PbKDyoYHlahoMo\n"
     ]
    }
   ],
   "source": [
    "# MessagesPrompt1\n",
    "prompt = weave.MessagesPrompt([\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a stegosaurus, but don't be too obvious about it.\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"What's good to eat around here?\"\n",
    "    }\n",
    "])\n",
    "weave.publish(prompt, name=\"dino_prompt\")\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "  model=\"gpt-4o\",\n",
    "  messages=prompt.format(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WY5UZLgIhHYW",
    "outputId": "7a1cb983-044b-4d14-81fa-941493c870a0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m\u001b[1mweave\u001b[0m: ğŸ“¦ Published to https://wandb.ai/wandb-japan/weave-handson-20251015/weave/objects/calculator_prompt/versions/YK3NAzaQ0fXYTJjNPlf7JITesCRt3hIGcb2aW1mWOy0\n"
     ]
    }
   ],
   "source": [
    "# parameterizing prompts\n",
    "prompt = weave.StringPrompt(\"Solve the equation {equation}\")\n",
    "weave.publish(prompt, name=\"calculator_prompt\")\n",
    "\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "  model=model_name,\n",
    "  messages=[\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": prompt.format(equation=\"1 + 1 = ?\")\n",
    "    }\n",
    "  ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0c873438"
   },
   "source": [
    "## ãƒ¢ãƒ‡ãƒ«ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e3e60d81",
    "outputId": "f0fb3219-66be-4048-e5a2-3db0c047a651"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That was so easy; it was a piece of cake!\n"
     ]
    }
   ],
   "source": [
    "class WandBInferenceGrammarCorrector(weave.Model):\n",
    "    # ãƒ—ãƒ­ãƒ‘ãƒ†ã‚£ã¯å®Œå…¨ã«ãƒ¦ãƒ¼ã‚¶ãƒ¼å®šç¾©\n",
    "    model_name: str\n",
    "    system_message: str\n",
    "\n",
    "    @weave.op()\n",
    "    def predict(self, user_input):\n",
    "\n",
    "        response = client.chat.completions.create(\n",
    "            model=self.model_name,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": self.system_message},\n",
    "                {\"role\": \"user\", \"content\": user_input},\n",
    "            ],\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "\n",
    "corrector = WandBInferenceGrammarCorrector(\n",
    "    model_name=model_name,\n",
    "    system_message=\"You are a grammar checker, correct the following user input.\",\n",
    ")\n",
    "\n",
    "\n",
    "result = corrector.predict(\"     That was so easy, it was a piece of pie!       \")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "624b04c1"
   },
   "source": [
    "## ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ac2c5145",
    "outputId": "2918a900-8263-49e8-d784-4c35cde32407"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m\u001b[1mweave\u001b[0m: ğŸ“¦ Published to https://wandb.ai/wandb-japan/weave-handson-20251015/weave/objects/grammar-correction/versions/Td1iXzvI1gFHlSq3VNyYytJBnzfzDgAyDaQWHY6X3bo\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ObjectRef(entity='wandb-japan', project='weave-handson-20251015', name='grammar-correction', _digest='Td1iXzvI1gFHlSq3VNyYytJBnzfzDgAyDaQWHY6X3bo', _extra=())"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = weave.Dataset(\n",
    "    name=\"grammar-correction\",\n",
    "    rows=[\n",
    "        {\n",
    "            \"user_input\": \"   That was so easy, it was a piece of pie!   \",\n",
    "            \"expected\": \"That was so easy, it was a piece of cake!\",\n",
    "        },\n",
    "        {\"user_input\": \"  I write good   \",\n",
    "         \"expected\": \"I write well\"},\n",
    "        {\n",
    "            \"user_input\": \"  GPT-3 is smartest AI model.   \",\n",
    "            \"expected\": \"GPT-3 is the smartest AI model.\",\n",
    "        },\n",
    "    ],\n",
    ")\n",
    "\n",
    "weave.publish(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "71c3aaea"
   },
   "source": [
    "## Retrieve Published Objects & Ops\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "caa5c5e0",
    "outputId": "57c8c724-9748-4892-864c-8f189b664a81"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MessagesPrompt(name=None, description=None, messages=WeaveList([{'role': 'system', 'content': \"You are a stegosaurus, but don't be too obvious about it.\"}, {'role': 'user', 'content': \"What's good to eat around here?\"}]))\n"
     ]
    }
   ],
   "source": [
    "ref_url = \"\"\n",
    "prompt = weave.ref(ref_url).get()\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "77b94dd5"
   },
   "source": [
    "# ã‚ªãƒ•ãƒ©ã‚¤ãƒ³è©•ä¾¡\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "72bdf072"
   },
   "source": [
    "## æ–¹æ³•1: [æ¨™æº–ãƒ¡ã‚½ãƒƒãƒ‰](https://weave-docs.wandb.ai/guides/core-types/evaluations)\n",
    "äºˆæ¸¬ã¨è©•ä¾¡ã®ä¸¡æ–¹ã‚’ã‚µãƒ³ãƒ—ãƒ«ã”ã¨ã«å®Ÿè¡Œã—ã€è©•ä¾¡ã‚’ã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 148
    },
    "id": "fpiwHM4WdV9A",
    "outputId": "f1cd62cf-422b-4de7-8990-a24b8a9f8f0b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m\u001b[1mweave\u001b[0m: wandb version 0.22.2 is available!  To upgrade, please run:\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[36m\u001b[1mweave\u001b[0m: Logged in as Weights & Biases user: keisuke-kamata.\n",
      "\u001b[36m\u001b[1mweave\u001b[0m: View Weave data at https://wandb.ai/wandb-japan/weave-handson-20251015/weave\n",
      "\u001b[36m\u001b[1mweave\u001b[0m: Evaluated 1 of 3 examples\n",
      "\u001b[36m\u001b[1mweave\u001b[0m: Evaluated 2 of 3 examples\n",
      "\u001b[36m\u001b[1mweave\u001b[0m: Evaluated 3 of 3 examples\n",
      "\u001b[36m\u001b[1mweave\u001b[0m: Evaluation summary {\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:   \"match_score\": {\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:     \"match\": {\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:       \"true_count\": 0,\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:       \"true_fraction\": 0.0\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:     }\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:   },\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:   \"model_latency\": {\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:     \"mean\": 0.005383412043253581\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:   }\n",
      "\u001b[36m\u001b[1mweave\u001b[0m: }\n",
      "\u001b[36m\u001b[1mweave\u001b[0m: ğŸ© https://wandb.ai/wandb-japan/weave-handson-20251015/r/call/0199e56d-0874-7b64-a215-7688af8ce2c6\n",
      "\u001b[36m\u001b[1mweave\u001b[0m: ğŸ© https://wandb.ai/wandb-japan/weave-handson-20251015/r/call/0199e580-1317-7bfe-b9f8-3fea239bf731\n",
      "\u001b[36m\u001b[1mweave\u001b[0m: ğŸ© https://wandb.ai/wandb-japan/weave-handson-20251015/r/call/0199e581-cbcc-7121-a459-b6d06bf4f37d\n",
      "\u001b[36m\u001b[1mweave\u001b[0m: ğŸ© https://wandb.ai/wandb-japan/weave-handson-20251015/r/call/0199e581-cbd2-7099-a0eb-c48ec7bc1a9f\n",
      "\u001b[36m\u001b[1mweave\u001b[0m: ğŸ© https://wandb.ai/wandb-japan/weave-handson-20251015/r/call/0199e581-cbd5-768e-999d-b8d28a0384de\n"
     ]
    }
   ],
   "source": [
    "from weave import Evaluation, Model\n",
    "import weave\n",
    "import asyncio\n",
    "import openai\n",
    "import os\n",
    "\n",
    "\n",
    "examples = [\n",
    "    {\"question\": \"What is the capital of France?\", \"expected\": \"Paris\"},\n",
    "    {\"question\": \"Who wrote 'To Kill a Mockingbird'?\", \"expected\": \"Harper Lee\"},\n",
    "    {\"question\": \"What is the square root of 64?\", \"expected\": \"8\"},\n",
    "]\n",
    "\n",
    "@weave.op()\n",
    "def exact_match_scorer(expected: str, output: dict) -> dict:\n",
    "    \"\"\"Exact string matching evaluation\"\"\"\n",
    "    generated = output.get('generated_text', '')\n",
    "    return {'exact_match': expected.lower().strip() == generated.lower().strip()}\n",
    "\n",
    "@weave.op()\n",
    "def contains_answer_scorer(expected: str, output: dict) -> dict:\n",
    "    \"\"\"Check if the expected answer is contained in the generated text\"\"\"\n",
    "    generated = output.get('generated_text', '').lower()\n",
    "    expected_lower = expected.lower()\n",
    "    return {'contains_answer': expected_lower in generated}\n",
    "\n",
    "class OpenAIQAModel(Model):\n",
    "    model_name: str = \"gpt-4o-mini\"\n",
    "    temperature: float = 0.3\n",
    "    max_tokens: int = 100\n",
    "\n",
    "    @weave.op()\n",
    "    def predict(self, question: str):\n",
    "        client = openai.OpenAI()\n",
    "        \"\"\"Generate answer using OpenAI API\"\"\"\n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=self.model_name,\n",
    "                messages=[\n",
    "                    {\n",
    "                        \"role\": \"system\",\n",
    "                        \"content\": \"You are a helpful assistant that provides accurate and concise answers to questions.\"\n",
    "                    },\n",
    "                    {\n",
    "                        \"role\": \"user\", \n",
    "                        \"content\": f\"Question: {question}\"\n",
    "                    }\n",
    "                ],\n",
    "                temperature=self.temperature,\n",
    "                max_tokens=self.max_tokens\n",
    "            )\n",
    "            \n",
    "            answer = response.choices[0].message.content.strip()\n",
    "            return {\n",
    "                'generated_text': answer,\n",
    "                'model': self.model_name,\n",
    "                'question': question\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error calling OpenAI API: {e}\")\n",
    "            return {\n",
    "                'generated_text': f\"Error: {str(e)}\",\n",
    "                'model': self.model_name,\n",
    "                'question': question\n",
    "            }\n",
    "\n",
    "# Create model and evaluation\n",
    "model = OpenAIQAModel()\n",
    "evaluation = Evaluation(\n",
    "    dataset=examples, \n",
    "    scorers=[exact_match_scorer, contains_answer_scorer]\n",
    ")\n",
    "\n",
    "# Run evaluation\n",
    "asyncio.run(evaluation.evaluate(model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v6VE09bweQzu"
   },
   "source": [
    "## æ–¹æ³•2: [EvaluationLogger](https://weave-docs.wandb.ai/guides/evaluation/evaluation_logger)\n",
    "ãƒãƒƒãƒäºˆæ¸¬ãŒé©ç”¨ã§ãã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5jeULyjweeBI",
    "outputId": "fa75ee71-71f9-4fbf-b211-6206714f01ce"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'weave.flow.eval_imperative'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mweave\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mweave\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01meval_imperative\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m EvaluationLogger\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Initialize the logger with optional metadata\u001b[39;00m\n\u001b[32m      5\u001b[39m eval_logger = EvaluationLogger(\n\u001b[32m      6\u001b[39m     model=\u001b[33m\"\u001b[39m\u001b[33mmy_local_model\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      7\u001b[39m     dataset=\u001b[33m\"\u001b[39m\u001b[33mmy_dataset\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      8\u001b[39m )\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'weave.flow.eval_imperative'"
     ]
    }
   ],
   "source": [
    "import weave\n",
    "from weave import EvaluationLogger\n",
    "\n",
    "# Initialize the logger with optional metadata\n",
    "eval_logger = EvaluationLogger(\n",
    "    model=\"my_local_model\",\n",
    "    dataset=\"my_dataset\"\n",
    ")\n",
    "\n",
    "# Example input data\n",
    "eval_samples = [\n",
    "    {'inputs': {'a': 1, 'b': 2}, 'expected': 3},\n",
    "    {'inputs': {'a': 2, 'b': 3}, 'expected': 5},\n",
    "    {'inputs': {'a': 3, 'b': 4}, 'expected': 7},\n",
    "]\n",
    "\n",
    "# Local model logic: simply add the numbers\n",
    "@weave.op\n",
    "def user_model(a: int, b: int) -> int:\n",
    "    return a + b\n",
    "# Example model logic using OpenAI\n",
    "@weave.op\n",
    "def user_model(a: int, b: int) -> int:\n",
    "    oai = openai.OpenAI()\n",
    "    response = oai.chat.completions.create(\n",
    "        messages=[{\"role\": \"user\", \"content\": f\"What is {a}+{b}?\"}],\n",
    "        model=\"gpt-4o-mini\"\n",
    "    )\n",
    "    # Use the response in some way (here we just return a + b for simplicity)\n",
    "    return a + b\n",
    "\n",
    "# Iterate through examples, predict, and log\n",
    "for sample in eval_samples:\n",
    "    inputs = sample[\"inputs\"]\n",
    "    model_output = user_model(**inputs) # Pass inputs as kwargs\n",
    "\n",
    "    # Log the prediction input and output\n",
    "    pred_logger = eval_logger.log_prediction(\n",
    "        inputs=inputs,\n",
    "        output=model_output\n",
    "    )\n",
    "\n",
    "    # Calculate and log a score for this prediction\n",
    "    expected = sample[\"expected\"]\n",
    "    correctness_score = model_output == expected\n",
    "    pred_logger.log_score(\n",
    "        scorer=\"correctness\", # Simple string name for the scorer\n",
    "        score=correctness_score\n",
    "    )\n",
    "\n",
    "    # Finish logging for this specific prediction\n",
    "    pred_logger.finish()\n",
    "\n",
    "# Log a final summary for the entire evaluation.\n",
    "# Weave auto-aggregates the 'correctness' scores logged above.\n",
    "summary_stats = {\"subjective_overall_score\": 0.8}\n",
    "eval_logger.log_summary(summary_stats)\n",
    "\n",
    "print(\"Evaluation logging complete. View results in the Weave UI.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lk4DY3RqkKnf"
   },
   "source": [
    "# ã‚ªãƒ³ãƒ©ã‚¤ãƒ³è©•ä¾¡"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9ZRRHZyaDdpu"
   },
   "source": [
    "# ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 356
    },
    "id": "kLBVpBlkxVdi",
    "outputId": "88608be4-dd3b-4609-dd4d-f160fe021222"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m\u001b[1mweave\u001b[0m: wandb version 0.22.2 is available!  To upgrade, please run:\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[36m\u001b[1mweave\u001b[0m: Logged in as Weights & Biases user: keisuke-kamata.\n",
      "\u001b[36m\u001b[1mweave\u001b[0m: View Weave data at https://wandb.ai/wandb-japan/weave-handson-20251015/weave\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'0199e57a-4aa2-7e86-9d4d-febd8f1e8b45'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client = weave.init(PROJECT)\n",
    "call = client.get_call(\"0199e574-02a6-7444-80f2-40b5e8fa4e06\") #@param\n",
    "\n",
    "# Adding an emoji reaction\n",
    "call.feedback.add_reaction(\"ğŸ‘\")\n",
    "\n",
    "# Adding a note\n",
    "call.feedback.add_note(\"this is a note\")\n",
    "\n",
    "# Adding custom key/value pairs.\n",
    "# The first argument is a user-defined \"type\" string.\n",
    "# Feedback must be JSON serializable and less than 1 KB when serialized.\n",
    "call.feedback.add(\"correctness\", { \"value\": 5 })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M497nWagx0PP"
   },
   "source": [
    "# Scorers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "d0tqUV9_1KgA"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ApplyScorerSuccess(result={'length': 13, 'is_short': True}, score_call=Call(_op_name=<Future at 0x11992c990 state=running>, trace_id='0199e580-1316-79ab-b0cf-985fe749ce91', project_id='wandb-japan/weave-handson-20251015', parent_id=None, inputs={'self': ObjectRef(entity='wandb-japan', project='weave-handson-20251015', name='LengthScorer', _digest=<Future at 0x119943e10 state=pending>, _extra=()), 'output': 'Hello, world!'}, id='0199e580-1317-7bfe-b9f8-3fea239bf731', output={'length': 13, 'is_short': True}, exception=None, summary={'status_counts': {<TraceStatus.SUCCESS: 'success'>: 1, <TraceStatus.ERROR: 'error'>: 0}}, _display_name=None, attributes=AttributesDict({'weave': {'python': {'type': 'function'}, 'client_version': '0.52.9', 'source': 'python-sdk', 'sys_version': '3.11.13 (main, Jun  3 2025, 18:38:25) [Clang 17.0.0 (clang-1700.0.13.3)]', 'os_name': 'Darwin', 'os_version': 'Darwin Kernel Version 24.6.0: Mon Aug 11 21:16:34 PDT 2025; root:xnu-11417.140.69.701.11~1/RELEASE_ARM64_T6020', 'os_release': '24.6.0'}}), started_at=None, ended_at=datetime.datetime(2025, 10, 15, 1, 33, 20, 25474, tzinfo=datetime.timezone.utc), deleted_at=None, thread_id=None, turn_id=None, wb_run_id=None, wb_run_step=None, wb_run_step_end=None, _children=[], _feedback=None))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m\u001b[1mweave\u001b[0m: ğŸ© https://wandb.ai/wandb-japan/weave-handson-20251015/r/call/0199e580-130f-7664-9773-3915adba8857\n",
      "\u001b[36m\u001b[1mweave\u001b[0m: ğŸ© https://wandb.ai/wandb-japan/weave-handson-20251015/r/call/0199e581-cbc5-7735-8218-4420efa34b6d\n",
      "\u001b[36m\u001b[1mweave\u001b[0m: ğŸ© https://wandb.ai/wandb-japan/weave-handson-20251015/r/call/0199e581-cbd0-7488-8ed0-ee2c66fd64ab\n",
      "\u001b[36m\u001b[1mweave\u001b[0m: ğŸ© https://wandb.ai/wandb-japan/weave-handson-20251015/r/call/0199e581-cbca-7efa-9ba4-0f2ba4456b0f\n"
     ]
    }
   ],
   "source": [
    "import weave\n",
    "from weave import Scorer\n",
    "\n",
    "\n",
    "class LengthScorer(Scorer):\n",
    "    @weave.op\n",
    "    def score(self, output: str) -> dict:\n",
    "        \"\"\"A simple scorer that checks output length.\"\"\"\n",
    "        return {\n",
    "            \"length\": len(output),\n",
    "            \"is_short\": len(output) < 100\n",
    "        }\n",
    "\n",
    "@weave.op\n",
    "def generate_text(prompt: str) -> str:\n",
    "    return \"Hello, world!\"\n",
    "\n",
    "# Get both result and Call object\n",
    "result, call = generate_text.call(\"Say hello\")\n",
    "\n",
    "# Now you can apply scorers\n",
    "await call.apply_scorer(LengthScorer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SfipbjSi2dee"
   },
   "source": [
    "## ã‚¬ãƒ¼ãƒ‰ãƒ¬ãƒ¼ãƒ«ã¨ã—ã¦ã®Scorersã®ä½¿ç”¨\n",
    "ã‚¬ãƒ¼ãƒ‰ãƒ¬ãƒ¼ãƒ«ã¯ã€LLMå‡ºåŠ›ãŒãƒ¦ãƒ¼ã‚¶ãƒ¼ã«åˆ°é”ã™ã‚‹å‰ã«å®Ÿè¡Œã•ã‚Œã‚‹å®‰å…¨ãƒã‚§ãƒƒã‚¯ã¨ã—ã¦æ©Ÿèƒ½ã—ã¾ã™ã€‚å®Ÿç”¨çš„ãªä¾‹ã‚’ä»¥ä¸‹ã«ç¤ºã—ã¾ã™ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "r9rKr-_g2YLk"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: hello\n",
      "Response: Hello! How can I help you?\n",
      "\n",
      "Prompt: bad\n",
      "Response: I cannot generate that content: Detected toxic language\n",
      "\n",
      "Prompt: good\n",
      "Response: You are wonderful!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import weave\n",
    "from weave import Scorer\n",
    "import asyncio\n",
    "import nest_asyncio  # Required for running asyncio in Google Colab\n",
    "\n",
    "# Apply nest_asyncio to avoid event loop issues in Google Colab\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# ==== 1. Define text generation function ====\n",
    "@weave.op\n",
    "def generate_text(prompt: str) -> str:\n",
    "    \"\"\"Simulated LLM text generation (basic logic).\"\"\"\n",
    "    responses = {\n",
    "        \"hello\": \"Hello! How can I help you?\",\n",
    "        \"bad\": \"You are terrible!\",  # Example of toxic response\n",
    "        \"good\": \"You are wonderful!\",\n",
    "    }\n",
    "    return responses.get(prompt.lower(), \"I don't understand your request.\")\n",
    "\n",
    "# ==== 2. Define the Toxicity Scorer ====\n",
    "class ToxicityScorer(Scorer):\n",
    "    @weave.op\n",
    "    def score(self, output: str) -> dict:\n",
    "        \"\"\"\n",
    "        Evaluate the generated content for toxic language.\n",
    "        \"\"\"\n",
    "        toxic_words = {\"terrible\", \"hate\", \"stupid\"}  # Simple keyword-based detection\n",
    "        flagged = any(word in output.lower() for word in toxic_words)\n",
    "\n",
    "        return {\n",
    "            \"flagged\": flagged,\n",
    "            \"reason\": \"Detected toxic language\" if flagged else None\n",
    "        }\n",
    "\n",
    "# ==== 3. Function to generate safe responses ====\n",
    "async def generate_safe_response(prompt: str) -> str:\n",
    "    # Generate text using LLM\n",
    "    result, call = generate_text.call(prompt)\n",
    "\n",
    "    # Apply toxicity scoring\n",
    "    safety = await call.apply_scorer(ToxicityScorer())\n",
    "\n",
    "    # If flagged as toxic, return a warning message\n",
    "    if safety.result[\"flagged\"]:\n",
    "        return f\"I cannot generate that content: {safety.result['reason']}\"\n",
    "\n",
    "    return result\n",
    "\n",
    "# ==== 4. Run test cases ====\n",
    "async def main():\n",
    "    prompts = [\"hello\", \"bad\", \"good\"]\n",
    "\n",
    "    for prompt in prompts:\n",
    "        response = await generate_safe_response(prompt)\n",
    "        print(f\"Prompt: {prompt}\\nResponse: {response}\\n\")\n",
    "\n",
    "# Run the async function in Google Colab\n",
    "loop = asyncio.get_event_loop()\n",
    "loop.run_until_complete(main())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kdfIOJOGDiL9"
   },
   "source": [
    "## ãƒ¢ãƒ‹ã‚¿ãƒ¼ã¨ã—ã¦ã®Scorersã®ä½¿ç”¨\n",
    "\n",
    "ãƒ¢ãƒ‹ã‚¿ãƒ¼ã¯ã€æ“ä½œã‚’ãƒ–ãƒ­ãƒƒã‚¯ã™ã‚‹ã“ã¨ãªãã€æ™‚é–“ã®çµŒéã¨ã¨ã‚‚ã«å“è³ªãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’è¿½è·¡ã™ã‚‹ã®ã«å½¹ç«‹ã¡ã¾ã™ã€‚ã“ã‚Œã¯ä»¥ä¸‹ã«æœ‰ç”¨ã§ã™ï¼š\n",
    "\n",
    "* å“è³ªãƒˆãƒ¬ãƒ³ãƒ‰ã®ç‰¹å®š\n",
    "* ãƒ¢ãƒ‡ãƒ«ãƒ‰ãƒªãƒ•ãƒˆã®æ¤œå‡º\n",
    "* ãƒ¢ãƒ‡ãƒ«æ”¹å–„ã®ãŸã‚ã®ãƒ‡ãƒ¼ã‚¿åé›†\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hpEQ3TDS3lVh"
   },
   "outputs": [],
   "source": [
    "import weave\n",
    "from weave import Scorer\n",
    "import json\n",
    "import xml.etree.ElementTree as ET\n",
    "import random\n",
    "import asyncio\n",
    "import nest_asyncio  # Required for Google Colab\n",
    "\n",
    "# Apply nest_asyncio to avoid event loop issues in Google Colab\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# ==== 1. Define Text Generation Function ====\n",
    "@weave.op\n",
    "def generate_text(prompt: str) -> str:\n",
    "    \"\"\"Simulated LLM response generation.\"\"\"\n",
    "    if prompt.lower() == \"json\":\n",
    "        return '{\"message\": \"Hello, world!\"}'  # Valid JSON\n",
    "    elif prompt.lower() == \"xml\":\n",
    "        return \"<message>Hello, world!</message>\"  # Valid XML\n",
    "    else:\n",
    "        return \"Generated response...\"\n",
    "\n",
    "# ==== 2. Custom Scorer for JSON Validation ====\n",
    "class CustomJSONScorer(Scorer):\n",
    "    @weave.op\n",
    "    def score(self, output: str) -> dict:\n",
    "        \"\"\"Check if the output is valid JSON.\"\"\"\n",
    "        try:\n",
    "            json.loads(output)\n",
    "            return {\"valid_json\": True}\n",
    "        except json.JSONDecodeError:\n",
    "            return {\"valid_json\": False}\n",
    "\n",
    "# ==== 3. Custom Scorer for XML Validation ====\n",
    "class CustomXMLScorer(Scorer):\n",
    "    @weave.op\n",
    "    def score(self, output: str) -> dict:\n",
    "        \"\"\"Check if the output is valid XML.\"\"\"\n",
    "        try:\n",
    "            ET.fromstring(output)\n",
    "            return {\"valid_xml\": True}\n",
    "        except ET.ParseError:\n",
    "            return {\"valid_xml\": False}\n",
    "\n",
    "# ==== 4. Function to Generate Response with Monitoring ====\n",
    "async def generate_with_monitoring(prompt: str) -> str:\n",
    "    \"\"\"\n",
    "    Generates a response and applies monitoring (randomly 10% of the time).\n",
    "    \"\"\"\n",
    "    # Generate text and capture tracking info\n",
    "    result, call = generate_text.call(prompt)\n",
    "\n",
    "    # Sample monitoring (only apply scorers to 10% of calls)\n",
    "    if random.random() < 0.1:\n",
    "        # Apply manual scorers asynchronously\n",
    "        json_score = await call.apply_scorer(CustomJSONScorer())\n",
    "        xml_score = await call.apply_scorer(CustomXMLScorer())\n",
    "\n",
    "        print(f\"Monitoring Applied - JSON Valid: {json_score.result['valid_json']}, XML Valid: {xml_score.result['valid_xml']}\")\n",
    "\n",
    "    return result\n",
    "\n",
    "# ==== 5. Run Test Cases ====\n",
    "async def main():\n",
    "    prompts = [\"json\", \"xml\", \"text\"]\n",
    "\n",
    "    for prompt in prompts:\n",
    "        response = await generate_with_monitoring(prompt)\n",
    "        print(f\"Prompt: {prompt}\\nResponse: {response}\\n\")\n",
    "\n",
    "# Run the async function in Google Colab\n",
    "loop = asyncio.get_event_loop()\n",
    "loop.run_until_complete(main())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ãƒ¡ãƒ‡ã‚£ã‚¢"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ç”»åƒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m\u001b[1mweave\u001b[0m: wandb version 0.22.2 is available!  To upgrade, please run:\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[36m\u001b[1mweave\u001b[0m: Logged in as Weights & Biases user: keisuke-kamata.\n",
      "\u001b[36m\u001b[1mweave\u001b[0m: View Weave data at https://wandb.ai/wandb-japan/weave-handson-20251015/weave\n",
      "\u001b[36m\u001b[1mweave\u001b[0m: ğŸ© https://wandb.ai/wandb-japan/weave-handson-20251015/r/call/0199e585-4cbb-75d4-8d3e-1b6059c7fe48\n"
     ]
    }
   ],
   "source": [
    "import weave\n",
    "from openai import OpenAI\n",
    "import requests\n",
    "from PIL import Image\n",
    "\n",
    "weave.init(project_name=PROJECT)\n",
    "#os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
    "client = OpenAI(\n",
    "    api_key=os.environ.get(\"OPENAI_API_KEY\")\n",
    ")\n",
    "\n",
    "@weave.op()\n",
    "def generate_image(prompt: str) -> Image:\n",
    "    response = client.images.generate(\n",
    "        model=\"dall-e-3\",\n",
    "        prompt=prompt,\n",
    "        size=\"1024x1024\",\n",
    "        quality=\"standard\",\n",
    "        n=1,\n",
    "    )\n",
    "    image_url = response.data[0].url\n",
    "    image_response = requests.get(image_url, stream=True)\n",
    "    image = Image.open(image_response.raw)\n",
    "\n",
    "    # return a PIL.Image.Image object to be logged as an image\n",
    "    return image\n",
    "\n",
    "image = generate_image(\"a cat with a pumpkin hat\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## éŸ³å£°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m\u001b[1mweave\u001b[0m: Flushing 2 pending tasks...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â†’ Flushing tasks: 0.0% (0/2)\n",
      "â†’ Flushing tasks: 0.0% (0/2)\n",
      "â†’ Flushing tasks: 0.0% (0/2)\n",
      "â†’ Flushing tasks: 0.0% (0/2)\n",
      "â†’ Flushing tasks: 0.0% (0/2)\n",
      "â†’ Flushing tasks: 0.0% (0/2)\n",
      "â†’ Flushing tasks: 0.0% (0/2)\n",
      "â†’ Flushing tasks: 50.0% (1/2)\n",
      "â†’ Flushing tasks: 50.0% (1/2)\n",
      "â†’ Flushing tasks: 33.3% (1/3)\n",
      "â†’ Flushing tasks: 33.3% (1/3)\n",
      "â†’ Flushing tasks: 33.3% (1/3)\n",
      "â†’ Flushing tasks: 33.3% (1/3)\n",
      "â†’ Flushing tasks: 33.3% (1/3)\n",
      "â†’ Flushing tasks: 33.3% (1/3)\n",
      "â†’ Flushing tasks: 33.3% (1/3)\n",
      "â†’ Flushing tasks: 33.3% (1/3)\n",
      "â†’ Flushing tasks: 33.3% (1/3)\n",
      "â†’ Flushing tasks: 66.7% (2/3)\n",
      "â†’ Flushing tasks: 66.7% (2/3)\n",
      "â†’ Flushing tasks: 66.7% (2/3)\n",
      "â†’ Flushing tasks: 66.7% (2/3)\n",
      "â†’ Flushing tasks: 66.7% (2/3)\n",
      "â†’ Flushing tasks: 100.0% (1/1)\n",
      "âœ“ Flushing tasks: 100.0% (1/1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m\u001b[1mweave\u001b[0m: wandb version 0.22.2 is available!  To upgrade, please run:\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[36m\u001b[1mweave\u001b[0m: Logged in as Weights & Biases user: keisuke-kamata.\n",
      "\u001b[36m\u001b[1mweave\u001b[0m: View Weave data at https://wandb.ai/wandb-japan/weave-handson-20251015/weave\n",
      "\u001b[36m\u001b[1mweave\u001b[0m: ğŸ© https://wandb.ai/wandb-japan/weave-handson-20251015/r/call/0199e585-9f0b-7930-aba3-e764b242dab1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<wave.Wave_read at 0x11995af10>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import weave\n",
    "from openai import OpenAI\n",
    "import wave\n",
    "\n",
    "weave.init(project_name=PROJECT)\n",
    "#os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
    "client = OpenAI(\n",
    "    api_key=os.environ.get(\"OPENAI_API_KEY\")\n",
    ")\n",
    "@weave.op\n",
    "def make_audio_file_streaming(text: str) -> wave.Wave_read:\n",
    "    with client.audio.speech.with_streaming_response.create(\n",
    "        model=\"tts-1\",\n",
    "        voice=\"alloy\",\n",
    "        input=text,\n",
    "        response_format=\"wav\",\n",
    "    ) as res:\n",
    "        res.stream_to_file(\"output.wav\")\n",
    "\n",
    "    # return a wave.Wave_read object to be logged as audio\n",
    "    return wave.open(\"output.wav\")\n",
    "\n",
    "make_audio_file_streaming(\"Hello, how are you? What did you do yesterday?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## å‹•ç”»"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m\u001b[1mweave\u001b[0m: Flushing 1 pending tasks...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â†’ Flushing tasks: 0.0% (0/1)\n",
      "â†’ Flushing tasks: 0.0% (0/1)\n",
      "â†’ Flushing tasks: 0.0% (0/1)\n",
      "â†’ Flushing tasks: 0.0% (0/1)\n",
      "â†’ Flushing tasks: 0.0% (0/1)\n",
      "â†’ Flushing tasks: 100.0% (1/1)\n",
      "âœ“ Flushing tasks: 100.0% (1/1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m\u001b[1mweave\u001b[0m: wandb version 0.22.2 is available!  To upgrade, please run:\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[36m\u001b[1mweave\u001b[0m: Logged in as Weights & Biases user: keisuke-kamata.\n",
      "\u001b[36m\u001b[1mweave\u001b[0m: View Weave data at https://wandb.ai/wandb-japan/weave-handson-20251015/weave\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Missing key inputs argument! To use the Google AI API, provide (`api_key`) arguments. To use the Google Cloud API, provide (`vertexai`, `project` & `location`) arguments.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 42\u001b[39m\n\u001b[32m     38\u001b[39m         generated_video.video.save(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mvideo\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.mp4\u001b[39m\u001b[33m\"\u001b[39m)  \u001b[38;5;66;03m# save the video\u001b[39;00m\n\u001b[32m     39\u001b[39m         save_videos_in_weave(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mvideo\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.mp4\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m \u001b[43mgenerate_videos\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mPanning wide shot of a calico kitten sleeping in the sunshine\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Project/wandb-handson/weave_introduction_handson/.venv/lib/python3.11/site-packages/weave/trace/op.py:1244\u001b[39m, in \u001b[36mop.<locals>.op_deco.<locals>.create_wrapper.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m   1242\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[32m   1243\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper\u001b[39m(*args: P.args, **kwargs: P.kwargs) -> R:\n\u001b[32m-> \u001b[39m\u001b[32m1244\u001b[39m     res, _ = \u001b[43m_call_sync_func\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1245\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mOp\u001b[49m\u001b[43m[\u001b[49m\u001b[43mP\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mR\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwrapper\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m__should_raise\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m   1246\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1247\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(R, res)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Project/wandb-handson/weave_introduction_handson/.venv/lib/python3.11/site-packages/weave/trace/op.py:501\u001b[39m, in \u001b[36m_call_sync_func\u001b[39m\u001b[34m(op, __weave, __should_raise, __require_explicit_finish, *args, **kwargs)\u001b[39m\n\u001b[32m    498\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m output\n\u001b[32m    500\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m501\u001b[39m     res = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    502\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    503\u001b[39m     finish(exception=e)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 22\u001b[39m, in \u001b[36mgenerate_videos\u001b[39m\u001b[34m(prompt)\u001b[39m\n\u001b[32m     20\u001b[39m \u001b[38;5;129m@weave\u001b[39m.op()\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate_videos\u001b[39m(prompt):\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m     client = \u001b[43mgenai\u001b[49m\u001b[43m.\u001b[49m\u001b[43mClient\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# read API key from GOOGLE_API_KEY\u001b[39;00m\n\u001b[32m     23\u001b[39m     operation = client.models.generate_videos(\n\u001b[32m     24\u001b[39m         model=\u001b[33m\"\u001b[39m\u001b[33mveo-2.0-generate-001\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     25\u001b[39m         prompt=\u001b[33m\"\u001b[39m\u001b[33mPanning wide shot of a calico kitten sleeping in the sunshine\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     29\u001b[39m             ),\n\u001b[32m     30\u001b[39m         )\n\u001b[32m     32\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m operation.done:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Project/wandb-handson/weave_introduction_handson/.venv/lib/python3.11/site-packages/google/genai/client.py:265\u001b[39m, in \u001b[36mClient.__init__\u001b[39m\u001b[34m(self, vertexai, api_key, credentials, project, location, debug_config, http_options)\u001b[39m\n\u001b[32m    262\u001b[39m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    263\u001b[39m     http_options = HttpOptions(base_url=base_url)\n\u001b[32m--> \u001b[39m\u001b[32m265\u001b[39m \u001b[38;5;28mself\u001b[39m._api_client = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_api_client\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    266\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvertexai\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvertexai\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    267\u001b[39m \u001b[43m    \u001b[49m\u001b[43mapi_key\u001b[49m\u001b[43m=\u001b[49m\u001b[43mapi_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    268\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcredentials\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcredentials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    269\u001b[39m \u001b[43m    \u001b[49m\u001b[43mproject\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproject\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    270\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    271\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdebug_config\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_debug_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    272\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhttp_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhttp_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    273\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    275\u001b[39m \u001b[38;5;28mself\u001b[39m._aio = AsyncClient(\u001b[38;5;28mself\u001b[39m._api_client)\n\u001b[32m    276\u001b[39m \u001b[38;5;28mself\u001b[39m._models = Models(\u001b[38;5;28mself\u001b[39m._api_client)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Project/wandb-handson/weave_introduction_handson/.venv/lib/python3.11/site-packages/google/genai/client.py:311\u001b[39m, in \u001b[36mClient._get_api_client\u001b[39m\u001b[34m(vertexai, api_key, credentials, project, location, debug_config, http_options)\u001b[39m\n\u001b[32m    294\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m debug_config \u001b[38;5;129;01mand\u001b[39;00m debug_config.client_mode \u001b[38;5;129;01min\u001b[39;00m [\n\u001b[32m    295\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mrecord\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m    296\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mreplay\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m    297\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mauto\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m    298\u001b[39m ]:\n\u001b[32m    299\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m ReplayApiClient(\n\u001b[32m    300\u001b[39m       mode=debug_config.client_mode,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[32m    301\u001b[39m       replay_id=debug_config.replay_id,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    308\u001b[39m       http_options=http_options,\n\u001b[32m    309\u001b[39m   )\n\u001b[32m--> \u001b[39m\u001b[32m311\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mBaseApiClient\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    312\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvertexai\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvertexai\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    313\u001b[39m \u001b[43m    \u001b[49m\u001b[43mapi_key\u001b[49m\u001b[43m=\u001b[49m\u001b[43mapi_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    314\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcredentials\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcredentials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    315\u001b[39m \u001b[43m    \u001b[49m\u001b[43mproject\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproject\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    316\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    317\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhttp_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhttp_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    318\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Project/wandb-handson/weave_introduction_handson/.venv/lib/python3.11/site-packages/google/genai/_api_client.py:661\u001b[39m, in \u001b[36mBaseApiClient.__init__\u001b[39m\u001b[34m(self, vertexai, api_key, credentials, project, location, http_options)\u001b[39m\n\u001b[32m    659\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# Implicit initialization or missing arguments.\u001b[39;00m\n\u001b[32m    660\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.api_key:\n\u001b[32m--> \u001b[39m\u001b[32m661\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    662\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mMissing key inputs argument! To use the Google AI API,\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    663\u001b[39m         \u001b[33m'\u001b[39m\u001b[33m provide (`api_key`) arguments. To use the Google Cloud API,\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    664\u001b[39m         \u001b[33m'\u001b[39m\u001b[33m provide (`vertexai`, `project` & `location`) arguments.\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    665\u001b[39m     )\n\u001b[32m    666\u001b[39m   \u001b[38;5;28mself\u001b[39m._http_options.base_url = \u001b[33m'\u001b[39m\u001b[33mhttps://generativelanguage.googleapis.com/\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    667\u001b[39m   \u001b[38;5;28mself\u001b[39m._http_options.api_version = \u001b[33m'\u001b[39m\u001b[33mv1beta\u001b[39m\u001b[33m'\u001b[39m\n",
      "\u001b[31mValueError\u001b[39m: Missing key inputs argument! To use the Google AI API, provide (`api_key`) arguments. To use the Google Cloud API, provide (`vertexai`, `project` & `location`) arguments."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m\u001b[1mweave\u001b[0m: ğŸ© https://wandb.ai/wandb-japan/weave-handson-20251015/r/call/0199e585-b66e-7723-a3a5-904a0b3be4ce\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "from moviepy.editor import VideoFileClip, ColorClip, VideoClip\n",
    "import weave\n",
    "\n",
    "weave.init(PROJECT)\n",
    "\n",
    "@weave.op()\n",
    "def store_videos(name, client, generated_video):\n",
    "  client.files.download(file=generated_video.video)\n",
    "  generated_video.video.save(f\"new_video{name}.mp4\")  # save the video\n",
    "\n",
    "@weave.op()\n",
    "def save_videos_in_weave(video_path):\n",
    "    clip = VideoFileClip(video_path, has_mask=False, audio=True)\n",
    "    new_clip = clip.subclip(0, 1)\n",
    "    return new_clip\n",
    "\n",
    "@weave.op()\n",
    "def generate_videos(prompt):\n",
    "    client = genai.Client()  # read API key from GOOGLE_API_KEY\n",
    "    operation = client.models.generate_videos(\n",
    "        model=\"veo-2.0-generate-001\",\n",
    "        prompt=\"Panning wide shot of a calico kitten sleeping in the sunshine\",\n",
    "        config=types.GenerateVideosConfig(\n",
    "            person_generation=\"dont_allow\",  # \"dont_allow\" or \"allow_adult\"\n",
    "            aspect_ratio=\"16:9\",  # \"16:9\" or \"9:16\"\n",
    "            ),\n",
    "        )\n",
    "    \n",
    "    while not operation.done:\n",
    "        time.sleep(10)\n",
    "        operation = client.operations.get(operation)\n",
    "    \n",
    "    for n, generated_video in enumerate(operation.response.generated_videos):\n",
    "        client.files.download(file=generated_video.video)\n",
    "        generated_video.video.save(f\"video{n}.mp4\")  # save the video\n",
    "        save_videos_in_weave(f\"video{n}.mp4\")\n",
    "        \n",
    " \n",
    "generate_videos(\"Panning wide shot of a calico kitten sleeping in the sunshine\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
