{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "974b2c96"
   },
   "source": [
    "# 🏃‍♀️ クイックスタート\n",
    "\n",
    "Weaveの機能を理解していきましょう！\n",
    "\n",
    "# <a href=\"https://colab.research.google.com/drive/1bdymP7p7d4z7izsS-PhMUxXcD38p9Hqr\" target=\"_blank\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Google Colabで開く\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "56dc5c9d"
   },
   "source": [
    "## 🪄 `weave`ライブラリのインストールとログイン\n",
    "\n",
    "\n",
    "ライブラリをインストールし、アカウントにログインすることから始めましょう。\n",
    "\n",
    "この例では、W&B Inferenceを使用するため、Public Cloud環境のWANDB_API_KEYが必要です。Dedicated CloudやOnpremiseを利用されている方は、Public Cloudのkeyも用意してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0921bbef-d089-4960-b624-505172f90d76",
    "outputId": "a262f278-48dc-411f-9fd9-58d69def7db9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkeisuke-kamata\u001b[0m (\u001b[33mwandb-smle\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[36m\u001b[1mweave\u001b[0m: wandb version 0.22.2 is available!  To upgrade, please run:\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[36m\u001b[1mweave\u001b[0m: weave version 0.52.9 is available!  To upgrade, please run:\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:  $ pip install weave --upgrade\n",
      "\u001b[36m\u001b[1mweave\u001b[0m: Logged in as Weights & Biases user: keisuke-kamata.\n",
      "\u001b[36m\u001b[1mweave\u001b[0m: View Weave data at https://wandb.ai/wandb-japan/weave-handson-20251015/weave\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<weave.trace.weave_client.WeaveClient at 0x11bb83a10>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import wandb\n",
    "import weave\n",
    "import openai\n",
    "import json\n",
    "#========================================\n",
    "# 環境変数を適切に設定してください\n",
    "#========================================\n",
    "# os.environ[\"WANDB_BASE_URL\"] = \"https://api.wandb.ai\" # \n",
    "\n",
    "wandb.login()\n",
    "PROJECT = \"wandb-japan/weave-handson\" # handsonで利用するprojectです。entity(team)のところをご自身のteamに置き換えてください。\n",
    "weave.init(PROJECT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "09467a33"
   },
   "source": [
    "# 関数の入力と出力をトラッキング\n",
    "\n",
    "Weaveにより、ユーザーは関数呼び出しを、コード、入力、出力、さらにはLLMトークンとコストまでトラッキングできます。以下のセクションでは次の内容をカバーします：\n",
    "\n",
    "* カスタム関数\n",
    "* ベンダー統合\n",
    "* ネストした関数呼び出し\n",
    "* エラートラッキング"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "99453d48"
   },
   "source": [
    "## カスタム関数のトラッキング\n",
    "\n",
    "トラッキングしたい関数に@weave.opデコレータを追加します"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cdcd1bb7",
    "outputId": "8c252a02-fc1f-4499-9f4a-7bd4362a2f61"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello hello\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m\u001b[1mweave\u001b[0m: 🍩 https://wandb.ai/wandb-japan/weave-handson-20251015/r/call/0199ebff-446d-7c30-a8d4-d8c5fcad5ce9\n",
      "\u001b[36m\u001b[1mweave\u001b[0m: 🍩 https://wandb.ai/wandb-japan/weave-handson-20251015/r/call/0199ebff-4b6a-7651-aaf1-81852f492acc\n",
      "\u001b[36m\u001b[1mweave\u001b[0m: 🍩 https://wandb.ai/wandb-japan/weave-handson-20251015/r/call/0199ebff-50fc-7ef7-b8d5-fe397229a659\n",
      "\u001b[36m\u001b[1mweave\u001b[0m: 🍩 https://wandb.ai/wandb-japan/weave-handson-20251015/r/call/0199ebff-616d-7a21-b2e6-9533f67c1df4\n"
     ]
    }
   ],
   "source": [
    "@weave.op()\n",
    "def echo(user_input):\n",
    "    return user_input + \" \" + user_input\n",
    "\n",
    "result = echo(\"hello\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "174cabef"
   },
   "source": [
    "上記の👆 wandbリンクをクリックすることで、インタラクティブダッシュボードを見つけることができます。\n",
    "\n",
    "`weave.op`を追加して関数を呼び出した後、リンクにアクセスして、プロジェクト内でトラッキングされているのを確認してください。\n",
    "\n",
    "💡 コードを自動的にトラッキングしています。コードタブをご覧ください！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5ef6f27b"
   },
   "source": [
    "## Integrationを利用したトラッキング（W&B Inference、OpenAI、Anthropic、Mistralなど）\n",
    "\n",
    "ここでは、`W&B Inference`への全ての呼び出しを自動的にトラッキングしています。Weaveは、多くのLLMライブラリを@weaveなしで自動的にトラッキングしてくれます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM APIの初期化\n",
    "use_openai = True # OpenAIのAPIを利用する場合はTrue, W&B Inferenceを利用する場合はFalseにしてください。\n",
    "\n",
    "if use_openai:\n",
    "    ## OpenAIのAPIを利用する場合\n",
    "    #os.environ[\"OPENAI_API_KEY\"] = \"\" #OpenAIのAPIを利用する場合は、OpenAIのAPI Keyを入力してください。\n",
    "    client = openai.OpenAI(\n",
    "        api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "    )\n",
    "    model_name=\"gpt-4.1-2025-04-14\"\n",
    "else:\n",
    "    ## W&B Inferenceを利用する場合\n",
    "    #os.environ[\"WANDB_API_KEY_PUBLIC_CLOUD\"] = \"\"  # Public cloudユーザーはWANDB_API_KEYと同じ値を使用して問題ないです。Dedicated Cloudを利用し、推論だけPublic Cloudを利用する場合などは、Public CloudのAPI Keyを入力してください。\n",
    "    INFERENCE_PROJECT=PROJECT # Inferenceの利用料トラックのために利用するProjectです。WeaveのPROJECTと一致しない場合（Dedicated Cloudを利用し、推論だけPublic Cloudを利用する場合など）は、public cloudの中のentityとteamの組み合わせを入れてください。\n",
    "    client = openai.OpenAI(\n",
    "        base_url='https://api.inference.wandb.ai/v1',\n",
    "        api_key=os.getenv(\"WANDB_API_KEY_PUBLIC_CLOUD\") or os.getenv(\"WANDB_API_KEY\"),\n",
    "        project=INFERENCE_PROJECT,\n",
    "    )\n",
    "    model_name=\"openai/gpt-oss-20b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "もちろん！では、一つどうぞ。\n",
      "\n",
      "パンダがレストランで食事をしました。食べ終わった後、急に立ち上がってお金も払わずに帰ろうとしました。店員が慌てて「お客様、お会計は？」と聞くと、パンダは言いました。\n",
      "\n",
      "「ごめん、持ちあい（持ち合い＝持ち合わせ）がないんだ。」\n",
      "\n",
      "いかがでしょうか？\n"
     ]
    }
   ],
   "source": [
    "# Trace the model call in Weave\n",
    "def run_chat():\n",
    "    response = client.chat.completions.create(\n",
    "        model=model_name,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": \"ジョークを言ってください。\"}\n",
    "        ],\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# Run and log the traced call\n",
    "output = run_chat()\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "35fe43dc"
   },
   "source": [
    "## Nested関数のトラッキング\n",
    "\n",
    "基本を確認したので、上記のすべてを組み合わせて、Nested関数をトラッキングしてみましょう。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "422050d0",
    "outputId": "9f1f22d6-2592-47ad-c83a-5c08095d632f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, hello, can you hear me now?  \n",
      "Waves of words are floating through the crowd  \n",
      "Hello, hello, let’s begin again  \n",
      "A brand new story waiting to be penned  \n",
      "\n",
      "(Chorus)  \n",
      "Hello, hello, let’s light up the night  \n",
      "With every greeting, we’re shining so bright  \n",
      "Hello, hello, let’s see where we go  \n",
      "With every “hello,” we open the door  \n",
      "\n",
      "Verse 2:  \n",
      "Hello, hello, to the world outside  \n",
      "Smiles and laughter, nowhere to hide  \n",
      "Hello, hello, let’s make it clear  \n",
      "Every “hello” brings someone near  \n",
      "\n",
      "(Chorus)  \n",
      "Hello, hello, let’s light up the night  \n",
      "With every greeting, we’re shining so bright  \n",
      "Hello, hello, let’s see where we go  \n",
      "With every “hello,” we open the door  \n",
      "\n",
      "(Bridge)  \n",
      "Every “hello” is a chance to start  \n",
      "A little kindness, a work of art  \n",
      "So say it loud, let your voice flow  \n",
      "The world gets warmer with every “hello”  \n",
      "\n",
      "(Chorus)  \n",
      "Hello, hello, let’s light up the night  \n",
      "With every greeting, we’re shining so bright  \n",
      "Hello, hello, let’s see where we go  \n",
      "With every “hello,” we open the door  \n",
      "\n",
      "(Outro)  \n",
      "Hello, hello, let’s begin again  \n",
      "With every “hello,” we find a friend\n"
     ]
    }
   ],
   "source": [
    "@weave.op()\n",
    "def correct_grammar(user_input):\n",
    "    echoed_input = echo(user_input)\n",
    "    response = client.chat.completions.create(\n",
    "        model=model_name,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are a helpful assistant. Please create a song by following the user input.\",\n",
    "            },\n",
    "            {\"role\": \"user\", \"content\": echoed_input},\n",
    "        ],\n",
    "        temperature=0,\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "\n",
    "result = correct_grammar(\"hello\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6f483413"
   },
   "source": [
    "## エラーのトラッキング\n",
    "\n",
    "コードがクラッシュするたびに、weaveは問題の原因をハイライトします。LLMレスポンスからデータを解析する際に時々発生するJSONパースエラーなどを見つけるのに特に有用です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 356
    },
    "id": "14e7e103",
    "outputId": "1a121775-5b5c-4212-f205-d97c0ef7cac2"
   },
   "outputs": [
    {
     "ename": "BadRequestError",
     "evalue": "Error code: 400 - {'error': {'message': \"'messages' must contain the word 'json' in some form, to use 'response_format' of type 'json_object'.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': None}}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mBadRequestError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 19\u001b[39m\n\u001b[32m      4\u001b[39m     response = client.chat.completions.create(\n\u001b[32m      5\u001b[39m         model=model_name,\n\u001b[32m      6\u001b[39m         messages=[\n\u001b[32m   (...)\u001b[39m\u001b[32m     14\u001b[39m         response_format={\u001b[33m\"\u001b[39m\u001b[33mtype\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mjson_object\u001b[39m\u001b[33m\"\u001b[39m},\n\u001b[32m     15\u001b[39m     )\n\u001b[32m     16\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m json.loads(response.choices[\u001b[32m0\u001b[39m].message.content)\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m result = \u001b[43mcorrect_grammar\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mhello\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Project/.venv/lib/python3.13/site-packages/weave/trace/op.py:1248\u001b[39m, in \u001b[36mop.<locals>.op_deco.<locals>.create_wrapper.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m   1246\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[32m   1247\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper\u001b[39m(*args: P.args, **kwargs: P.kwargs) -> R:\n\u001b[32m-> \u001b[39m\u001b[32m1248\u001b[39m     res, _ = \u001b[43m_call_sync_func\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1249\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mOp\u001b[49m\u001b[43m[\u001b[49m\u001b[43mP\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mR\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwrapper\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m__should_raise\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m   1250\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1251\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(R, res)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Project/.venv/lib/python3.13/site-packages/weave/trace/op.py:502\u001b[39m, in \u001b[36m_call_sync_func\u001b[39m\u001b[34m(op, __weave, __should_raise, __require_explicit_finish, *args, **kwargs)\u001b[39m\n\u001b[32m    499\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m output\n\u001b[32m    501\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m502\u001b[39m     res = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    503\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    504\u001b[39m     finish(exception=e)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 4\u001b[39m, in \u001b[36mcorrect_grammar\u001b[39m\u001b[34m(user_input)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;129m@weave\u001b[39m.op()\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcorrect_grammar\u001b[39m(user_input):\n\u001b[32m      3\u001b[39m     echoed_input = echo(user_input)\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m     response = \u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mchat\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompletions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrole\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msystem\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcontent\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mYou are a helpful assistant. Please create a song by following the user input.\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrole\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcontent\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mechoed_input\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m        \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtype\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mjson_object\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m json.loads(response.choices[\u001b[32m0\u001b[39m].message.content)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Project/.venv/lib/python3.13/site-packages/weave/trace/op.py:1248\u001b[39m, in \u001b[36mop.<locals>.op_deco.<locals>.create_wrapper.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m   1246\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[32m   1247\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper\u001b[39m(*args: P.args, **kwargs: P.kwargs) -> R:\n\u001b[32m-> \u001b[39m\u001b[32m1248\u001b[39m     res, _ = \u001b[43m_call_sync_func\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1249\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mOp\u001b[49m\u001b[43m[\u001b[49m\u001b[43mP\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mR\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwrapper\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m__should_raise\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m   1250\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1251\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(R, res)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Project/.venv/lib/python3.13/site-packages/weave/trace/op.py:502\u001b[39m, in \u001b[36m_call_sync_func\u001b[39m\u001b[34m(op, __weave, __should_raise, __require_explicit_finish, *args, **kwargs)\u001b[39m\n\u001b[32m    499\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m output\n\u001b[32m    501\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m502\u001b[39m     res = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    503\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    504\u001b[39m     finish(exception=e)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Project/.venv/lib/python3.13/site-packages/weave/integrations/openai/openai_sdk.py:421\u001b[39m, in \u001b[36mcreate_wrapper_sync.<locals>.wrapper.<locals>._add_stream_options.<locals>._wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    418\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m urlparse(base_url).hostname == \u001b[33m\"\u001b[39m\u001b[33mapi.openai.com\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    419\u001b[39m         kwargs[\u001b[33m\"\u001b[39m\u001b[33mstream_options\u001b[39m\u001b[33m\"\u001b[39m] = {\u001b[33m\"\u001b[39m\u001b[33minclude_usage\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m}\n\u001b[32m--> \u001b[39m\u001b[32m421\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Project/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py:286\u001b[39m, in \u001b[36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    284\u001b[39m             msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[32m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    285\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[32m--> \u001b[39m\u001b[32m286\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Project/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py:1147\u001b[39m, in \u001b[36mCompletions.create\u001b[39m\u001b[34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, prompt_cache_key, reasoning_effort, response_format, safety_identifier, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, verbosity, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m   1101\u001b[39m \u001b[38;5;129m@required_args\u001b[39m([\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m], [\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m   1102\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate\u001b[39m(\n\u001b[32m   1103\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1144\u001b[39m     timeout: \u001b[38;5;28mfloat\u001b[39m | httpx.Timeout | \u001b[38;5;28;01mNone\u001b[39;00m | NotGiven = NOT_GIVEN,\n\u001b[32m   1145\u001b[39m ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[32m   1146\u001b[39m     validate_response_format(response_format)\n\u001b[32m-> \u001b[39m\u001b[32m1147\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1148\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/chat/completions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1149\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1150\u001b[39m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m   1151\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1152\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1153\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43maudio\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1154\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfrequency_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1155\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunction_call\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1156\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunctions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1157\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogit_bias\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1158\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1159\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_completion_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1160\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1161\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1162\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodalities\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodalities\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1163\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mn\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1164\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mparallel_tool_calls\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1165\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprediction\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1166\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpresence_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1167\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprompt_cache_key\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_cache_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1168\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mreasoning_effort\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_effort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1169\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mresponse_format\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1170\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msafety_identifier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msafety_identifier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1171\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mseed\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1172\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mservice_tier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1173\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstop\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1174\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstore\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1175\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1176\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1177\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtemperature\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1178\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtool_choice\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1179\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtools\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1180\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_logprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1181\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_p\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1182\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1183\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mverbosity\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbosity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1184\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mweb_search_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mweb_search_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1185\u001b[39m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1186\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParamsStreaming\u001b[49m\n\u001b[32m   1187\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\n\u001b[32m   1188\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParamsNonStreaming\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1189\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1190\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1191\u001b[39m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m   1192\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1193\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1194\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1195\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1196\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Project/.venv/lib/python3.13/site-packages/openai/_base_client.py:1259\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1245\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1246\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1247\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1254\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1255\u001b[39m ) -> ResponseT | _StreamT:\n\u001b[32m   1256\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1257\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001b[32m   1258\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1259\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Project/.venv/lib/python3.13/site-packages/openai/_base_client.py:1047\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1044\u001b[39m             err.response.read()\n\u001b[32m   1046\u001b[39m         log.debug(\u001b[33m\"\u001b[39m\u001b[33mRe-raising status error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1047\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_status_error_from_response(err.response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1049\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m   1051\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mcould not resolve response (should never happen)\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mBadRequestError\u001b[39m: Error code: 400 - {'error': {'message': \"'messages' must contain the word 'json' in some form, to use 'response_format' of type 'json_object'.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': None}}"
     ]
    }
   ],
   "source": [
    "@weave.op()\n",
    "def correct_grammar(user_input):\n",
    "    echoed_input = echo(user_input)\n",
    "    response = client.chat.completions.create(\n",
    "        model=model_name,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are a helpful assistant. Please create a song by following the user input.\",\n",
    "            },\n",
    "            {\"role\": \"user\", \"content\": echoed_input},\n",
    "        ],\n",
    "        temperature=0,\n",
    "        response_format={\"type\": \"json_object\"},\n",
    "    )\n",
    "    return json.loads(response.choices[0].message.content)\n",
    "\n",
    "\n",
    "result = correct_grammar(\"hello\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "knBo-VtWxl6g"
   },
   "source": [
    "# Traceの高度なTips\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6KWJhiBsxl6h"
   },
   "source": [
    "### サンプリングレートの制御\n",
    "\n",
    "@weave.opデコレータのtracing_sample_rateパラメータを設定することで、opの呼び出しがトレースされる頻度を制御できます。これは、呼び出しのサブセットのみをトレースする必要がある高頻度opに有用です。\n",
    "\n",
    "サンプリングレートはルート呼び出しにのみ適用されることに注意してください。opにサンプルレートがあっても、最初に別のopによって呼び出された場合、そのサンプリングレートは無視されます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cpX43hPbdeOZ"
   },
   "outputs": [],
   "source": [
    "@weave.op(tracing_sample_rate=0.1)  # Only trace ~10% of calls\n",
    "def high_frequency_op(x: int) -> int:\n",
    "    return x + 1\n",
    "\n",
    "@weave.op(tracing_sample_rate=1.0)  # Always trace (default)\n",
    "def always_traced_op(x: int) -> int:\n",
    "    return x + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ntJpevJQeLoA"
   },
   "source": [
    "### 呼び出し表示名"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iqOMqMIseYSV",
    "outputId": "f1fa226c-f099-4c5a-f411-6f4bc0caab19"
   },
   "outputs": [],
   "source": [
    "# Decorate your function\n",
    "@weave.op\n",
    "def my_function(name: str):\n",
    "    return f\"Hello, {name}!\"\n",
    "\n",
    "# Call your function -- Weave will automatically track inputs and outputs\n",
    "print(my_function(\"World\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RV9wwY4veaz8"
   },
   "outputs": [],
   "source": [
    "# 1st method\n",
    "result = my_function(\"World\", __weave={\"display_name\": \"My Custom Display Name\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R8d4mcWserLq",
    "outputId": "1938848a-75f4-487b-d46b-09d4995a66cb"
   },
   "outputs": [],
   "source": [
    "# 2nd method\n",
    "result, call = my_function.call(\"World\")\n",
    "call.set_display_name(\"My Custom Display Name\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "-M2abQSbew9M",
    "outputId": "81b67f69-507c-46e1-9d9b-3ae52d23c762"
   },
   "outputs": [],
   "source": [
    "# 3rd method\n",
    "@weave.op(call_display_name=\"My Custom Display Name\")\n",
    "def my_function(name: str):\n",
    "    return f\"Hello, {name}!\"\n",
    "\n",
    "my_function(\"World\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ExWp8BFIxl6g"
   },
   "source": [
    "\n",
    "### PIIの編集\n",
    "\n",
    "一部の組織では、大規模言語モデル（LLM）ワークフローで名前、電話番号、メールアドレスなどの個人識別情報（PII）を処理します。このデータをWeights & Biases（W&B）Weaveに保存することは、コンプライアンスとセキュリティのリスクをもたらします。\n",
    "\n",
    "機密データ保護機能により、トレースがWeaveサーバーに送信される前に、個人識別情報（PII）を自動的に編集できます。この機能はMicrosoft PresidioをWeave Python SDKに統合しており、SDK レベルで編集設定を制御できることを意味します。\n",
    "\n",
    "[詳細ドキュメント](https://weave-docs.wandb.ai/guides/tracking/redact-pii)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f957c9c8"
   },
   "source": [
    "# オブジェクトのトラッキング\n",
    "\n",
    "Weaveでは、システムプロンプトや使用しているモデル、データセットなどのアセットを`weave.Objects`内でバージョン管理できます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8f6129cc"
   },
   "source": [
    "## プロンプトトラッキング"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "08e4e852",
    "outputId": "d5b8c38a-0f00-4e9b-a09f-268f1ea1eb1c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m\u001b[1mweave\u001b[0m: 📦 Published to https://wandb.ai/wandb-japan/weave-handson-20251015/weave/objects/pirate_prompt/versions/sShdJdKADGGYJiZ85UxB5wOJprgdiSwLH7VXMYxrHSA\n"
     ]
    }
   ],
   "source": [
    "# StringPrompt1\n",
    "system_prompt = weave.StringPrompt(\"You are a pirate\")\n",
    "weave.publish(system_prompt, name=\"pirate_prompt\")\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "  model=model_name,\n",
    "  messages=[\n",
    "    {\n",
    "      \"role\": \"system\",\n",
    "      \"content\": system_prompt.format()\n",
    "    },\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": \"Explain general relativity in one paragraph.\"\n",
    "    }\n",
    "  ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ke21Y2OJg9Uo",
    "outputId": "f821bf2a-0577-4c96-9661-df18e4eb1524"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m\u001b[1mweave\u001b[0m: 📦 Published to https://wandb.ai/wandb-japan/weave-handson-20251015/weave/objects/pirate_prompt/versions/GDp90qF5SsaoFgeMqEliU2e99AXxKDZwBub08BVipRQ\n"
     ]
    }
   ],
   "source": [
    "# StringPrompt2\n",
    "system_prompt = weave.StringPrompt(\"Talk like a pirate. I need to know I'm listening to a pirate.\")\n",
    "weave.publish(system_prompt, name=\"pirate_prompt\")\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "  model=model_name,\n",
    "  messages=[\n",
    "    {\n",
    "      \"role\": \"system\",\n",
    "      \"content\": system_prompt.format()\n",
    "    },\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": \"Explain general relativity in one paragraph.\"\n",
    "    }\n",
    "  ],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZrpZa01lg_-D",
    "outputId": "661423d9-9a58-4635-bce8-4f360838706d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m\u001b[1mweave\u001b[0m: 📦 Published to https://wandb.ai/wandb-japan/weave-handson-20251015/weave/objects/dino_prompt/versions/rXZ6qrk5SFNdXIkGehiVXrv9PYX62PbKDyoYHlahoMo\n"
     ]
    }
   ],
   "source": [
    "# MessagesPrompt1\n",
    "prompt = weave.MessagesPrompt([\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a stegosaurus, but don't be too obvious about it.\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"What's good to eat around here?\"\n",
    "    }\n",
    "])\n",
    "weave.publish(prompt, name=\"dino_prompt\")\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "  model=\"gpt-4o\",\n",
    "  messages=prompt.format(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WY5UZLgIhHYW",
    "outputId": "7a1cb983-044b-4d14-81fa-941493c870a0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m\u001b[1mweave\u001b[0m: 📦 Published to https://wandb.ai/wandb-japan/weave-handson-20251015/weave/objects/calculator_prompt/versions/YK3NAzaQ0fXYTJjNPlf7JITesCRt3hIGcb2aW1mWOy0\n"
     ]
    }
   ],
   "source": [
    "# parameterizing prompts\n",
    "prompt = weave.StringPrompt(\"Solve the equation {equation}\")\n",
    "weave.publish(prompt, name=\"calculator_prompt\")\n",
    "\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "  model=model_name,\n",
    "  messages=[\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": prompt.format(equation=\"1 + 1 = ?\")\n",
    "    }\n",
    "  ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0c873438"
   },
   "source": [
    "## モデルトラッキング\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e3e60d81",
    "outputId": "f0fb3219-66be-4048-e5a2-3db0c047a651"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That was so easy; it was a piece of cake!\n"
     ]
    }
   ],
   "source": [
    "class WandBInferenceGrammarCorrector(weave.Model):\n",
    "    # プロパティは完全にユーザー定義\n",
    "    model_name: str\n",
    "    system_message: str\n",
    "\n",
    "    @weave.op()\n",
    "    def predict(self, user_input):\n",
    "\n",
    "        response = client.chat.completions.create(\n",
    "            model=self.model_name,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": self.system_message},\n",
    "                {\"role\": \"user\", \"content\": user_input},\n",
    "            ],\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "\n",
    "corrector = WandBInferenceGrammarCorrector(\n",
    "    model_name=model_name,\n",
    "    system_message=\"You are a grammar checker, correct the following user input.\",\n",
    ")\n",
    "\n",
    "\n",
    "result = corrector.predict(\"     That was so easy, it was a piece of pie!       \")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "624b04c1"
   },
   "source": [
    "## データセットトラッキング\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ac2c5145",
    "outputId": "2918a900-8263-49e8-d784-4c35cde32407"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m\u001b[1mweave\u001b[0m: 📦 Published to https://wandb.ai/wandb-japan/weave-handson-20251015/weave/objects/grammar-correction/versions/Td1iXzvI1gFHlSq3VNyYytJBnzfzDgAyDaQWHY6X3bo\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ObjectRef(entity='wandb-japan', project='weave-handson-20251015', name='grammar-correction', _digest='Td1iXzvI1gFHlSq3VNyYytJBnzfzDgAyDaQWHY6X3bo', _extra=())"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = weave.Dataset(\n",
    "    name=\"grammar-correction\",\n",
    "    rows=[\n",
    "        {\n",
    "            \"user_input\": \"   That was so easy, it was a piece of pie!   \",\n",
    "            \"expected\": \"That was so easy, it was a piece of cake!\",\n",
    "        },\n",
    "        {\"user_input\": \"  I write good   \",\n",
    "         \"expected\": \"I write well\"},\n",
    "        {\n",
    "            \"user_input\": \"  GPT-3 is smartest AI model.   \",\n",
    "            \"expected\": \"GPT-3 is the smartest AI model.\",\n",
    "        },\n",
    "    ],\n",
    ")\n",
    "\n",
    "weave.publish(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "71c3aaea"
   },
   "source": [
    "## Retrieve Published Objects & Ops\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "caa5c5e0",
    "outputId": "57c8c724-9748-4892-864c-8f189b664a81"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MessagesPrompt(name=None, description=None, messages=WeaveList([{'role': 'system', 'content': \"You are a stegosaurus, but don't be too obvious about it.\"}, {'role': 'user', 'content': \"What's good to eat around here?\"}]))\n"
     ]
    }
   ],
   "source": [
    "ref_url = \"\"\n",
    "prompt = weave.ref(ref_url).get()\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "77b94dd5"
   },
   "source": [
    "# オフライン評価\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "72bdf072"
   },
   "source": [
    "## 方法1: [標準メソッド](https://weave-docs.wandb.ai/guides/core-types/evaluations)\n",
    "予測と評価の両方をサンプルごとに実行し、評価をします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 148
    },
    "id": "fpiwHM4WdV9A",
    "outputId": "f1cd62cf-422b-4de7-8990-a24b8a9f8f0b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m\u001b[1mweave\u001b[0m: wandb version 0.22.2 is available!  To upgrade, please run:\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[36m\u001b[1mweave\u001b[0m: Logged in as Weights & Biases user: keisuke-kamata.\n",
      "\u001b[36m\u001b[1mweave\u001b[0m: View Weave data at https://wandb.ai/wandb-japan/weave-handson-20251015/weave\n",
      "\u001b[36m\u001b[1mweave\u001b[0m: Evaluated 1 of 3 examples\n",
      "\u001b[36m\u001b[1mweave\u001b[0m: Evaluated 2 of 3 examples\n",
      "\u001b[36m\u001b[1mweave\u001b[0m: Evaluated 3 of 3 examples\n",
      "\u001b[36m\u001b[1mweave\u001b[0m: Evaluation summary {\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:   \"match_score\": {\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:     \"match\": {\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:       \"true_count\": 0,\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:       \"true_fraction\": 0.0\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:     }\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:   },\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:   \"model_latency\": {\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:     \"mean\": 0.005383412043253581\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:   }\n",
      "\u001b[36m\u001b[1mweave\u001b[0m: }\n",
      "\u001b[36m\u001b[1mweave\u001b[0m: 🍩 https://wandb.ai/wandb-japan/weave-handson-20251015/r/call/0199e56d-0874-7b64-a215-7688af8ce2c6\n",
      "\u001b[36m\u001b[1mweave\u001b[0m: 🍩 https://wandb.ai/wandb-japan/weave-handson-20251015/r/call/0199e580-1317-7bfe-b9f8-3fea239bf731\n",
      "\u001b[36m\u001b[1mweave\u001b[0m: 🍩 https://wandb.ai/wandb-japan/weave-handson-20251015/r/call/0199e581-cbcc-7121-a459-b6d06bf4f37d\n",
      "\u001b[36m\u001b[1mweave\u001b[0m: 🍩 https://wandb.ai/wandb-japan/weave-handson-20251015/r/call/0199e581-cbd2-7099-a0eb-c48ec7bc1a9f\n",
      "\u001b[36m\u001b[1mweave\u001b[0m: 🍩 https://wandb.ai/wandb-japan/weave-handson-20251015/r/call/0199e581-cbd5-768e-999d-b8d28a0384de\n"
     ]
    }
   ],
   "source": [
    "from weave import Evaluation, Model\n",
    "import weave\n",
    "import asyncio\n",
    "import openai\n",
    "import os\n",
    "\n",
    "\n",
    "examples = [\n",
    "    {\"question\": \"What is the capital of France?\", \"expected\": \"Paris\"},\n",
    "    {\"question\": \"Who wrote 'To Kill a Mockingbird'?\", \"expected\": \"Harper Lee\"},\n",
    "    {\"question\": \"What is the square root of 64?\", \"expected\": \"8\"},\n",
    "]\n",
    "\n",
    "@weave.op()\n",
    "def exact_match_scorer(expected: str, output: dict) -> dict:\n",
    "    \"\"\"Exact string matching evaluation\"\"\"\n",
    "    generated = output.get('generated_text', '')\n",
    "    return {'exact_match': expected.lower().strip() == generated.lower().strip()}\n",
    "\n",
    "@weave.op()\n",
    "def contains_answer_scorer(expected: str, output: dict) -> dict:\n",
    "    \"\"\"Check if the expected answer is contained in the generated text\"\"\"\n",
    "    generated = output.get('generated_text', '').lower()\n",
    "    expected_lower = expected.lower()\n",
    "    return {'contains_answer': expected_lower in generated}\n",
    "\n",
    "class OpenAIQAModel(Model):\n",
    "    model_name: str = \"gpt-4o-mini\"\n",
    "    temperature: float = 0.3\n",
    "    max_tokens: int = 100\n",
    "\n",
    "    @weave.op()\n",
    "    def predict(self, question: str):\n",
    "        client = openai.OpenAI()\n",
    "        \"\"\"Generate answer using OpenAI API\"\"\"\n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=self.model_name,\n",
    "                messages=[\n",
    "                    {\n",
    "                        \"role\": \"system\",\n",
    "                        \"content\": \"You are a helpful assistant that provides accurate and concise answers to questions.\"\n",
    "                    },\n",
    "                    {\n",
    "                        \"role\": \"user\", \n",
    "                        \"content\": f\"Question: {question}\"\n",
    "                    }\n",
    "                ],\n",
    "                temperature=self.temperature,\n",
    "                max_tokens=self.max_tokens\n",
    "            )\n",
    "            \n",
    "            answer = response.choices[0].message.content.strip()\n",
    "            return {\n",
    "                'generated_text': answer,\n",
    "                'model': self.model_name,\n",
    "                'question': question\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error calling OpenAI API: {e}\")\n",
    "            return {\n",
    "                'generated_text': f\"Error: {str(e)}\",\n",
    "                'model': self.model_name,\n",
    "                'question': question\n",
    "            }\n",
    "\n",
    "# Create model and evaluation\n",
    "model = OpenAIQAModel()\n",
    "evaluation = Evaluation(\n",
    "    dataset=examples, \n",
    "    scorers=[exact_match_scorer, contains_answer_scorer]\n",
    ")\n",
    "\n",
    "# Run evaluation\n",
    "asyncio.run(evaluation.evaluate(model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v6VE09bweQzu"
   },
   "source": [
    "## 方法2: [EvaluationLogger](https://weave-docs.wandb.ai/guides/evaluation/evaluation_logger)\n",
    "バッチ予測が適用できます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5jeULyjweeBI",
    "outputId": "fa75ee71-71f9-4fbf-b211-6206714f01ce"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'weave.flow.eval_imperative'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mweave\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mweave\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01meval_imperative\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m EvaluationLogger\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Initialize the logger with optional metadata\u001b[39;00m\n\u001b[32m      5\u001b[39m eval_logger = EvaluationLogger(\n\u001b[32m      6\u001b[39m     model=\u001b[33m\"\u001b[39m\u001b[33mmy_local_model\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      7\u001b[39m     dataset=\u001b[33m\"\u001b[39m\u001b[33mmy_dataset\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      8\u001b[39m )\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'weave.flow.eval_imperative'"
     ]
    }
   ],
   "source": [
    "import weave\n",
    "from weave import EvaluationLogger\n",
    "\n",
    "# Initialize the logger with optional metadata\n",
    "eval_logger = EvaluationLogger(\n",
    "    model=\"my_local_model\",\n",
    "    dataset=\"my_dataset\"\n",
    ")\n",
    "\n",
    "# Example input data\n",
    "eval_samples = [\n",
    "    {'inputs': {'a': 1, 'b': 2}, 'expected': 3},\n",
    "    {'inputs': {'a': 2, 'b': 3}, 'expected': 5},\n",
    "    {'inputs': {'a': 3, 'b': 4}, 'expected': 7},\n",
    "]\n",
    "\n",
    "# Local model logic: simply add the numbers\n",
    "@weave.op\n",
    "def user_model(a: int, b: int) -> int:\n",
    "    return a + b\n",
    "# Example model logic using OpenAI\n",
    "@weave.op\n",
    "def user_model(a: int, b: int) -> int:\n",
    "    oai = openai.OpenAI()\n",
    "    response = oai.chat.completions.create(\n",
    "        messages=[{\"role\": \"user\", \"content\": f\"What is {a}+{b}?\"}],\n",
    "        model=\"gpt-4o-mini\"\n",
    "    )\n",
    "    # Use the response in some way (here we just return a + b for simplicity)\n",
    "    return a + b\n",
    "\n",
    "# Iterate through examples, predict, and log\n",
    "for sample in eval_samples:\n",
    "    inputs = sample[\"inputs\"]\n",
    "    model_output = user_model(**inputs) # Pass inputs as kwargs\n",
    "\n",
    "    # Log the prediction input and output\n",
    "    pred_logger = eval_logger.log_prediction(\n",
    "        inputs=inputs,\n",
    "        output=model_output\n",
    "    )\n",
    "\n",
    "    # Calculate and log a score for this prediction\n",
    "    expected = sample[\"expected\"]\n",
    "    correctness_score = model_output == expected\n",
    "    pred_logger.log_score(\n",
    "        scorer=\"correctness\", # Simple string name for the scorer\n",
    "        score=correctness_score\n",
    "    )\n",
    "\n",
    "    # Finish logging for this specific prediction\n",
    "    pred_logger.finish()\n",
    "\n",
    "# Log a final summary for the entire evaluation.\n",
    "# Weave auto-aggregates the 'correctness' scores logged above.\n",
    "summary_stats = {\"subjective_overall_score\": 0.8}\n",
    "eval_logger.log_summary(summary_stats)\n",
    "\n",
    "print(\"Evaluation logging complete. View results in the Weave UI.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lk4DY3RqkKnf"
   },
   "source": [
    "# オンライン評価"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9ZRRHZyaDdpu"
   },
   "source": [
    "# フィードバック"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 356
    },
    "id": "kLBVpBlkxVdi",
    "outputId": "88608be4-dd3b-4609-dd4d-f160fe021222"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m\u001b[1mweave\u001b[0m: wandb version 0.22.2 is available!  To upgrade, please run:\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[36m\u001b[1mweave\u001b[0m: Logged in as Weights & Biases user: keisuke-kamata.\n",
      "\u001b[36m\u001b[1mweave\u001b[0m: View Weave data at https://wandb.ai/wandb-japan/weave-handson-20251015/weave\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'0199e57a-4aa2-7e86-9d4d-febd8f1e8b45'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client = weave.init(PROJECT)\n",
    "call = client.get_call(\"0199e574-02a6-7444-80f2-40b5e8fa4e06\") #@param\n",
    "\n",
    "# Adding an emoji reaction\n",
    "call.feedback.add_reaction(\"👍\")\n",
    "\n",
    "# Adding a note\n",
    "call.feedback.add_note(\"this is a note\")\n",
    "\n",
    "# Adding custom key/value pairs.\n",
    "# The first argument is a user-defined \"type\" string.\n",
    "# Feedback must be JSON serializable and less than 1 KB when serialized.\n",
    "call.feedback.add(\"correctness\", { \"value\": 5 })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M497nWagx0PP"
   },
   "source": [
    "# Scorers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "d0tqUV9_1KgA"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ApplyScorerSuccess(result={'length': 13, 'is_short': True}, score_call=Call(_op_name=<Future at 0x11992c990 state=running>, trace_id='0199e580-1316-79ab-b0cf-985fe749ce91', project_id='wandb-japan/weave-handson-20251015', parent_id=None, inputs={'self': ObjectRef(entity='wandb-japan', project='weave-handson-20251015', name='LengthScorer', _digest=<Future at 0x119943e10 state=pending>, _extra=()), 'output': 'Hello, world!'}, id='0199e580-1317-7bfe-b9f8-3fea239bf731', output={'length': 13, 'is_short': True}, exception=None, summary={'status_counts': {<TraceStatus.SUCCESS: 'success'>: 1, <TraceStatus.ERROR: 'error'>: 0}}, _display_name=None, attributes=AttributesDict({'weave': {'python': {'type': 'function'}, 'client_version': '0.52.9', 'source': 'python-sdk', 'sys_version': '3.11.13 (main, Jun  3 2025, 18:38:25) [Clang 17.0.0 (clang-1700.0.13.3)]', 'os_name': 'Darwin', 'os_version': 'Darwin Kernel Version 24.6.0: Mon Aug 11 21:16:34 PDT 2025; root:xnu-11417.140.69.701.11~1/RELEASE_ARM64_T6020', 'os_release': '24.6.0'}}), started_at=None, ended_at=datetime.datetime(2025, 10, 15, 1, 33, 20, 25474, tzinfo=datetime.timezone.utc), deleted_at=None, thread_id=None, turn_id=None, wb_run_id=None, wb_run_step=None, wb_run_step_end=None, _children=[], _feedback=None))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m\u001b[1mweave\u001b[0m: 🍩 https://wandb.ai/wandb-japan/weave-handson-20251015/r/call/0199e580-130f-7664-9773-3915adba8857\n",
      "\u001b[36m\u001b[1mweave\u001b[0m: 🍩 https://wandb.ai/wandb-japan/weave-handson-20251015/r/call/0199e581-cbc5-7735-8218-4420efa34b6d\n",
      "\u001b[36m\u001b[1mweave\u001b[0m: 🍩 https://wandb.ai/wandb-japan/weave-handson-20251015/r/call/0199e581-cbd0-7488-8ed0-ee2c66fd64ab\n",
      "\u001b[36m\u001b[1mweave\u001b[0m: 🍩 https://wandb.ai/wandb-japan/weave-handson-20251015/r/call/0199e581-cbca-7efa-9ba4-0f2ba4456b0f\n"
     ]
    }
   ],
   "source": [
    "import weave\n",
    "from weave import Scorer\n",
    "\n",
    "\n",
    "class LengthScorer(Scorer):\n",
    "    @weave.op\n",
    "    def score(self, output: str) -> dict:\n",
    "        \"\"\"A simple scorer that checks output length.\"\"\"\n",
    "        return {\n",
    "            \"length\": len(output),\n",
    "            \"is_short\": len(output) < 100\n",
    "        }\n",
    "\n",
    "@weave.op\n",
    "def generate_text(prompt: str) -> str:\n",
    "    return \"Hello, world!\"\n",
    "\n",
    "# Get both result and Call object\n",
    "result, call = generate_text.call(\"Say hello\")\n",
    "\n",
    "# Now you can apply scorers\n",
    "await call.apply_scorer(LengthScorer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SfipbjSi2dee"
   },
   "source": [
    "## ガードレールとしてのScorersの使用\n",
    "ガードレールは、LLM出力がユーザーに到達する前に実行される安全チェックとして機能します。実用的な例を以下に示します："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "r9rKr-_g2YLk"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: hello\n",
      "Response: Hello! How can I help you?\n",
      "\n",
      "Prompt: bad\n",
      "Response: I cannot generate that content: Detected toxic language\n",
      "\n",
      "Prompt: good\n",
      "Response: You are wonderful!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import weave\n",
    "from weave import Scorer\n",
    "import asyncio\n",
    "import nest_asyncio  # Required for running asyncio in Google Colab\n",
    "\n",
    "# Apply nest_asyncio to avoid event loop issues in Google Colab\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# ==== 1. Define text generation function ====\n",
    "@weave.op\n",
    "def generate_text(prompt: str) -> str:\n",
    "    \"\"\"Simulated LLM text generation (basic logic).\"\"\"\n",
    "    responses = {\n",
    "        \"hello\": \"Hello! How can I help you?\",\n",
    "        \"bad\": \"You are terrible!\",  # Example of toxic response\n",
    "        \"good\": \"You are wonderful!\",\n",
    "    }\n",
    "    return responses.get(prompt.lower(), \"I don't understand your request.\")\n",
    "\n",
    "# ==== 2. Define the Toxicity Scorer ====\n",
    "class ToxicityScorer(Scorer):\n",
    "    @weave.op\n",
    "    def score(self, output: str) -> dict:\n",
    "        \"\"\"\n",
    "        Evaluate the generated content for toxic language.\n",
    "        \"\"\"\n",
    "        toxic_words = {\"terrible\", \"hate\", \"stupid\"}  # Simple keyword-based detection\n",
    "        flagged = any(word in output.lower() for word in toxic_words)\n",
    "\n",
    "        return {\n",
    "            \"flagged\": flagged,\n",
    "            \"reason\": \"Detected toxic language\" if flagged else None\n",
    "        }\n",
    "\n",
    "# ==== 3. Function to generate safe responses ====\n",
    "async def generate_safe_response(prompt: str) -> str:\n",
    "    # Generate text using LLM\n",
    "    result, call = generate_text.call(prompt)\n",
    "\n",
    "    # Apply toxicity scoring\n",
    "    safety = await call.apply_scorer(ToxicityScorer())\n",
    "\n",
    "    # If flagged as toxic, return a warning message\n",
    "    if safety.result[\"flagged\"]:\n",
    "        return f\"I cannot generate that content: {safety.result['reason']}\"\n",
    "\n",
    "    return result\n",
    "\n",
    "# ==== 4. Run test cases ====\n",
    "async def main():\n",
    "    prompts = [\"hello\", \"bad\", \"good\"]\n",
    "\n",
    "    for prompt in prompts:\n",
    "        response = await generate_safe_response(prompt)\n",
    "        print(f\"Prompt: {prompt}\\nResponse: {response}\\n\")\n",
    "\n",
    "# Run the async function in Google Colab\n",
    "loop = asyncio.get_event_loop()\n",
    "loop.run_until_complete(main())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kdfIOJOGDiL9"
   },
   "source": [
    "## モニターとしてのScorersの使用\n",
    "\n",
    "モニターは、操作をブロックすることなく、時間の経過とともに品質メトリクスを追跡するのに役立ちます。これは以下に有用です：\n",
    "\n",
    "* 品質トレンドの特定\n",
    "* モデルドリフトの検出\n",
    "* モデル改善のためのデータ収集\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hpEQ3TDS3lVh"
   },
   "outputs": [],
   "source": [
    "import weave\n",
    "from weave import Scorer\n",
    "import json\n",
    "import xml.etree.ElementTree as ET\n",
    "import random\n",
    "import asyncio\n",
    "import nest_asyncio  # Required for Google Colab\n",
    "\n",
    "# Apply nest_asyncio to avoid event loop issues in Google Colab\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# ==== 1. Define Text Generation Function ====\n",
    "@weave.op\n",
    "def generate_text(prompt: str) -> str:\n",
    "    \"\"\"Simulated LLM response generation.\"\"\"\n",
    "    if prompt.lower() == \"json\":\n",
    "        return '{\"message\": \"Hello, world!\"}'  # Valid JSON\n",
    "    elif prompt.lower() == \"xml\":\n",
    "        return \"<message>Hello, world!</message>\"  # Valid XML\n",
    "    else:\n",
    "        return \"Generated response...\"\n",
    "\n",
    "# ==== 2. Custom Scorer for JSON Validation ====\n",
    "class CustomJSONScorer(Scorer):\n",
    "    @weave.op\n",
    "    def score(self, output: str) -> dict:\n",
    "        \"\"\"Check if the output is valid JSON.\"\"\"\n",
    "        try:\n",
    "            json.loads(output)\n",
    "            return {\"valid_json\": True}\n",
    "        except json.JSONDecodeError:\n",
    "            return {\"valid_json\": False}\n",
    "\n",
    "# ==== 3. Custom Scorer for XML Validation ====\n",
    "class CustomXMLScorer(Scorer):\n",
    "    @weave.op\n",
    "    def score(self, output: str) -> dict:\n",
    "        \"\"\"Check if the output is valid XML.\"\"\"\n",
    "        try:\n",
    "            ET.fromstring(output)\n",
    "            return {\"valid_xml\": True}\n",
    "        except ET.ParseError:\n",
    "            return {\"valid_xml\": False}\n",
    "\n",
    "# ==== 4. Function to Generate Response with Monitoring ====\n",
    "async def generate_with_monitoring(prompt: str) -> str:\n",
    "    \"\"\"\n",
    "    Generates a response and applies monitoring (randomly 10% of the time).\n",
    "    \"\"\"\n",
    "    # Generate text and capture tracking info\n",
    "    result, call = generate_text.call(prompt)\n",
    "\n",
    "    # Sample monitoring (only apply scorers to 10% of calls)\n",
    "    if random.random() < 0.1:\n",
    "        # Apply manual scorers asynchronously\n",
    "        json_score = await call.apply_scorer(CustomJSONScorer())\n",
    "        xml_score = await call.apply_scorer(CustomXMLScorer())\n",
    "\n",
    "        print(f\"Monitoring Applied - JSON Valid: {json_score.result['valid_json']}, XML Valid: {xml_score.result['valid_xml']}\")\n",
    "\n",
    "    return result\n",
    "\n",
    "# ==== 5. Run Test Cases ====\n",
    "async def main():\n",
    "    prompts = [\"json\", \"xml\", \"text\"]\n",
    "\n",
    "    for prompt in prompts:\n",
    "        response = await generate_with_monitoring(prompt)\n",
    "        print(f\"Prompt: {prompt}\\nResponse: {response}\\n\")\n",
    "\n",
    "# Run the async function in Google Colab\n",
    "loop = asyncio.get_event_loop()\n",
    "loop.run_until_complete(main())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# メディア"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 画像"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m\u001b[1mweave\u001b[0m: wandb version 0.22.2 is available!  To upgrade, please run:\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[36m\u001b[1mweave\u001b[0m: Logged in as Weights & Biases user: keisuke-kamata.\n",
      "\u001b[36m\u001b[1mweave\u001b[0m: View Weave data at https://wandb.ai/wandb-japan/weave-handson-20251015/weave\n",
      "\u001b[36m\u001b[1mweave\u001b[0m: 🍩 https://wandb.ai/wandb-japan/weave-handson-20251015/r/call/0199e585-4cbb-75d4-8d3e-1b6059c7fe48\n"
     ]
    }
   ],
   "source": [
    "import weave\n",
    "from openai import OpenAI\n",
    "import requests\n",
    "from PIL import Image\n",
    "\n",
    "weave.init(project_name=PROJECT)\n",
    "#os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
    "client = OpenAI(\n",
    "    api_key=os.environ.get(\"OPENAI_API_KEY\")\n",
    ")\n",
    "\n",
    "@weave.op()\n",
    "def generate_image(prompt: str) -> Image:\n",
    "    response = client.images.generate(\n",
    "        model=\"dall-e-3\",\n",
    "        prompt=prompt,\n",
    "        size=\"1024x1024\",\n",
    "        quality=\"standard\",\n",
    "        n=1,\n",
    "    )\n",
    "    image_url = response.data[0].url\n",
    "    image_response = requests.get(image_url, stream=True)\n",
    "    image = Image.open(image_response.raw)\n",
    "\n",
    "    # return a PIL.Image.Image object to be logged as an image\n",
    "    return image\n",
    "\n",
    "image = generate_image(\"a cat with a pumpkin hat\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 音声"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m\u001b[1mweave\u001b[0m: Flushing 2 pending tasks...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ Flushing tasks: 0.0% (0/2)\n",
      "→ Flushing tasks: 0.0% (0/2)\n",
      "→ Flushing tasks: 0.0% (0/2)\n",
      "→ Flushing tasks: 0.0% (0/2)\n",
      "→ Flushing tasks: 0.0% (0/2)\n",
      "→ Flushing tasks: 0.0% (0/2)\n",
      "→ Flushing tasks: 0.0% (0/2)\n",
      "→ Flushing tasks: 50.0% (1/2)\n",
      "→ Flushing tasks: 50.0% (1/2)\n",
      "→ Flushing tasks: 33.3% (1/3)\n",
      "→ Flushing tasks: 33.3% (1/3)\n",
      "→ Flushing tasks: 33.3% (1/3)\n",
      "→ Flushing tasks: 33.3% (1/3)\n",
      "→ Flushing tasks: 33.3% (1/3)\n",
      "→ Flushing tasks: 33.3% (1/3)\n",
      "→ Flushing tasks: 33.3% (1/3)\n",
      "→ Flushing tasks: 33.3% (1/3)\n",
      "→ Flushing tasks: 33.3% (1/3)\n",
      "→ Flushing tasks: 66.7% (2/3)\n",
      "→ Flushing tasks: 66.7% (2/3)\n",
      "→ Flushing tasks: 66.7% (2/3)\n",
      "→ Flushing tasks: 66.7% (2/3)\n",
      "→ Flushing tasks: 66.7% (2/3)\n",
      "→ Flushing tasks: 100.0% (1/1)\n",
      "✓ Flushing tasks: 100.0% (1/1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m\u001b[1mweave\u001b[0m: wandb version 0.22.2 is available!  To upgrade, please run:\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[36m\u001b[1mweave\u001b[0m: Logged in as Weights & Biases user: keisuke-kamata.\n",
      "\u001b[36m\u001b[1mweave\u001b[0m: View Weave data at https://wandb.ai/wandb-japan/weave-handson-20251015/weave\n",
      "\u001b[36m\u001b[1mweave\u001b[0m: 🍩 https://wandb.ai/wandb-japan/weave-handson-20251015/r/call/0199e585-9f0b-7930-aba3-e764b242dab1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<wave.Wave_read at 0x11995af10>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import weave\n",
    "from openai import OpenAI\n",
    "import wave\n",
    "\n",
    "weave.init(project_name=PROJECT)\n",
    "#os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
    "client = OpenAI(\n",
    "    api_key=os.environ.get(\"OPENAI_API_KEY\")\n",
    ")\n",
    "@weave.op\n",
    "def make_audio_file_streaming(text: str) -> wave.Wave_read:\n",
    "    with client.audio.speech.with_streaming_response.create(\n",
    "        model=\"tts-1\",\n",
    "        voice=\"alloy\",\n",
    "        input=text,\n",
    "        response_format=\"wav\",\n",
    "    ) as res:\n",
    "        res.stream_to_file(\"output.wav\")\n",
    "\n",
    "    # return a wave.Wave_read object to be logged as audio\n",
    "    return wave.open(\"output.wav\")\n",
    "\n",
    "make_audio_file_streaming(\"Hello, how are you? What did you do yesterday?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 動画"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m\u001b[1mweave\u001b[0m: Flushing 1 pending tasks...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ Flushing tasks: 0.0% (0/1)\n",
      "→ Flushing tasks: 0.0% (0/1)\n",
      "→ Flushing tasks: 0.0% (0/1)\n",
      "→ Flushing tasks: 0.0% (0/1)\n",
      "→ Flushing tasks: 0.0% (0/1)\n",
      "→ Flushing tasks: 100.0% (1/1)\n",
      "✓ Flushing tasks: 100.0% (1/1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m\u001b[1mweave\u001b[0m: wandb version 0.22.2 is available!  To upgrade, please run:\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[36m\u001b[1mweave\u001b[0m: Logged in as Weights & Biases user: keisuke-kamata.\n",
      "\u001b[36m\u001b[1mweave\u001b[0m: View Weave data at https://wandb.ai/wandb-japan/weave-handson-20251015/weave\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Missing key inputs argument! To use the Google AI API, provide (`api_key`) arguments. To use the Google Cloud API, provide (`vertexai`, `project` & `location`) arguments.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 42\u001b[39m\n\u001b[32m     38\u001b[39m         generated_video.video.save(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mvideo\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.mp4\u001b[39m\u001b[33m\"\u001b[39m)  \u001b[38;5;66;03m# save the video\u001b[39;00m\n\u001b[32m     39\u001b[39m         save_videos_in_weave(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mvideo\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.mp4\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m \u001b[43mgenerate_videos\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mPanning wide shot of a calico kitten sleeping in the sunshine\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Project/wandb-handson/weave_introduction_handson/.venv/lib/python3.11/site-packages/weave/trace/op.py:1244\u001b[39m, in \u001b[36mop.<locals>.op_deco.<locals>.create_wrapper.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m   1242\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[32m   1243\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper\u001b[39m(*args: P.args, **kwargs: P.kwargs) -> R:\n\u001b[32m-> \u001b[39m\u001b[32m1244\u001b[39m     res, _ = \u001b[43m_call_sync_func\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1245\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mOp\u001b[49m\u001b[43m[\u001b[49m\u001b[43mP\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mR\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwrapper\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m__should_raise\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m   1246\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1247\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(R, res)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Project/wandb-handson/weave_introduction_handson/.venv/lib/python3.11/site-packages/weave/trace/op.py:501\u001b[39m, in \u001b[36m_call_sync_func\u001b[39m\u001b[34m(op, __weave, __should_raise, __require_explicit_finish, *args, **kwargs)\u001b[39m\n\u001b[32m    498\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m output\n\u001b[32m    500\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m501\u001b[39m     res = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    502\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    503\u001b[39m     finish(exception=e)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 22\u001b[39m, in \u001b[36mgenerate_videos\u001b[39m\u001b[34m(prompt)\u001b[39m\n\u001b[32m     20\u001b[39m \u001b[38;5;129m@weave\u001b[39m.op()\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate_videos\u001b[39m(prompt):\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m     client = \u001b[43mgenai\u001b[49m\u001b[43m.\u001b[49m\u001b[43mClient\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# read API key from GOOGLE_API_KEY\u001b[39;00m\n\u001b[32m     23\u001b[39m     operation = client.models.generate_videos(\n\u001b[32m     24\u001b[39m         model=\u001b[33m\"\u001b[39m\u001b[33mveo-2.0-generate-001\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     25\u001b[39m         prompt=\u001b[33m\"\u001b[39m\u001b[33mPanning wide shot of a calico kitten sleeping in the sunshine\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     29\u001b[39m             ),\n\u001b[32m     30\u001b[39m         )\n\u001b[32m     32\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m operation.done:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Project/wandb-handson/weave_introduction_handson/.venv/lib/python3.11/site-packages/google/genai/client.py:265\u001b[39m, in \u001b[36mClient.__init__\u001b[39m\u001b[34m(self, vertexai, api_key, credentials, project, location, debug_config, http_options)\u001b[39m\n\u001b[32m    262\u001b[39m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    263\u001b[39m     http_options = HttpOptions(base_url=base_url)\n\u001b[32m--> \u001b[39m\u001b[32m265\u001b[39m \u001b[38;5;28mself\u001b[39m._api_client = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_api_client\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    266\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvertexai\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvertexai\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    267\u001b[39m \u001b[43m    \u001b[49m\u001b[43mapi_key\u001b[49m\u001b[43m=\u001b[49m\u001b[43mapi_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    268\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcredentials\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcredentials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    269\u001b[39m \u001b[43m    \u001b[49m\u001b[43mproject\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproject\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    270\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    271\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdebug_config\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_debug_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    272\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhttp_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhttp_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    273\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    275\u001b[39m \u001b[38;5;28mself\u001b[39m._aio = AsyncClient(\u001b[38;5;28mself\u001b[39m._api_client)\n\u001b[32m    276\u001b[39m \u001b[38;5;28mself\u001b[39m._models = Models(\u001b[38;5;28mself\u001b[39m._api_client)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Project/wandb-handson/weave_introduction_handson/.venv/lib/python3.11/site-packages/google/genai/client.py:311\u001b[39m, in \u001b[36mClient._get_api_client\u001b[39m\u001b[34m(vertexai, api_key, credentials, project, location, debug_config, http_options)\u001b[39m\n\u001b[32m    294\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m debug_config \u001b[38;5;129;01mand\u001b[39;00m debug_config.client_mode \u001b[38;5;129;01min\u001b[39;00m [\n\u001b[32m    295\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mrecord\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m    296\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mreplay\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m    297\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mauto\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m    298\u001b[39m ]:\n\u001b[32m    299\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m ReplayApiClient(\n\u001b[32m    300\u001b[39m       mode=debug_config.client_mode,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[32m    301\u001b[39m       replay_id=debug_config.replay_id,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    308\u001b[39m       http_options=http_options,\n\u001b[32m    309\u001b[39m   )\n\u001b[32m--> \u001b[39m\u001b[32m311\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mBaseApiClient\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    312\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvertexai\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvertexai\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    313\u001b[39m \u001b[43m    \u001b[49m\u001b[43mapi_key\u001b[49m\u001b[43m=\u001b[49m\u001b[43mapi_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    314\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcredentials\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcredentials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    315\u001b[39m \u001b[43m    \u001b[49m\u001b[43mproject\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproject\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    316\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    317\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhttp_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhttp_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    318\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Project/wandb-handson/weave_introduction_handson/.venv/lib/python3.11/site-packages/google/genai/_api_client.py:661\u001b[39m, in \u001b[36mBaseApiClient.__init__\u001b[39m\u001b[34m(self, vertexai, api_key, credentials, project, location, http_options)\u001b[39m\n\u001b[32m    659\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# Implicit initialization or missing arguments.\u001b[39;00m\n\u001b[32m    660\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.api_key:\n\u001b[32m--> \u001b[39m\u001b[32m661\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    662\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mMissing key inputs argument! To use the Google AI API,\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    663\u001b[39m         \u001b[33m'\u001b[39m\u001b[33m provide (`api_key`) arguments. To use the Google Cloud API,\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    664\u001b[39m         \u001b[33m'\u001b[39m\u001b[33m provide (`vertexai`, `project` & `location`) arguments.\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    665\u001b[39m     )\n\u001b[32m    666\u001b[39m   \u001b[38;5;28mself\u001b[39m._http_options.base_url = \u001b[33m'\u001b[39m\u001b[33mhttps://generativelanguage.googleapis.com/\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    667\u001b[39m   \u001b[38;5;28mself\u001b[39m._http_options.api_version = \u001b[33m'\u001b[39m\u001b[33mv1beta\u001b[39m\u001b[33m'\u001b[39m\n",
      "\u001b[31mValueError\u001b[39m: Missing key inputs argument! To use the Google AI API, provide (`api_key`) arguments. To use the Google Cloud API, provide (`vertexai`, `project` & `location`) arguments."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m\u001b[1mweave\u001b[0m: 🍩 https://wandb.ai/wandb-japan/weave-handson-20251015/r/call/0199e585-b66e-7723-a3a5-904a0b3be4ce\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "from moviepy.editor import VideoFileClip, ColorClip, VideoClip\n",
    "import weave\n",
    "\n",
    "weave.init(PROJECT)\n",
    "\n",
    "@weave.op()\n",
    "def store_videos(name, client, generated_video):\n",
    "  client.files.download(file=generated_video.video)\n",
    "  generated_video.video.save(f\"new_video{name}.mp4\")  # save the video\n",
    "\n",
    "@weave.op()\n",
    "def save_videos_in_weave(video_path):\n",
    "    clip = VideoFileClip(video_path, has_mask=False, audio=True)\n",
    "    new_clip = clip.subclip(0, 1)\n",
    "    return new_clip\n",
    "\n",
    "@weave.op()\n",
    "def generate_videos(prompt):\n",
    "    client = genai.Client()  # read API key from GOOGLE_API_KEY\n",
    "    operation = client.models.generate_videos(\n",
    "        model=\"veo-2.0-generate-001\",\n",
    "        prompt=\"Panning wide shot of a calico kitten sleeping in the sunshine\",\n",
    "        config=types.GenerateVideosConfig(\n",
    "            person_generation=\"dont_allow\",  # \"dont_allow\" or \"allow_adult\"\n",
    "            aspect_ratio=\"16:9\",  # \"16:9\" or \"9:16\"\n",
    "            ),\n",
    "        )\n",
    "    \n",
    "    while not operation.done:\n",
    "        time.sleep(10)\n",
    "        operation = client.operations.get(operation)\n",
    "    \n",
    "    for n, generated_video in enumerate(operation.response.generated_videos):\n",
    "        client.files.download(file=generated_video.video)\n",
    "        generated_video.video.save(f\"video{n}.mp4\")  # save the video\n",
    "        save_videos_in_weave(f\"video{n}.mp4\")\n",
    "        \n",
    " \n",
    "generate_videos(\"Panning wide shot of a calico kitten sleeping in the sunshine\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
