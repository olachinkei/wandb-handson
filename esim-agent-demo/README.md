# eSIM Agent Demo

A multi-agent system for eSIM services built with OpenAI Agents SDK and Weights & Biases Weave.

## ğŸŒŸ Features

- **Multi-Agent Architecture**: Specialized agents for different tasks
  - **eSIM Agent** (Main): Orchestrates all interactions
  - **Plan Search Agent**: Finds eSIM plans by destination and duration
  - **Booking Agent**: Handles purchase process
  - **RAG Agent**: Answers general eSIM questions using knowledge base

- **OpenAI Agents SDK**: Modern agent framework with tool calling and handoffs
- **Weave Integration**: Full observability and tracing
- **RAG with Vector Store**: Knowledge base for eSIM information

## ğŸ—ï¸ Architecture

```
User
  â†“
eSIM Agent (Main Orchestrator)
  â”œâ”€â”€ Plan Search Agent
  â”‚   â”œâ”€â”€ ask_country_period (tool)
  â”‚   â””â”€â”€ plan_search (tool)
  â”œâ”€â”€ Booking Agent
  â”‚   â”œâ”€â”€ status_check (tool)
  â”‚   â””â”€â”€ cost_calculator (tool)
  â””â”€â”€ RAG Agent
      â””â”€â”€ file_search (OpenAI Vector Store)
```

### Agent Details

#### 1. **eSIM Agent** (Main Orchestrator)
- **Role:** Master coordinator that handles overall communication
- Routes user requests to appropriate sub-agents
- Manages conversation flow and context
- Coordinates responses from multiple agents

#### 2. **Plan Search Agent**
- **Role:** Provides pricing information based on country and duration
- **Tools:**
  - `ask_country_period`: Collects travel destination and dates, converts date ranges to days
  - `plan_search`: Searches database for plans (local/regional/global)
- **Returns:** Plan details with pricing, coverage, and booking prompt

#### 3. **RAG Agent**
- **Role:** Handles Q&A using Retrieval-Augmented Generation
- **Workflow:**
  1. Determines if question is eSIM-related
  2. If not related: Politely declines to answer
  3. If related: Searches knowledge base using Vector Store
  4. Provides response with source citations
- **Knowledge Base:** 9 markdown documents covering device compatibility, activation, troubleshooting, etc.

#### 4. **Booking Agent**
- **Role:** Handles booking and payment processing
- **Tools:**
  - `status_check`: Verifies user login status and payment method
  - `cost_calculator`: Calculates total cost (price Ã— quantity + 8% tax)
- **Flow:** Guides user through login â†’ payment setup â†’ booking confirmation

## ğŸ“‹ Prerequisites

- Python 3.10 or higher
- [uv](https://github.com/astral-sh/uv) package manager
- OpenAI API key
- Weights & Biases account (for Weave)

## ğŸš€ Setup

### 1. Install Dependencies

```bash
# Install uv if you haven't already
curl -LsSf https://astral.sh/uv/install.sh | sh

# Install project dependencies
uv sync --extra dev
```

### 2. Configure Environment Variables

Create a `.env` file in the project root:

```bash
# OpenAI API Key
OPENAI_API_KEY=sk-...

# Weights & Biases API Key
WANDB_API_KEY=...
```

Get your API keys:
- OpenAI: https://platform.openai.com/api-keys
- W&B: https://wandb.ai/authorize

### 3. Configure Project Settings

Edit `config/config.yaml` to customize:
- Weave project entity and name
- Agent models
- RAG settings
- Tool configurations

### 4. Set Up RAG Knowledge Base

Run the RAG preparation script to create and populate the vector store:

```bash
uv run python rag_prep.py
```

This will:
- Upload knowledge base documents to OpenAI
- Create a vector store
- Save vector store info for the RAG agent

## ğŸ’» Usage

### Demo Modes

The demo supports three modes to test the eSIM agent system:

```bash
# 1. Comprehensive Scenarios (30 test cases - recommended for thorough testing)
uv run python demo.py comprehensive

# 2. Quick Sample Queries (10 test cases - fast overview)
uv run python demo.py sample

# 3. Interactive Chat Mode (manual testing)
uv run python demo.py interactive

# Show help
uv run python demo.py help
```

#### Comprehensive Mode (30 Scenarios)

Tests all agent capabilities across 6 categories:

- **ğŸ“‹ Plan Search (8)**: Basic queries, regional/global plans, unsupported countries, ambiguous requests
- **ğŸ“š RAG Questions (8)**: Device compatibility, activation, setup, troubleshooting, security
- **ğŸ›’ Booking (4)**: Direct purchase, quantity specification, post-search booking
- **ğŸ”€ Mixed (5)**: Combined plan search + RAG + booking workflows
- **âŒ Out of Scope (3)**: Unrelated questions (weather, restaurants, stocks)
- **ğŸ¤” Ambiguous (2)**: Unclear user intent requiring clarification

```bash
# Run with verbose output to see guardrail checks
uv run python demo.py comprehensive --verbose
```

#### Sample Mode (10 Queries)

Quick test with representative queries from each category:

```bash
uv run python demo.py sample
```

#### Interactive Mode

Chat directly with the agent:

```bash
uv run python demo.py interactive
```

### Run with Intelligent Guardrails (Programmatic)

The `run_agent_with_guardrails()` function applies intelligent, context-aware guardrails:

```python
from demo import run_agent_with_guardrails
import asyncio

# Run with guardrails (default, silent mode)
response = asyncio.run(run_agent_with_guardrails("What devices support eSIM?"))

# Run with verbose mode to see categorization and guardrail checks
response = asyncio.run(run_agent_with_guardrails("What devices support eSIM?", verbose=True))
```

**Intelligent Guardrail System:**

1. **Query Categorization** (LLM-as-a-Judge):
   - Automatically categorizes every query: `PLAN_SEARCH`, `RAG_QUESTION`, `BOOKING`, `MIXED`, `OUT_OF_SCOPE`
   - Sub-categories provide detailed intent analysis
   - Recorded to Weave for analytics

2. **Conditional Citation Check** (RAG queries only):
   - **Only applied to `RAG_QUESTION` and `MIXED` queries**
   - Plan Search and Booking queries skip citation checks (performance optimization)
   - Blocks response if RAG response lacks proper source citations

**Blocking Behavior:**
- For RAG queries without citations â†’ Returns error message in Japanese
- For other query types â†’ No blocking, citation check skipped

**Error Message:**
```
âš ï¸ ç”³ã—è¨³ã”ã–ã„ã¾ã›ã‚“ã€‚é©åˆ‡ãªå‚ç…§æƒ…å ±ã‚’å«ã‚€å›ç­”ã‚’ä½œæˆã§ãã¾ã›ã‚“ã§ã—ãŸã€‚è³ªå•ã‚’è¨€ã„æ›ãˆã¦ãŠè©¦ã—ãã ã•ã„ã€‚
(Sorry, we couldn't create a response with appropriate references. Please rephrase your question.)
```

**Example Output (Verbose Mode):**
```
ğŸ” Step 1: Categorizing Query...
  ğŸ“‚ Category: RAG_QUESTION
  ğŸ“ Sub-category: compatibility
  ğŸ’­ Reasoning: The user is asking about device compatibility...

ğŸ¤– Step 2: Running Agent...

ğŸ›¡ï¸ Step 3: Applying Citation Guardrail (RAG Query Detected)...
  ğŸ“š Source Citation: True

âœ… Citation check passed! Response includes proper references.
```

### Test with Sample Queries

Run 10 predefined queries with intelligent guardrails:

```python
# Run with verbose mode to see categorization and guardrails
asyncio.run(run_sample_queries(verbose=True))

# Run in silent mode (default)
asyncio.run(run_sample_queries())
```

Or test a single query:

```python
# With verbose mode (shows categorization + citation checks)
asyncio.run(single_query_demo("What devices support eSIM?", verbose=True))

# Silent mode
asyncio.run(single_query_demo("I'm traveling to Japan for 7 days"))
```

## ğŸ§ª Testing

Run the test suite:

```bash
# Run all tests
uv run pytest

# Run specific test file
uv run pytest tests/test_utils.py -v

# Run with coverage
uv run pytest --cov=src --cov-report=html
```

## ğŸ“Š Evaluation

### Running Evaluations

This project includes comprehensive evaluation using Weights & Biases Weave:

```bash
# Run all evaluations
uv run python evaluation/eval.py

# Run individual evaluations
uv run python evaluation/eval.py plan_search  # Plan Search Agent only
uv run python evaluation/eval.py rag          # RAG Agent only
uv run python evaluation/eval.py booking      # Booking Agent only
uv run python evaluation/eval.py end_to_end   # Full system workflow
```

### Evaluation Metrics

#### **Plan Search Agent** (6 scorers, 8 scenarios)
- âœ“ **tool_accuracy**: Correct tool selection and execution
- âœ“ **accuracy**: Accurate pricing, plan type, country, and days
- âœ“ **booking_prompt_correct**: Appropriate booking confirmation prompts
- âœ“ **service_availability_correct**: Graceful handling of unavailable countries
- âœ“ **provides_complete_answer**: Complete response vs asking clarification vs incomplete
- âœ“ **clarification_appropriate**: Whether asking for clarification is appropriate

#### **RAG Agent** (7 scorers - LLM-as-a-judge, 8 scenarios)
- âœ“ **faithfulness**: Response accuracy to retrieved context
- âœ“ **answer_relevancy**: Response relevance to user question
- âœ“ **source_citation**: Provides reference indicators
- âœ“ **out_of_scope_handling**: Correctly redirects non-eSIM questions
- âœ“ **accuracy**: Overall answer correctness with topic coverage
- âœ“ **provides_complete_answer**: Complete response vs asking clarification vs incomplete
- âœ“ **clarification_appropriate**: Whether asking for clarification is appropriate

#### **Booking Agent** (5 scorers, 6 scenarios)
- âœ“ **tool_accuracy**: Correct tool usage (status_check, cost_calculator)
- âœ“ **booking_flow_completion**: Proper flow with login/payment prompts
- âœ“ **accuracy**: Accurate total cost calculation (LLM judge)
- âœ“ **provides_complete_answer**: Complete response vs asking clarification vs incomplete
- âœ“ **clarification_appropriate**: Whether asking for clarification is appropriate

#### **End-to-End System** (8 scorers, 15 scenarios)
- âœ“ **agent_sequence_correct**: Correct agent handoff sequence
- âœ“ **tool_usage_correct**: All expected tools used
- âœ“ **final_score & meets_requirements**: End result validation via LLM-as-a-judge
- âœ“ **step_count_correct & step_efficiency**: Workflow completed within expected step range
- âœ“ **reflection_detected**: Error correction and retry behavior
- âœ“ **overall_success**: Complete workflow success
- âœ“ **provides_complete_answer**: Complete response vs asking clarification vs incomplete
- âœ“ **clarification_appropriate**: Whether asking for clarification is appropriate

### Test Scenarios

- **Plan Search**: 8 scenarios (5 standard + 2 negative/unavailable + 1 ambiguous)
- **RAG**: 8 scenarios (6 eSIM questions + 2 out-of-scope)
- **Booking**: 6 scenarios (5 standard + 1 ambiguous)
- **End-to-End**: 15 scenarios (8 plan search flows + 4 RAG flows + 3 direct booking flows)

View evaluation results in Weave: `https://wandb.ai/{entity}/{project}/weave`

## ğŸ“ Project Structure

```
esim-agent-demo/
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ config.yaml              # Main configuration
â”‚   â””â”€â”€ README.md                # Config documentation
â”œâ”€â”€ database/
â”‚   â”œâ”€â”€ price_list.json          # eSIM pricing data (35 countries, 9 regions)
â”‚   â”œâ”€â”€ RAG_docs/                # Knowledge base (9 markdown documents)
â”‚   â””â”€â”€ vector_store_info.json   # Vector store metadata
â”œâ”€â”€ cache/
â”‚   â””â”€â”€ user_cache.json          # Mock user data
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ agents/
â”‚   â”‚   â”œâ”€â”€ esim_agent.py        # Main orchestrator
â”‚   â”‚   â”œâ”€â”€ plan_search_agent.py # Plan search specialist
â”‚   â”‚   â”œâ”€â”€ booking_agent.py     # Booking specialist
â”‚   â”‚   â””â”€â”€ rag_agent.py         # Q&A specialist
â”‚   â”œâ”€â”€ tools.py                 # Agent tools (4 tools)
â”‚   â””â”€â”€ utils.py                 # Utility functions
â”œâ”€â”€ evaluation/
â”‚   â”œâ”€â”€ eval.py                  # Main evaluation runner (480+ lines)
â”‚   â”œâ”€â”€ scorers.py               # Base & common scorers (2 common scorers)
â”‚   â”œâ”€â”€ scorers_plan_search.py   # Plan Search scorers (4 + 2 common = 6)
â”‚   â”œâ”€â”€ scorers_rag.py           # RAG scorers (5 + 2 common = 7)
â”‚   â”œâ”€â”€ scorers_booking.py       # Booking scorers (3 + 2 common = 5)
â”‚   â”œâ”€â”€ scorers_end_to_end.py    # End-to-End scorers (6 + 2 common = 8)
â”‚   â”œâ”€â”€ scenarios/               # Test scenarios (37 total)
â”‚   â”‚   â”œâ”€â”€ plan_search_scenarios.json    # 8 scenarios
â”‚   â”‚   â”œâ”€â”€ rag_scenarios.json            # 8 scenarios
â”‚   â”‚   â”œâ”€â”€ booking_scenarios.json        # 6 scenarios
â”‚   â”‚   â””â”€â”€ end_to_end_scenarios.json     # 15 scenarios
â”‚   â””â”€â”€ README.md                # Evaluation guide
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_utils.py            # Utils tests (20 tests)
â”‚   â”œâ”€â”€ test_tools.py            # Tools tests (13 tests)
â”‚   â”œâ”€â”€ test_rag_prep.py         # RAG prep tests (4 tests)
â”‚   â””â”€â”€ evaluation/
â”‚       â””â”€â”€ test_eval.py         # Evaluation tests (21 tests)
â”œâ”€â”€ demo.py                      # Interactive demo
â”œâ”€â”€ rag_prep.py                  # RAG setup script
â”œâ”€â”€ pyproject.toml               # Dependencies
â”œâ”€â”€ uv.lock                      # Lock file
â”œâ”€â”€ README.md                    # This file
â””â”€â”€ AGENT.md                     # Detailed technical documentation
```

## ğŸ“Š Observability

All agent interactions are automatically traced to Weights & Biases Weave:

1. Run the demo or tests
2. Check your Weave project at: `https://wandb.ai/{entity}/{project}/weave`

Weave provides:
- Full conversation traces
- Tool call details
- Agent handoff visualization
- Performance metrics

## ğŸ“ Development Notes

- All tools have `_impl` versions for testing
- Use `@function_tool` decorator for agent tools
- Vector store is created once and reused
- Mock data in `cache/` for testing without real auth

## ğŸ“„ License

MIT License - see LICENSE file for details

## ğŸ™ Acknowledgments

- OpenAI Agents SDK
- Weights & Biases Weave
- Python community

## ğŸ“š Additional Resources

- [OpenAI Agents Documentation](https://openai.github.io/openai-agents-python/)
- [Weave Documentation](https://wandb.me/weave)
- [Project Documentation](AGENT.md)
