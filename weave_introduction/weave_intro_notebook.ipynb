{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "974b2c96"
   },
   "source": [
    "# üèÉ‚Äç‚ôÄÔ∏è Quickstart\n",
    "\n",
    "Get started using Weave to:\n",
    "- Log and debug language model inputs, outputs, and traces\n",
    "- Build rigorous, apples-to-apples evaluations for language model use cases\n",
    "- Organize all the information generated across the LLM workflow, from experimentation to evaluations to production\n",
    "\n",
    "See the full Weave documentation [here](https://wandb.me/weave).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "56dc5c9d"
   },
   "source": [
    "## ü™Ñ Install `weave` library and login\n",
    "\n",
    "\n",
    "Start by installing the library and logging in to your account.\n",
    "\n",
    "In this example, we're using openai so you should [add an openai API key](https://platform.openai.com/docs/quickstart/step-2-setup-your-api-key).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0921bbef-d089-4960-b624-505172f90d76",
    "outputId": "a262f278-48dc-411f-9fd9-58d69def7db9"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import wandb\n",
    "\n",
    "#========================================\n",
    "# Set up your environment variables properly\n",
    "#========================================\n",
    "# os.environ[\"WANDB_BASE_URL\"] = \"https://api.wandb.ai\"\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
    "# os.environ[\"WANDB_API_KEY\"] = \"\"\n",
    "# os.environ[\"AZURE_OPENAI_API_KEY\"] = os.getenv(\"AZURE_OPENAI_API_KEY\", \"\")\n",
    "# os.environ[\"AZURE_OPENAI_ENDPOINT\"] = os.getenv(\"AZURE_OPENAI_ENDPOINT\", \"\")\n",
    "# os.environ[\"AZURE_OPENAI_API_VERSION\"] = os.getenv(\"AZURE_OPENAI_API_VERSION\", \"2024-02-15-preview\")\n",
    "USE_AZURE_OPENAI = os.getenv(\"USE_AZURE_OPENAI\", \"false\").lower() == \"true\"\n",
    "\n",
    "wandb.login()\n",
    "PROJECT = \"<your entity name>/weave-handson\"\n",
    "\n",
    "import weave\n",
    "weave.init(PROJECT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "09467a33"
   },
   "source": [
    "# Track inputs & outputs of functions\n",
    "\n",
    "Weave allows users to track function calls: the code, inputs, outputs, and even LLM tokens & costs! In the following sections we will cover:\n",
    "\n",
    "* Custom Functions\n",
    "* Vendor Integrations\n",
    "* Nested Function Calling\n",
    "* Error Tracking\n",
    "\n",
    "Note: in all cases, we will:\n",
    "\n",
    "```python\n",
    "import weave                    # import the weave library\n",
    "weave.init('project-name')      # initialize tracking for a specific W&B project\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "99453d48"
   },
   "source": [
    "## Track custom functions\n",
    "\n",
    "Add the @weave.op decorator to the functions you want to track"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cdcd1bb7",
    "outputId": "8c252a02-fc1f-4499-9f4a-7bd4362a2f61"
   },
   "outputs": [],
   "source": [
    "@weave.op()\n",
    "def hello_world(user_input):\n",
    "    return user_input + \" world\"\n",
    "\n",
    "\n",
    "result = hello_world(\"hello\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "174cabef"
   },
   "source": [
    "You can find your interactive dashboard by clicking any of the  üëÜ wandb links above.\n",
    "\n",
    "After adding `weave.op` and calling the function, visit the link and see it tracked within your project.\n",
    "\n",
    "üí° We automatically track your code, have a look at the code tab!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5ef6f27b"
   },
   "source": [
    "## Vendor Integrations (OpenAI, Anthropic, Mistral, etc...)\n",
    "\n",
    "Here, we're automatically tracking all calls to `openai`. We automatically track a lot of LLM libraries, but it's really easy to add support for whatever LLM you're using, as you'll see below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_AZURE_OPENAI:\n",
    "    from openai import AzureOpenAI as OpenAI\n",
    "    client = OpenAI(\n",
    "        api_key=os.environ[\"AZURE_OPENAI_API_KEY\"],\n",
    "        api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n",
    "        azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"]\n",
    "    )\n",
    "    model_name = \"gpt-35-turbo-1106\"  # change to your model name\n",
    "else:\n",
    "    from openai import OpenAI\n",
    "    client = OpenAI()\n",
    "    model_name = \"gpt-3.5-turbo-1106\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ac8a2b59",
    "outputId": "2a289a04-6add-466a-fd57-dcfe76f290a8"
   },
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=model_name,\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a grammar checker, correct the following user input.\",\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": \"That was so easy, it was a piece of pie!\"},\n",
    "    ],\n",
    "    temperature=0,\n",
    ")\n",
    "generation = response.choices[0].message.content\n",
    "print(generation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "35fe43dc"
   },
   "source": [
    "## Track nested functions\n",
    "\n",
    "Now that you've seen the basics, let's combine all of the above and track some deeply nested functions alongside LLM calls.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "422050d0",
    "outputId": "9f1f22d6-2592-47ad-c83a-5c08095d632f"
   },
   "outputs": [],
   "source": [
    "@weave.op()\n",
    "def correct_grammar(user_input):\n",
    "\n",
    "    stripped = hello_world(user_input)\n",
    "    response = client.chat.completions.create(\n",
    "        model=model_name,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are a grammar checker, correct the following user input.\",\n",
    "            },\n",
    "            {\"role\": \"user\", \"content\": stripped},\n",
    "        ],\n",
    "        temperature=0,\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "\n",
    "result = correct_grammar(\"   That was so easy, it was a piece of pie!    \")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6f483413"
   },
   "source": [
    "## Track Errors\n",
    "\n",
    "Whenever your code crashes, weave will highlight what caused the issue. This is especially useful for finding things like JSON parsing issues that can occasionally happen when parsing data from LLM responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 356
    },
    "id": "14e7e103",
    "outputId": "1a121775-5b5c-4212-f205-d97c0ef7cac2"
   },
   "outputs": [],
   "source": [
    "@weave.op()\n",
    "def correct_grammar(user_input):\n",
    "    stripped = hello_world(user_input)\n",
    "    response = client.chat.completions.create(\n",
    "        model=model_name,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are a grammar checker, correct the following user input.\",\n",
    "            },\n",
    "            {\"role\": \"user\", \"content\": stripped},\n",
    "        ],\n",
    "        temperature=0,\n",
    "        response_format={\"type\": \"json_object\"},\n",
    "    )\n",
    "    return json.loads(response.choices[0].message.content)\n",
    "\n",
    "\n",
    "result = correct_grammar(\"   That was so easy, it was a piece of pie!    \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "knBo-VtWxl6g"
   },
   "source": [
    "# Trace Advanced Tips\n",
    "\n",
    "* Customize logged inputs and outputs\n",
    "* Control Sampling rate\n",
    "* Call Display Name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6KWJhiBsxl6h"
   },
   "source": [
    "### Control sampling rate\n",
    "\n",
    "You can control how frequently an op's calls are traced by setting the tracing_sample_rate parameter in the @weave.op decorator. This is useful for high-frequency ops where you only need to trace a subset of calls.\n",
    "\n",
    "Note that sampling rates are only applied to root calls. If an op has a sample rate, but is called by another op first, then that sampling rate will be ignored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cpX43hPbdeOZ"
   },
   "outputs": [],
   "source": [
    "@weave.op(tracing_sample_rate=0.1)  # Only trace ~10% of calls\n",
    "def high_frequency_op(x: int) -> int:\n",
    "    return x + 1\n",
    "\n",
    "@weave.op(tracing_sample_rate=1.0)  # Always trace (default)\n",
    "def always_traced_op(x: int) -> int:\n",
    "    return x + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ntJpevJQeLoA"
   },
   "source": [
    "### Call Display Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iqOMqMIseYSV",
    "outputId": "f1fa226c-f099-4c5a-f411-6f4bc0caab19"
   },
   "outputs": [],
   "source": [
    "# Decorate your function\n",
    "@weave.op\n",
    "def my_function(name: str):\n",
    "    return f\"Hello, {name}!\"\n",
    "\n",
    "# Call your function -- Weave will automatically track inputs and outputs\n",
    "print(my_function(\"World\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RV9wwY4veaz8"
   },
   "outputs": [],
   "source": [
    "# 1st method\n",
    "result = my_function(\"World\", __weave={\"display_name\": \"My Custom Display Name\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R8d4mcWserLq",
    "outputId": "1938848a-75f4-487b-d46b-09d4995a66cb"
   },
   "outputs": [],
   "source": [
    "# 2nd method\n",
    "result, call = my_function.call(\"World\")\n",
    "call.set_display_name(\"My Custom Display Name\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "-M2abQSbew9M",
    "outputId": "81b67f69-507c-46e1-9d9b-3ae52d23c762"
   },
   "outputs": [],
   "source": [
    "# 3rd method\n",
    "@weave.op(call_display_name=\"My Custom Display Name\")\n",
    "def my_function(name: str):\n",
    "    return f\"Hello, {name}!\"\n",
    "\n",
    "my_function(\"World\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ExWp8BFIxl6g"
   },
   "source": [
    "\n",
    "### Redacting PII\n",
    "\n",
    "Some organizations process Personally Identifiable Information (PII) such as names, phone numbers, and email addresses in their Large Language Model (LLM) workflows. Storing this data in Weights & Biases (W&B) Weave poses compliance and security risks.\n",
    "\n",
    "The Sensitive Data Protection feature allows you to automatically redact Personally Identifiable Information (PII) from a trace before it is sent to Weave servers. This feature integrates Microsoft Presidio into the Weave Python SDK, which means that you can control redaction settings at the SDK level.\n",
    "\n",
    "[Detailed Documentation](https://weave-docs.wandb.ai/guides/tracking/redact-pii)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f957c9c8"
   },
   "source": [
    "# Tracking Objects\n",
    "\n",
    "Organizing experimentation is difficult when there are many moving pieces. You can capture and organize the experimental details of your app like your system prompt or the model you're using within `weave.Objects`. This helps organize and compare different iterations of your app. In this section, we will cover:\n",
    "\n",
    "* General Object Tracking\n",
    "* Tracking Models\n",
    "* Tracking Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8f6129cc"
   },
   "source": [
    "## Prompt Tracking\n",
    "\n",
    "\n",
    "Weave is unopinionated about how a Prompt is constructed. If your needs are simple, you can use our built-in weave.StringPrompt or weave.MessagesPrompt classes. If your needs are more complex you can subclass those or our base class weave.Prompt and override the format method.\n",
    "\n",
    "When you publish one of these objects with weave.publish, it will appear in your Weave project on the \"Prompts\" page.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "08e4e852",
    "outputId": "d5b8c38a-0f00-4e9b-a09f-268f1ea1eb1c"
   },
   "outputs": [],
   "source": [
    "# StringPrompt1\n",
    "system_prompt = weave.StringPrompt(\"You are a pirate\")\n",
    "weave.publish(system_prompt, name=\"pirate_prompt\")\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "  model=model_name,\n",
    "  messages=[\n",
    "    {\n",
    "      \"role\": \"system\",\n",
    "      \"content\": system_prompt.format()\n",
    "    },\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": \"Explain general relativity in one paragraph.\"\n",
    "    }\n",
    "  ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ke21Y2OJg9Uo",
    "outputId": "f821bf2a-0577-4c96-9661-df18e4eb1524"
   },
   "outputs": [],
   "source": [
    "# StringPrompt2\n",
    "system_prompt = weave.StringPrompt(\"Talk like a pirate. I need to know I'm listening to a pirate.\")\n",
    "weave.publish(system_prompt, name=\"pirate_prompt\")\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "  model=model_name,\n",
    "  messages=[\n",
    "    {\n",
    "      \"role\": \"system\",\n",
    "      \"content\": system_prompt.format()\n",
    "    },\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": \"Explain general relativity in one paragraph.\"\n",
    "    }\n",
    "  ],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZrpZa01lg_-D",
    "outputId": "661423d9-9a58-4635-bce8-4f360838706d"
   },
   "outputs": [],
   "source": [
    "# MessagesPrompt1\n",
    "prompt = weave.MessagesPrompt([\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a stegosaurus, but don't be too obvious about it.\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"What's good to eat around here?\"\n",
    "    }\n",
    "])\n",
    "weave.publish(prompt, name=\"dino_prompt\")\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "  model=\"gpt-4o\",\n",
    "  messages=prompt.format(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WY5UZLgIhHYW",
    "outputId": "7a1cb983-044b-4d14-81fa-941493c870a0"
   },
   "outputs": [],
   "source": [
    "# parameterizing prompts\n",
    "prompt = weave.StringPrompt(\"Solve the equation {equation}\")\n",
    "weave.publish(prompt, name=\"calculator_prompt\")\n",
    "\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "  model=model_name,\n",
    "  messages=[\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": prompt.format(equation=\"1 + 1 = ?\")\n",
    "    }\n",
    "  ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0c873438"
   },
   "source": [
    "## Model Tracking\n",
    "\n",
    "Models are so common of an object type, that we have a special class to represent them: `weave.Model`. The only requirement is that we define a `predict` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e3e60d81",
    "outputId": "f0fb3219-66be-4048-e5a2-3db0c047a651"
   },
   "outputs": [],
   "source": [
    "class OpenAIGrammarCorrector(weave.Model):\n",
    "    # Properties are entirely user-defined\n",
    "    openai_model_name: str\n",
    "    system_message: str\n",
    "\n",
    "    @weave.op()\n",
    "    def predict(self, user_input):\n",
    "\n",
    "        response = client.chat.completions.create(\n",
    "            model=self.openai_model_name,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": self.system_message},\n",
    "                {\"role\": \"user\", \"content\": user_input},\n",
    "            ],\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "\n",
    "corrector = OpenAIGrammarCorrector(\n",
    "    openai_model_name=model_name,\n",
    "    system_message=\"You are a grammar checker, correct the following user input.\",\n",
    ")\n",
    "\n",
    "\n",
    "result = corrector.predict(\"     That was so easy, it was a piece of pie!       \")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "624b04c1"
   },
   "source": [
    "## Dataset Tracking\n",
    "\n",
    "Similar to models, a `weave.Dataset` object exists to help track, organize, and operate on datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ac2c5145",
    "outputId": "2918a900-8263-49e8-d784-4c35cde32407"
   },
   "outputs": [],
   "source": [
    "dataset = weave.Dataset(\n",
    "    name=\"grammar-correction\",\n",
    "    rows=[\n",
    "        {\n",
    "            \"user_input\": \"   That was so easy, it was a piece of pie!   \",\n",
    "            \"expected\": \"That was so easy, it was a piece of cake!\",\n",
    "        },\n",
    "        {\"user_input\": \"  I write good   \",\n",
    "         \"expected\": \"I write well\"},\n",
    "        {\n",
    "            \"user_input\": \"  GPT-3 is smartest AI model.   \",\n",
    "            \"expected\": \"GPT-3 is the smartest AI model.\",\n",
    "        },\n",
    "    ],\n",
    ")\n",
    "\n",
    "weave.publish(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f7d43bc3"
   },
   "source": [
    "Notice that we saved a versioned `GrammarCorrector` object that captures the configurations you're experimenting with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "71c3aaea"
   },
   "source": [
    "## Retrieve Published Objects & Ops\n",
    "\n",
    "You can publish objects and then retrieve them in your code. You can even call functions from your retrieved objects!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "caa5c5e0",
    "outputId": "57c8c724-9748-4892-864c-8f189b664a81"
   },
   "outputs": [],
   "source": [
    "ref_url = \"\"\n",
    "prompt = weave.ref(ref_url).get()\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "77b94dd5"
   },
   "source": [
    "# Offline Evaluation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "72bdf072"
   },
   "source": [
    "## Method1: [Standard Method](https://weave-docs.wandb.ai/guides/core-types/evaluations)\n",
    "Forces you to do both prediction and evaluation sample by sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 148
    },
    "id": "fpiwHM4WdV9A",
    "outputId": "f1cd62cf-422b-4de7-8990-a24b8a9f8f0b"
   },
   "outputs": [],
   "source": [
    "# Method1\n",
    "import weave\n",
    "from weave import Evaluation\n",
    "\n",
    "# Our dataset has \"input_text\" but our model expects \"question\"\n",
    "examples = [\n",
    "    {\"input_text\": \"What is the capital of France?\", \"expected\": \"Paris\"},\n",
    "    {\"input_text\": \"Who wrote 'To Kill a Mockingbird'?\", \"expected\": \"Harper Lee\"},\n",
    "    {\"input_text\": \"What is the square root of 64?\", \"expected\": \"8\"},\n",
    "]\n",
    "\n",
    "@weave.op()\n",
    "def preprocess_example(example):\n",
    "    # Rename input_text to question\n",
    "    return {\n",
    "        \"question\": example[\"input_text\"]\n",
    "    }\n",
    "\n",
    "@weave.op()\n",
    "def match_score(expected: str, output: dict) -> dict:\n",
    "    return {'match': expected == output['generated_text']}\n",
    "\n",
    "@weave.op()\n",
    "def function_to_evaluate(question: str):\n",
    "    return {'generated_text': f'Answer to: {question}'}\n",
    "\n",
    "# Create evaluation with preprocessing\n",
    "evaluation = weave.Evaluation(\n",
    "    dataset=examples,\n",
    "    scorers=[match_score],\n",
    "    preprocess_model_input=preprocess_example\n",
    ")\n",
    "\n",
    "# Initialize the Weave project\n",
    "weave.init(PROJECT)\n",
    "\n",
    "# In Jupyter/Colab, use 'await' instead of 'asyncio.run'\n",
    "async def run_eval():\n",
    "    await evaluation.evaluate(function_to_evaluate)\n",
    "\n",
    "await run_eval()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v6VE09bweQzu"
   },
   "source": [
    "## Method2: [EvaluationLogger](https://weave-docs.wandb.ai/guides/evaluation/evaluation_logger)\n",
    "Allows you to do batch predictions first!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5jeULyjweeBI",
    "outputId": "fa75ee71-71f9-4fbf-b211-6206714f01ce"
   },
   "outputs": [],
   "source": [
    "import weave\n",
    "from weave.flow.eval_imperative import EvaluationLogger\n",
    "\n",
    "# Initialize the logger with optional metadata\n",
    "eval_logger = EvaluationLogger(\n",
    "    model=\"my_local_model\",\n",
    "    dataset=\"my_dataset\"\n",
    ")\n",
    "\n",
    "# Example input data\n",
    "eval_samples = [\n",
    "    {'inputs': {'a': 1, 'b': 2}, 'expected': 3},\n",
    "    {'inputs': {'a': 2, 'b': 3}, 'expected': 5},\n",
    "    {'inputs': {'a': 3, 'b': 4}, 'expected': 7},\n",
    "]\n",
    "\n",
    "# Local model logic: simply add the numbers\n",
    "@weave.op\n",
    "def user_model(a: int, b: int) -> int:\n",
    "    return a + b\n",
    "\n",
    "# Evaluate each sample\n",
    "for sample in eval_samples:\n",
    "    inputs = sample[\"inputs\"]\n",
    "    model_output = user_model(**inputs)  # Call model with unpacked input\n",
    "\n",
    "    # Log the prediction\n",
    "    pred_logger = eval_logger.log_prediction(\n",
    "        inputs=inputs,\n",
    "        output=model_output\n",
    "    )\n",
    "\n",
    "    # Compare output with expected value\n",
    "    expected = sample[\"expected\"]\n",
    "    correctness_score = model_output == expected\n",
    "    pred_logger.log_score(\n",
    "        scorer=\"correctness\",\n",
    "        score=correctness_score\n",
    "    )\n",
    "\n",
    "    # Finalize log for this prediction\n",
    "    pred_logger.finish()\n",
    "\n",
    "# Log overall evaluation summary\n",
    "summary_stats = {\"subjective_overall_score\": 1.0}\n",
    "eval_logger.log_summary(summary_stats)\n",
    "\n",
    "print(\"Evaluation logging complete. View results in the Weave UI.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lk4DY3RqkKnf"
   },
   "source": [
    "# Online Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9ZRRHZyaDdpu"
   },
   "source": [
    "# Feedbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 356
    },
    "id": "kLBVpBlkxVdi",
    "outputId": "88608be4-dd3b-4609-dd4d-f160fe021222"
   },
   "outputs": [],
   "source": [
    "client = weave.init(PROJECT)\n",
    "call = client.get_call(\"\") #@param\n",
    "\n",
    "# Adding an emoji reaction\n",
    "call.feedback.add_reaction(\"üëç\")\n",
    "\n",
    "# Adding a note\n",
    "call.feedback.add_note(\"this is a note\")\n",
    "\n",
    "# Adding custom key/value pairs.\n",
    "# The first argument is a user-defined \"type\" string.\n",
    "# Feedback must be JSON serializable and less than 1 KB when serialized.\n",
    "call.feedback.add(\"correctness\", { \"value\": 5 })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M497nWagx0PP"
   },
   "source": [
    "# Scorers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FBnXzOzr1LcT"
   },
   "source": [
    "## Getting started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d0tqUV9_1KgA"
   },
   "outputs": [],
   "source": [
    "import weave\n",
    "from weave import Scorer\n",
    "\n",
    "\n",
    "class LengthScorer(Scorer):\n",
    "    @weave.op\n",
    "    def score(self, output: str) -> dict:\n",
    "        \"\"\"A simple scorer that checks output length.\"\"\"\n",
    "        return {\n",
    "            \"length\": len(output),\n",
    "            \"is_short\": len(output) < 100\n",
    "        }\n",
    "\n",
    "@weave.op\n",
    "def generate_text(prompt: str) -> str:\n",
    "    return \"Hello, world!\"\n",
    "\n",
    "# Get both result and Call object\n",
    "result, call = generate_text.call(\"Say hello\")\n",
    "\n",
    "# Now you can apply scorers\n",
    "await call.apply_scorer(LengthScorer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SfipbjSi2dee"
   },
   "source": [
    "## Using Scorers as Guardrails\n",
    "Guardrails act as safety checks that run before allowing LLM output to reach users. Here's a practical example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r9rKr-_g2YLk"
   },
   "outputs": [],
   "source": [
    "import weave\n",
    "from weave import Scorer\n",
    "import asyncio\n",
    "import nest_asyncio  # Required for running asyncio in Google Colab\n",
    "\n",
    "# Apply nest_asyncio to avoid event loop issues in Google Colab\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# ==== 1. Define text generation function ====\n",
    "@weave.op\n",
    "def generate_text(prompt: str) -> str:\n",
    "    \"\"\"Simulated LLM text generation (basic logic).\"\"\"\n",
    "    responses = {\n",
    "        \"hello\": \"Hello! How can I help you?\",\n",
    "        \"bad\": \"You are terrible!\",  # Example of toxic response\n",
    "        \"good\": \"You are wonderful!\",\n",
    "    }\n",
    "    return responses.get(prompt.lower(), \"I don't understand your request.\")\n",
    "\n",
    "# ==== 2. Define the Toxicity Scorer ====\n",
    "class ToxicityScorer(Scorer):\n",
    "    @weave.op\n",
    "    def score(self, output: str) -> dict:\n",
    "        \"\"\"\n",
    "        Evaluate the generated content for toxic language.\n",
    "        \"\"\"\n",
    "        toxic_words = {\"terrible\", \"hate\", \"stupid\"}  # Simple keyword-based detection\n",
    "        flagged = any(word in output.lower() for word in toxic_words)\n",
    "\n",
    "        return {\n",
    "            \"flagged\": flagged,\n",
    "            \"reason\": \"Detected toxic language\" if flagged else None\n",
    "        }\n",
    "\n",
    "# ==== 3. Function to generate safe responses ====\n",
    "async def generate_safe_response(prompt: str) -> str:\n",
    "    # Generate text using LLM\n",
    "    result, call = generate_text.call(prompt)\n",
    "\n",
    "    # Apply toxicity scoring\n",
    "    safety = await call.apply_scorer(ToxicityScorer())\n",
    "\n",
    "    # If flagged as toxic, return a warning message\n",
    "    if safety.result[\"flagged\"]:\n",
    "        return f\"I cannot generate that content: {safety.result['reason']}\"\n",
    "\n",
    "    return result\n",
    "\n",
    "# ==== 4. Run test cases ====\n",
    "async def main():\n",
    "    prompts = [\"hello\", \"bad\", \"good\"]\n",
    "\n",
    "    for prompt in prompts:\n",
    "        response = await generate_safe_response(prompt)\n",
    "        print(f\"Prompt: {prompt}\\nResponse: {response}\\n\")\n",
    "\n",
    "# Run the async function in Google Colab\n",
    "loop = asyncio.get_event_loop()\n",
    "loop.run_until_complete(main())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S8inm3YT2X-7"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kdfIOJOGDiL9"
   },
   "source": [
    "## Using Scorers as Monitors\n",
    "\n",
    "Monitors help track quality metrics over time without blocking operations. This is useful for:\n",
    "\n",
    "* Identifying quality trends\n",
    "* Detecting model drift\n",
    "* Gathering data for model improvements\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hpEQ3TDS3lVh"
   },
   "outputs": [],
   "source": [
    "import weave\n",
    "from weave import Scorer\n",
    "import json\n",
    "import xml.etree.ElementTree as ET\n",
    "import random\n",
    "import asyncio\n",
    "import nest_asyncio  # Required for Google Colab\n",
    "\n",
    "# Apply nest_asyncio to avoid event loop issues in Google Colab\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# ==== 1. Define Text Generation Function ====\n",
    "@weave.op\n",
    "def generate_text(prompt: str) -> str:\n",
    "    \"\"\"Simulated LLM response generation.\"\"\"\n",
    "    if prompt.lower() == \"json\":\n",
    "        return '{\"message\": \"Hello, world!\"}'  # Valid JSON\n",
    "    elif prompt.lower() == \"xml\":\n",
    "        return \"<message>Hello, world!</message>\"  # Valid XML\n",
    "    else:\n",
    "        return \"Generated response...\"\n",
    "\n",
    "# ==== 2. Custom Scorer for JSON Validation ====\n",
    "class CustomJSONScorer(Scorer):\n",
    "    @weave.op\n",
    "    def score(self, output: str) -> dict:\n",
    "        \"\"\"Check if the output is valid JSON.\"\"\"\n",
    "        try:\n",
    "            json.loads(output)\n",
    "            return {\"valid_json\": True}\n",
    "        except json.JSONDecodeError:\n",
    "            return {\"valid_json\": False}\n",
    "\n",
    "# ==== 3. Custom Scorer for XML Validation ====\n",
    "class CustomXMLScorer(Scorer):\n",
    "    @weave.op\n",
    "    def score(self, output: str) -> dict:\n",
    "        \"\"\"Check if the output is valid XML.\"\"\"\n",
    "        try:\n",
    "            ET.fromstring(output)\n",
    "            return {\"valid_xml\": True}\n",
    "        except ET.ParseError:\n",
    "            return {\"valid_xml\": False}\n",
    "\n",
    "# ==== 4. Function to Generate Response with Monitoring ====\n",
    "async def generate_with_monitoring(prompt: str) -> str:\n",
    "    \"\"\"\n",
    "    Generates a response and applies monitoring (randomly 10% of the time).\n",
    "    \"\"\"\n",
    "    # Generate text and capture tracking info\n",
    "    result, call = generate_text.call(prompt)\n",
    "\n",
    "    # Sample monitoring (only apply scorers to 10% of calls)\n",
    "    if random.random() < 0.1:\n",
    "        # Apply manual scorers asynchronously\n",
    "        json_score = await call.apply_scorer(CustomJSONScorer())\n",
    "        xml_score = await call.apply_scorer(CustomXMLScorer())\n",
    "\n",
    "        print(f\"Monitoring Applied - JSON Valid: {json_score.result['valid_json']}, XML Valid: {xml_score.result['valid_xml']}\")\n",
    "\n",
    "    return result\n",
    "\n",
    "# ==== 5. Run Test Cases ====\n",
    "async def main():\n",
    "    prompts = [\"json\", \"xml\", \"text\"]\n",
    "\n",
    "    for prompt in prompts:\n",
    "        response = await generate_with_monitoring(prompt)\n",
    "        print(f\"Prompt: {prompt}\\nResponse: {response}\\n\")\n",
    "\n",
    "# Run the async function in Google Colab\n",
    "loop = asyncio.get_event_loop()\n",
    "loop.run_until_complete(main())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Media"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import weave\n",
    "from openai import OpenAI\n",
    "import requests\n",
    "from PIL import Image\n",
    "\n",
    "weave.init(project_name=PROJECT)\n",
    "client = OpenAI()\n",
    "\n",
    "@weave.op()\n",
    "def generate_image(prompt: str) -> Image:\n",
    "    response = client.images.generate(\n",
    "        model=\"dall-e-3\",\n",
    "        prompt=prompt,\n",
    "        size=\"1024x1024\",\n",
    "        quality=\"standard\",\n",
    "        n=1,\n",
    "    )\n",
    "    image_url = response.data[0].url\n",
    "    image_response = requests.get(image_url, stream=True)\n",
    "    image = Image.open(image_response.raw)\n",
    "\n",
    "    # return a PIL.Image.Image object to be logged as an image\n",
    "    return image\n",
    "\n",
    "image = generate_image(\"a cat with a pumpkin hat\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import weave\n",
    "from openai import OpenAI\n",
    "import wave\n",
    "\n",
    "weave.init(project_name=PROJECT)\n",
    "client = OpenAI()\n",
    "\n",
    "@weave.op\n",
    "def make_audio_file_streaming(text: str) -> wave.Wave_read:\n",
    "    with client.audio.speech.with_streaming_response.create(\n",
    "        model=\"tts-1\",\n",
    "        voice=\"alloy\",\n",
    "        input=text,\n",
    "        response_format=\"wav\",\n",
    "    ) as res:\n",
    "        res.stream_to_file(\"output.wav\")\n",
    "\n",
    "    # return a wave.Wave_read object to be logged as audio\n",
    "    return wave.open(\"output.wav\")\n",
    "\n",
    "make_audio_file_streaming(\"Hello, how are you? What did you do yesterday?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "from moviepy.editor import VideoFileClip, ColorClip, VideoClip\n",
    "import weave\n",
    "\n",
    "weave.init(PROJECT)\n",
    "\n",
    "@weave.op()\n",
    "def store_videos(name, client, generated_video):\n",
    "  client.files.download(file=generated_video.video)\n",
    "  generated_video.video.save(f\"new_video{name}.mp4\")  # save the video\n",
    "\n",
    "@weave.op()\n",
    "def save_videos_in_weave(video_path):\n",
    "    VideoFileClip(video_path, has_mask=False, audio=True)\n",
    "    new_clip = clip.subclip(0, 1)\n",
    "    return new_clip\n",
    "\n",
    "@weave.op()\n",
    "def generate_videos(prompt):\n",
    "    client = genai.Client()  # read API key from GOOGLE_API_KEY\n",
    "    operation = client.models.generate_videos(\n",
    "        model=\"veo-2.0-generate-001\",\n",
    "        prompt=\"Panning wide shot of a calico kitten sleeping in the sunshine\",\n",
    "        config=types.GenerateVideosConfig(\n",
    "            person_generation=\"dont_allow\",  # \"dont_allow\" or \"allow_adult\"\n",
    "            aspect_ratio=\"16:9\",  # \"16:9\" or \"9:16\"\n",
    "            ),\n",
    "        )\n",
    "    \n",
    "    while not operation.done:\n",
    "        time.sleep(10)\n",
    "        operation = client.operations.get(operation)\n",
    "    \n",
    "    for n, generated_video in enumerate(operation.response.generated_videos):\n",
    "        client.files.download(file=generated_video.video)\n",
    "        generated_video.video.save(f\"video{n}.mp4\")  # save the video\n",
    "        save_videos_in_weave(f\"video{n}.mp4\")\n",
    "        \n",
    " \n",
    "generate_videos(\"Panning wide shot of a calico kitten sleeping in the sunshine\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
